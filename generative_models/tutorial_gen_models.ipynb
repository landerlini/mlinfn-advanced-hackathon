{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcVGQpi5i5xG"
   },
   "source": [
    "# Hands-on Deep Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/landerlini/mlinfn-advanced-hackathon/tutorial_gen_models.ipynb)\n",
    "[![Open In Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/landerlini/mlinfn-advanced-hackathon/tutorial_gen_models.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open_in_GitHub-blue?style=flat&logo=github&logoColor=white&labelColor=555)](https://github.com/landerlini/mlinfn-advanced-hackathon/tutorial_gen_models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Authors:</b> M. Barbetti (INFN-CNAF), S. Capelli (INFN-MiB), F. Vaselli (INFN-Pisa)\n",
    "\n",
    "  <b>Date created:</b> 30/10/2023\n",
    "  \n",
    "  <b>Last modified:</b> 02/11/2023\n",
    "\n",
    "  <b>Description: </b> This hands-on demonstrates how deep generative models succeed in reproducing the high-level response of a generic HEP experiment, offering a viable solution to reduce the pressure on the computing budget for simulation production. In particular, in this notebook we will test the performance of Generative Adversarial Networks and Normalizing Flows to parameterize the errors introduced during the detection and reconstruction of high-energy particle jets using the CMS detector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvTVEAa7i5xJ"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opu773xWi5xK"
   },
   "source": [
    "### What is CMS?\n",
    "\n",
    "The Compact Muon Solenoid (CMS) experiment is a key research project at the Large Hadron Collider (LHC) at CERN. It's a particle physics detector that observes and measures the byproducts of high-energy particle collisions in the LHC. CMS is designed to investigate a wide range of physics phenomena, and it played a pivotal role in the discovery of the Higgs boson in 2012.\n",
    "\n",
    "CMS is a general-purpose detector, meaning it's equipped to study various aspects of particle physics. Its main components include a superconducting solenoid magnet, various particle detectors, and systems for tracking, energy measurement, and muon detection. This sophisticated setup allows scientists to explore several key questions in fundamental physics, such as the properties of the Higgs boson, the search for supersymmetry, the nature of dark matter, and potential evidence of extra dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTEAA_GpgXXM"
   },
   "source": [
    "#### Measuring jets at CMS\n",
    "\n",
    "In CMS, particle jets are crucial for understanding high-energy processes. They are sprays of particles formed when quarks and gluons, produced in high-energy collisions like those at the LHC, 'hadronize' or turn into ordinary matter. By analyzing the jets' energy and momentum, physicists can infer properties of the original particles, helping in studies like proton structure, the strong force, and new physics beyond the Standard Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://cms.cern/sites/default/files/field/image/Sketch_PartonParticleCaloJet.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Deep Generative Models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPnfIVMoi5xK"
   },
   "source": [
    "#### Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks [[1][1]] are a powerful class of _generative models_ based on the simultaneous training of two neural networks:\n",
    "\n",
    "*  **Discriminator network** ($D$) - trained by a classification task to separate the generator output from the reference dataset\n",
    "* **Generator network** ($G$) - trained by a simulation task to reproduce the reference dataset trying to fake the discriminator\n",
    "\n",
    "The goal is that $D$ optimally discriminates on the origin of the two samples, and simultaneously the training procedure for $G$ is to maximize the _probability_ of $D$ making a mistake. This framework corresponds to a **minimax two-player game** [[1][1]].\n",
    "\n",
    "[1]: https://arxiv.org/abs/1406.2661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Hs8F8pi5xK"
   },
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/mbarbetti/pidgan-notebooks/main/.github/images/gan-scheme.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional details\n",
    "\n",
    "Traditional GAN systems suffer from many issues, particularly during the training phase:\n",
    "\n",
    "* the generator may _collapse_ producing only a single sample or a small family of very similar samples (**mode collapse**)\n",
    "* the two players may _oscillate_ during training rather than converging to the [**Nash equilibrium**](https://en.wikipedia.org/wiki/Nash_equilibrium)\n",
    "* if _imbalance_ between the two players occurs, then the system is incapable of learning at all\n",
    "\n",
    "All these drawbacks result from the [**vanishing gradient problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), namely the lack of information for the update of the $G$ parameters. This is due to the saturation of the $D$ that is so good in distinguishing the origin of the two samples that no errors remain to the $G$ to improve the generated space [[2][2]]. To fix such problem, the typical solution proposed by the literature is to change the loss function that drive the training procedure [[3][3], [4][4]], to force the smoothness of the discriminator response to the classification task [[5][5], [6][6], [7][7]], or to adoopt a set of strategies in the architecture of the two players [[8][8]]. Each of these solutions can be easily implemented through [pidgan](https://github.com/mbarbetti/pidgan), a Python package designed to simplify the provisioning of GAN-based models to flash-simulate the LHCb experiment.\n",
    "\n",
    "[2]: https://arxiv.org/abs/1701.04862\n",
    "[3]: https://arxiv.org/abs/1611.04076\n",
    "[4]: https://arxiv.org/abs/1701.07875\n",
    "[5]: https://arxiv.org/abs/1704.00028\n",
    "[6]: https://arxiv.org/abs/1705.10743\n",
    "[7]: https://arxiv.org/abs/1907.05681\n",
    "[8]: https://arxiv.org/abs/1606.03498\n",
    "[9]: https://arxiv.org/abs/1411.1784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Flows\n",
    "\n",
    "GANs do not explicitly learn $p(\\mathbf{x})$, the probability density function of real data--it can be really hard to! Taking the generative model with latent variables as an example, $p(\\mathbf{x}) = \\int p(\\mathbf{x}\\vert\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code.\n",
    "\n",
    "Flow-based deep generative models conquer this hard problem with the help of normalizing flows, a powerful statistics tool for density estimation. A good estimation of $p(\\mathbf{x})$ makes it possible to efficiently complete many downstream tasks: sample unobserved but realistic new data points (**data generation**), predict the rareness of future events (density estimation), infer latent variables, fill in incomplete data samples, etc.\n",
    "\n",
    "Here comes a Normalizing Flow (NF) model for better and more powerful distribution approximation. A normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformation functions. Flowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable. During training we learn $z=f(x)$ sending data into the Gaussian space, then we can invert $f$ to get new samples $\\hat{x}= f^{-1}(z)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvpIdCnSi5xL"
   },
   "source": [
    "## Hands-on generative models with CMS data\n",
    "\n",
    "General Simulation Steps in CMS and HEP\n",
    "\n",
    "In High Energy Physics (HEP), simulations are crucial for understanding the outcomes of particle collisions, like those observed in the CMS experiment. The general simulation process involves several steps:\n",
    "\n",
    "    -Event Generation: Simulating the initial high-energy collision and the primary processes that follow.\n",
    "    -Particle Tracking and Interaction: Simulating how the particles produced in the collision travel through and interact with the detector material.\n",
    "    -Detector Response: Modeling how the detector responds to these particles.\n",
    "    -Data Reconstruction (Reco-Level): Processing the simulated detector signals to reconstruct what happened in the collision.\n",
    "\n",
    "These steps are computationally expensive due to the complex physics involved and the high precision required. Each collision can produce a multitude of particles, making the simulation of even a single event a resource-intensive task.\n",
    "Advantages of Using ML\n",
    "\n",
    "Machine Learning (ML) can significantly speed up this process, particularly the reconstruction phase. By training models to emulate the behavior of particles in the detector and their interaction, we can bypass the need for detailed, step-by-step simulation, reducing computation time and resource usage.\n",
    "\n",
    "In this exercise, we aim to simulate reconstruction-level (reco-level) jets from generator-level (gen-level) data. We have prepared Ntuple numpy arrays for about 1,000,000 pairs of genjets and their associated recojets from with Pythia. These numpy arrays are essentially large datasets containing features of both genjets and recojets.\n",
    "\n",
    "**Problem Statement**: Learn to generate the features of recojets given the values of the associated genjet features. This exercise will involve predicting the properties of the recojets (like momentum, eta, etc.) based on the known properties of the genjets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Initial setup for your ML-INFN instance (not needed if you are running on Colab or Kaggle).\n",
    "\n",
    "**Note:** If you plan to run this notebook live during the hands-on, we suggest to set `True` the variable `LIVE` in order to limit the execution time of the whole notebook to about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVE = True\n",
    "\n",
    "import os, subprocess, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\".join(\n",
    "  re.findall(\n",
    "    \"UUID: (MIG-[^)]+)\\)\",\n",
    "    str(subprocess.check_output([\"nvidia-smi\", \"-L\"]), \"ascii\")\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ohrh3dQi5xL"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "The first step is to import the main packages of this hands-on, namely [pidgan](https://github.com/mbarbetti/pidgan) and [nflows](https://github.com/bayesiains/nflows). If you are running out of the provided ML-INFN instances (like Colab or Kaggle), before to run the following code cells, you need to install the both the packages and some other ones that are typically needed in machine learning applications to HEP (i.e., uproot, scipy, scikit-learn, matplotlib). This can be done adding a new code cell containing what follows:\n",
    "\n",
    "```\n",
    "%%capture\n",
    "!pip install pidgan[hep] nflows corner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmb7g7HZi5xL"
   },
   "source": [
    "Let's verify the correct installation of [pidgan](https://github.com/mbarbetti/pidgan) printing its version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CWg67WFhi5xL",
    "outputId": "8fc7cdac-b48c-4da2-e106-f95f2196c9e2"
   },
   "outputs": [],
   "source": [
    "import pidgan\n",
    "\n",
    "pidgan.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8lt6bjDi5xM"
   },
   "source": [
    "Since [pidgan](https://github.com/mbarbetti/pidgan) relies on TensorFlow and Keras as backends, we also need to verify the correct installation of TensorFlow checking that it is able to assess and run on the equipped GPU (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGoxRYbGi5xM",
    "outputId": "99fdba38-dd7f-4b82-b534-8256dcfddbba"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "avail_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "avail_gpus  # outputs a non-empty list in case of GPU equipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify the correct installation of [nflows](https://github.com/bayesiains/nflows) printing its version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.version import __version__ as nflows_version\n",
    "\n",
    "nflows_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since [nflows](https://github.com/bayesiains/nflows) relies on PyTroch as backend, we also need to verify the correct installation of PyTorch checking that it is able to assess and run on the equipped GPU (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#device = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "#print(device)# outputs a non-empty list in case of GPU equipped\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "x = torch.randn((100,1)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uslhx_PjvOwk"
   },
   "source": [
    "Finally, we just have to import all the modules that we will use in the following code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twM1ba1PvgVl"
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import lines as mlines\n",
    "import corner\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from scipy import stats\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import ConditionalDiagonalNormal,StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform,MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.nn.nets import ResidualNet\n",
    "from nflows.distributions.normal import ConditionalDiagonalNormal,StandardNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_histograms_and_ratios(model_sample, original_sample):\n",
    "    \"\"\"\n",
    "    Plot histograms and ratios of features from model and original samples.\n",
    "\n",
    "    :param model_sample: Numpy array of samples generated by the model.\n",
    "    :param original_sample: Numpy array of original (FullSim) samples.\n",
    "    \"\"\"\n",
    "    # Names of the variables for the plots\n",
    "    feature_names = [\"pt\", \"Eta\", \"phi\", \"nConstituents\", \"bTag\"]\n",
    "\n",
    "    # Set up the figure and axes for the plot\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 6), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    fig.suptitle(\"Histograms and Ratio Plots\", fontsize=16, y=1.05)\n",
    "\n",
    "    for j, (ax, rax) in enumerate(zip(axes[0], axes[1])):\n",
    "        # Adjust binning based on the feature\n",
    "        bins = np.arange(-0.5, 80.5, 2) if j == 3 else 50\n",
    "\n",
    "        # Histogram range for original sample\n",
    "        _, rangeR, _ = ax.hist(original_sample[:, j], histtype=\"step\", lw=1, bins=bins, label=\"FullSim\")\n",
    "\n",
    "        # Saturate model samples to histogram range\n",
    "        saturated_samples = np.clip(model_sample[:, j], np.min(rangeR), np.max(rangeR))\n",
    "\n",
    "        # Plot histogram for model samples\n",
    "        ax.hist(saturated_samples, histtype=\"step\", lw=1, range=[np.min(rangeR), np.max(rangeR)], bins=bins, label=\"Model\")\n",
    "        ax.legend(frameon=False, loc=\"upper right\")\n",
    "\n",
    "        # Calculate and plot the ratio of the histograms\n",
    "        hist_reco, bins_reco = np.histogram(original_sample[:, j], bins=bins, range=[np.min(rangeR), np.max(rangeR)])\n",
    "        hist_model, bins_model = np.histogram(saturated_samples, bins=bins, range=[np.min(rangeR), np.max(rangeR)])\n",
    "        ratio = np.where(hist_reco > 0, hist_model / hist_reco, 0)\n",
    "        rax.plot(bins_reco[:-1], ratio, marker=\"x\", markersize=3.5, linestyle=\"\", label=\"Model/FullSim\")\n",
    "        rax.axhline(y=1, color='red', linestyle='--', linewidth=0.5)\n",
    "        rax.set_ylabel(\"Ratio\")\n",
    "        rax.set_ylim(0.5, 1.5)  # Set the y-axis range for clarity\n",
    "\n",
    "        # Set titles and labels\n",
    "        ax.set_title(feature_names[j])\n",
    "\n",
    "    # Adjust the layout for better readability\n",
    "    plt.subplots_adjust(top=0.9, hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "def make_corner(reco, samples, title):\n",
    "    \"\"\"\n",
    "    Create a corner plot comparing FullSim and model samples.\n",
    "\n",
    "    :param reco: Numpy array of FullSim samples.\n",
    "    :param samples: Numpy array of samples generated by the model.\n",
    "    :param title: Title for the corner plot.\n",
    "    \"\"\"\n",
    "    # Legend lines for the plot\n",
    "    blue_line = mlines.Line2D([], [], color=\"tab:blue\", label=\"FullSim\")\n",
    "    red_line = mlines.Line2D([], [], color=\"tab:orange\", label=\"Model\")\n",
    "\n",
    "    # Define ranges and labels for the corner plot\n",
    "    ranges = [(0, 300), (-2, 2), (0, 6.4), (0, 80), (-0.2, 1.2)]  # Ranges for pt, eta, phi, nConst, btag\n",
    "    labels = [\"pt\", \"eta\", \"phi\", \"NConstituents\", \"btag\"]\n",
    "\n",
    "    # Create the corner plot for FullSim samples\n",
    "    fig = corner.corner(reco, range=ranges, labels=labels, color=\"tab:blue\", levels=[0.5, 0.9, 0.99],\n",
    "                        hist_bin_factor=3, scale_hist=True, plot_datapoints=False, hist_kwargs={\"ls\": \"--\"},\n",
    "                        contour_kwargs={\"linestyles\": \"--\"}, label_kwargs={\"fontsize\": 16})\n",
    "\n",
    "    # Add the model samples to the same plot\n",
    "    fig = corner.corner(samples, range=ranges, fig=fig, color=\"tab:orange\", levels=[0.5, 0.9, 0.99],\n",
    "                  hist_bin_factor=3, scale_hist=True, plot_datapoints=False, label_kwargs={\"fontsize\": 16})\n",
    "\n",
    "    # Set the title and return the figure\n",
    "    plt.suptitle(title, fontsize=20)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz9PhSN8wP2h"
   },
   "source": [
    "### Data loading and preparation\n",
    "\n",
    "The data needed for this hands-on is provided within the `data` folder as a root file (`'cms_data.root'`). The dataset contains about 1.2M instances, 5 input features and 5 output features. Let's import the data as a pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "QfnPXl161n2f",
    "outputId": "5185da95-9df0-41ea-96b4-bbd0d97d7c3e"
   },
   "outputs": [],
   "source": [
    "with uproot.open(\"./data/cms_data.root\") as file:\n",
    "  data = file[\"recojet\"].arrays(library=\"pd\").drop(\"index\", axis=1)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you are running out of the provided ML-INFN instances (like Colab or Kaggle), a reduced version of the original dataset is available through a CSV file and can be imported as a pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) using the following code lines:\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/landerlini/mlinfn-advanced-hackathon/main/generative_models/data/cms_data_reduced.csv\"\n",
    "data = pd.read_csv(url)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhaU0lWcZ-6L"
   },
   "source": [
    "TBA (add some information about the input features)\n",
    "\n",
    "Gen-level info:\n",
    "* `data.pT` - transverse momentum $p_T$ in GeV/$c$ of the jet\n",
    "* `data.eta` - pseudorapidity $\\eta$ of the jet\n",
    "* `data.phi` - angle $\\varphi$ in the x-y plane of the jet\n",
    "* `data.E` - energy $E$ in GeV of the jet\n",
    "* `data.flavour` - flavour of the jet (-6: $\\bar{b}$, -4: $\\bar{c}$, 0: $g$, 4: $c$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "j99DTVou5KdZ",
    "outputId": "36e50219-16c2-41c4-80ba-42b1bbab2481"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.xlabel(\"Generated $p_T$ [GeV/$c$]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"pT\"], bins=np.linspace(0.0, 500.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.xlabel(r\"Generated $\\eta$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"eta\"], bins=np.linspace(-2*np.pi, 2*np.pi, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.xlabel(r\"Generated $\\varphi$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"phi\"], bins=np.linspace(0.0, 2*np.pi, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.xlabel(\"Generated energy [GeV]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"E\"], bins=np.linspace(0.0, 2500.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.xlabel(\"Generated jet flavour\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"flavour\"], bins=np.linspace(-6.0, 6.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV9PdrDtb5gK"
   },
   "source": [
    "TBA (add some information about the output features)\n",
    "\n",
    "Reco-level info:\n",
    "* `data.reco_pT` - reconstructed transverse momentum $p_T$ in GeV/$c$ of the jet\n",
    "* `data.reco_eta` - reconstructed pseudorapidity $\\eta$ of the jet\n",
    "* `data.reco_phi` - reconstructed angle $\\varphi$ in the x-y plane of the jet\n",
    "* `data.reco_nConstituents` - reconstructed number of particles within the jet\n",
    "* `data.b_tag` - TBA (add description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "2r8zWzbI7A2V",
    "outputId": "9c0ac896-739d-40cf-9c9b-ade8ef10e380"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.xlabel(\"Reconstructed $p_T$ [GeV/$c$]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"reco_pT\"], bins=np.linspace(0.0, 500.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.xlabel(r\"Reconstructed $\\eta$ [rad]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"reco_eta\"], bins=np.linspace(-2*np.pi, 2*np.pi, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.xlabel(r\"Reconstructed $\\varphi$ [rad]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"reco_phi\"], bins=np.linspace(0.0, 2*np.pi, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.xlabel(\"Number of particles within the jet\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"reco_nConstituents\"], bins=np.linspace(0.0, 50.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.xlabel(\"$b$-tagging\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(data[\"b_tag\"], bins=np.linspace(0.0, 1.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDfCdUkIi5xM"
   },
   "source": [
    "As usual in a machine learning application, the data sample is split into **training** and **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data)\n",
    "\n",
    "chunk_size = -1 if not LIVE else 500000\n",
    "data = data[:chunk_size]\n",
    "\n",
    "x_vars = [\"pT\", \"eta\", \"phi\", \"E\", \"flavour\"]\n",
    "x = data[x_vars].values\n",
    "\n",
    "y_vars = [\"reco_pT\", \"reco_eta\", \"reco_phi\", \"reco_nConstituents\", \"b_tag\"]\n",
    "y = data[y_vars].values\n",
    "\n",
    "train_ratio = 0.7\n",
    "train_size = int(train_ratio * len(data))\n",
    "\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} \\t x_test shape: {x_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape} \\t y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "\n",
    "To normalize the different ranges and orders of magnitude of input $x$ and output $y$ features highlighted by the previous histograms, we will use the scikit-learn [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) and [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) preprocessing operators.\n",
    "\n",
    "Since one of the $x$ features is categorical (`data.flavour`), the latter should be kept unchanged by the preprocessing operators and transformed through a [**one-hot**](https://en.wikipedia.org/wiki/One-hot) encoder.\n",
    "\n",
    "If we want to combine together different preprocessing operators in one go, we can use the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) class by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = ColumnTransformer(\n",
    "  [\n",
    "    (\"quantile\", QuantileTransformer(output_distribution=\"normal\"), (0, 3)),  # pT, E\n",
    "    (\"standard\", StandardScaler(), (1, 2)),  # theta, phi\n",
    "  ]\n",
    ")\n",
    "x_train_prep = x_scaler.fit_transform(x_train[:, :-1])  # except 'flavour' column\n",
    "\n",
    "pd.DataFrame(x_train_prep, columns=[\"pT\", \"E\", \"eta\", \"phi\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "int_encoded = label_encoder.fit_transform(x_train[:, -1])  # 'flavour' column\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(int_encoded[:, None])  # one-hot encoding\n",
    "\n",
    "x_train_prep = np.concatenate([x_train_prep, onehot_encoded], axis=1)\n",
    "x_train_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY6OOsx6BhQP"
   },
   "source": [
    "Since most of the output $y$ features are the reconstructed version of the input $x$ features, we should build a model able to reproduce only the **errors** introduced in the detection and reconstruction steps instead of directly parameterize the reconstruction algorithms. This _regularizes_ the training procedure and may simplify any debugging campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prep = y_train.copy()\n",
    "y_train_prep[:, :3] -= x_train[:, :3]\n",
    "y_train_prep[:, 3] += np.random.uniform(-0.5, 0.5, size=(len(y_train),))  # 'reco_nConstituents' from discrete to continous\n",
    "\n",
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.xlabel(\"$\\Delta p_T$ [GeV/$c$]\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_train_prep[:, 0], bins=np.linspace(-50.0, 50.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.xlabel(r\"$\\Delta \\eta$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_train_prep[:, 1], bins=np.linspace(-0.1, 0.1, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.xlabel(r\"$\\Delta \\varphi$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_train_prep[:, 2], bins=np.linspace(-0.1, 0.1, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.xlabel(\"Number of particles within the jet (+ random noise)\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_train_prep[:, 3], bins=np.linspace(0.0, 50.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.xlabel(\"$b$-tagging\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_train_prep[:, 4], bins=np.linspace(0.0, 1.0, 101), color=\"#3288bd\", label=\"MC data\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = QuantileTransformer(output_distribution=\"normal\")\n",
    "y_train_prep = y_scaler.fit_transform(y_train_prep)\n",
    "\n",
    "pd.DataFrame(y_train_prep, columns=y_vars).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVNTdWLPi5xM"
   },
   "source": [
    "Implementing a GAN in TensorFlow is straightforward, even if becoming familiar with the various tricks to stabilize the training procedure or moving from a GAN flavour to another may be non-trivial. The aim of this hands-on is to prove the effectiveness of generative models for HEP simulations, hence we will rely on [pidgan](https://github.com/mbarbetti/pidgan) for the GAN implementations. Such Python package has been designed in the context of LHCb flash-simulation and offers a wide set of GAN algorithms implemented customizing the TensorFlow [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) class.\n",
    "\n",
    "| Algorithms* | Lipschitzianity** | Design inspired by | Tutorial |\n",
    "|:-----------:|:-----------------:|:------------------:|:--------:|\n",
    "| [`GAN`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/GAN.py) | ❌ | [1][1], [2][2], [8][8] | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-GAN-LHCb_RICH.ipynb) |\n",
    "| [`BceGAN`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/BceGAN.py) | ❌ | [1][1], [2][2], [8][8] | 🛠️ |\n",
    "| [`BceGAN_GP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/BceGAN_GP.py) | ✅ | [1][1], [5][5], [8][8] | 🛠️ |\n",
    "| [`BceGAN_ALP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/BceGAN_ALP.py) | ✅ | [1][1], [7][7], [8][8] | 🛠️ |\n",
    "| [`LSGAN`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/LSGAN.py) | ❌ | [3][3], [2][2], [8][8] | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-LSGAN-LHCb_RICH.ipynb) |\n",
    "| [`WGAN`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN.py) | ✅ | [4][4], [8][8] | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-WGAN-LHCb_RICH.ipynb) |\n",
    "| [`WGAN_GP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN_GP.py) | ✅ | [5][5], [8][8] | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-WGAN_GP-LHCb_RICH.ipynb) |\n",
    "| [`CramerGAN`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/CramerGAN.py) | ✅ | [6][6], [8][8] | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-CramerGAN-LHCb_RICH.ipynb) |\n",
    "| [`WGAN_ALP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN_ALP.py) | ✅ | [7][7], [8][8] |  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbarbetti/pidgan-notebooks/blob/main/tutorial-WGAN_ALP-LHCb_RICH.ipynb) |\n",
    "\n",
    "*each GAN algorithm is designed to operate taking __conditions__ as input [[9][9]]\n",
    "\n",
    "**the GAN training is regularized to ensure that the discriminator encodes a 1-Lipschitz function\n",
    "\n",
    "Training a GAN system corresponds to perform a competition, namely the **minimax game**, between the generator and discriminator networks. The latters are provided by the [pidgan](https://github.com/mbarbetti/pidgan) package through the [`players`](https://github.com/mbarbetti/pidgan/tree/main/src/pidgan/players) module that implements the two networks via custom TensorFlow [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)s.\n",
    "\n",
    "[1]: https://arxiv.org/abs/1406.2661\n",
    "[2]: https://arxiv.org/abs/1701.04862\n",
    "[3]: https://arxiv.org/abs/1611.04076\n",
    "[4]: https://arxiv.org/abs/1701.07875\n",
    "[5]: https://arxiv.org/abs/1704.00028\n",
    "[6]: https://arxiv.org/abs/1705.10743\n",
    "[7]: https://arxiv.org/abs/1907.05681\n",
    "[8]: https://arxiv.org/abs/1606.03498\n",
    "[9]: https://arxiv.org/abs/1411.1784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx8Dkbz5o9Q"
   },
   "source": [
    "#### The generator\n",
    "\n",
    "The **generator player** can be implemented with the high-level [`Generator`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/players/generators/Generator.py) class that provides the neural network model wrapping the TensorFlow [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model). Before to initialize a `Generator` instance, let's have a look at its arguments:\n",
    "\n",
    "* **`output_dim`** (_int_) - Dimensionality of the generator output space.\n",
    "* **`latent_dim`** (_int_) - Dimensionality of the latent space.\n",
    "* **`num_hidden_layers`** (_int_, default=5) - Number of the hidden layers passed to the [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model. Each hidden layer has a LeakyReLU as activation function (see [`tf.keras.layers.LeakyReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU)).\n",
    "* **`mlp_hidden_units`** (_int_ or _array_like_, default=128) - Number of neuron units per each hidden layer. It can also be passed as an array of unit numbers having length equal to the number of hidden layers.\n",
    "* **`mlp_dropout_rates`** (_float_ or _array_like_, default=0.0) - Fraction of the input neurons to drop per each hidden layer. It can also be passed as an array of float numbers having length equal to the number of hidden layers.\n",
    "* **`output_activation`** (_activation_like_ or _None_, default=None) - Activation function to use in the generator output layer (see [`tf.keras.activations`](https://www.tensorflow.org/api_docs/python/tf/keras/activations)). If None, no activation is applied (i.e., \"linear\" activation: $a(x) = x$).\n",
    "* **`name`** (_str_ of _None_, default=None) - The name of the generator model.\n",
    "* **`dtype`** (_dtype_ or _None_, default=None) - The dtype of the generator layer computations and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0th7xpVKCGN0"
   },
   "outputs": [],
   "source": [
    "from pidgan.players.generators import Generator\n",
    "\n",
    "generator = Generator(\n",
    "    output_dim=y_train_prep.shape[-1],\n",
    "    latent_dim=64,\n",
    "    num_hidden_layers=5,\n",
    "    mlp_hidden_units=128,\n",
    "    mlp_dropout_rates=0.0,\n",
    "    output_activation=None,\n",
    "    name=\"generator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luH3v1XVEBlc",
    "outputId": "3a7f39eb-89a1-4689-e539-4979ba7b7b8b"
   },
   "outputs": [],
   "source": [
    "_ = generator(keras.Input(shape=x_train_prep.shape[1:]))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MkE6GA14-hz"
   },
   "source": [
    "#### The discriminator\n",
    "\n",
    "The **discriminator player** can be implemented with the high-level [`Discriminator`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/players/discriminators/Discriminator.py) class that provides the neural network model wrapping the TensorFlow [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model). Again, before to initialize a `Discriminator` instance, let's have a look to its arguments:\n",
    "\n",
    "* **`output_dim`** (_int_) - Dimensionality of the discriminator output space.\n",
    "* **`num_hidden_layers`** (_int_, default=5) - Number of the hidden layers passed to the [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model. Each hidden layer has a LeakyReLU as activation function (see [`tf.keras.layers.LeakyReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU)).\n",
    "* **`mlp_hidden_units`** (_int_ or _array_like_, default=128) - Number of neuron units per each hidden layer. It can also be passed as an array of unit numbers having length equal to the number of hidden layers.\n",
    "* **`mlp_dropout_rates`** (_float_ or _array_like_, default=0.0) - Fraction of the input neurons to drop per each hidden layer. It can also be passed as an array of float numbers having length equal to the number of hidden layers.\n",
    "* **`output_activation`** (_activation_like_ or _None_, default=\"sigmoid\") - Activation function to use in the dicriminator output layer (see [`tf.keras.activations`](https://www.tensorflow.org/api_docs/python/tf/keras/activations)). If None, no activation is applied (i.e., \"linear\" activation: $a(x) = x$).\n",
    "* **`name`** (_str_ of _None_, default=None) - The name of the discriminator model.\n",
    "* **`dtype`** (_dtype_ or _None_, default=None) - The dtype of the discriminator layer computations and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufj8Xi4fC_Ot"
   },
   "outputs": [],
   "source": [
    "from pidgan.players.discriminators import Discriminator\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    output_dim=1,\n",
    "    num_hidden_layers=5,\n",
    "    mlp_hidden_units=128,\n",
    "    mlp_dropout_rates=0.0,\n",
    "    output_activation=None,\n",
    "    name=\"discriminator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfqi9DHvETlY",
    "outputId": "6534dd3f-5545-46ce-f0aa-557a7cb4a965"
   },
   "outputs": [],
   "source": [
    "_ = discriminator((keras.Input(shape=x_train_prep.shape[1:]), keras.Input(shape=y_train_prep.shape[1:])))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpQQWkPS5htN"
   },
   "source": [
    "#### The GAN algorithm\n",
    "\n",
    "TBA (fix the following description)\n",
    "\n",
    "The **WGAN-ALP algorithm** [[3][3]] can be implemented with the high-level [`WGAN_ALP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN_ALP.py) class that defines the training procedure of the two players customizing the TensorFlow [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) class and, in particular, its `fit()` method. Once again, before to initialize a `WGAN_ALP` instance, let's have a look at its arguments:\n",
    "\n",
    "* **`generator`** (_generator_like_) - The generator network. It must be passed as a pidgan [`Generator`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/players/generators/Generator.py) instance.\n",
    "* **`discriminator`** (_discriminator_like_) - The discriminator network. It must be passed as a pidgan [`Discriminator`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/players/discriminators/Discriminator.py) instance.\n",
    "* **`lipschitz_penalty`** (_float_, default=1.0) - Importance of the regularization term ALP that explicitly constrains the discriminator to induce a 1-Lipschitz function as proposed in Ref. [[4][4]].\n",
    "* **`lipschitz_penalty_strategy`** (_str_, default=\"one-sided\") - If \"two-sided\" is passed, the Lipschitz constant computed along the virtual adversarial direction is forced to be equal to 1. When \"one-sided\" is preferred, only Lipschitz constant values greater than 1 are penalized.\n",
    "* **`feature_matching_penalty`** (_float_, default=0.0) - Importance of the regularization term added to the generator training to force the hidden state of the discriminator to match between reference and generated examples. Its design follows from what proposed in Ref. [[8][8]].\n",
    "* **`referee`** (_classifier_like_ or _None_, default=None) - When passed it enables the training of a third independent player: the referee network. It can be used to assess the quality of the generator output during the training. If passed, it must be a pidgan [`Classifier`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/players/classifiers/Classifier.py) instance.\n",
    "* **`name`** (_str_ or _None_, default=None) - The name of the WGAN-ALP model.\n",
    "* **`dtype`** (_dtype_ or _None_, default=None) - The dtype of the WGAN-ALP model computations.\n",
    "\n",
    "[1]: https://arxiv.org/abs/1406.2661\n",
    "[2]: https://arxiv.org/abs/1701.04862\n",
    "[3]: https://arxiv.org/abs/1611.04076\n",
    "[4]: https://arxiv.org/abs/1701.07875\n",
    "[5]: https://arxiv.org/abs/1704.00028\n",
    "[6]: https://arxiv.org/abs/1705.10743\n",
    "[7]: https://arxiv.org/abs/1907.05681\n",
    "[8]: https://arxiv.org/abs/1606.03498\n",
    "[9]: https://arxiv.org/abs/1411.1784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgW6y0Ru1vPp"
   },
   "source": [
    "To not further complicate this tutorial, we avoid to use the referee network in this case, even if its initialization is straightforward:\n",
    "\n",
    "```python\n",
    "from pidgan.players.classifiers import Classifier\n",
    "\n",
    "referee = Classifier(\n",
    "    num_hidden_layers=5,\n",
    "    mlp_hidden_units=128,\n",
    "    mlp_dropout_rates=0.0,\n",
    "    name=\"referee\"\n",
    ")\n",
    "```\n",
    "\n",
    "It's time to initialize a [`WGAN_ALP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN_ALP.py) instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBEOKmeEDWC4"
   },
   "outputs": [],
   "source": [
    "from pidgan.algorithms import BceGAN_ALP\n",
    "\n",
    "gan = BceGAN_ALP(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    lipschitz_penalty=1.0,\n",
    "    lipschitz_penalty_strategy=\"one-sided\",\n",
    "    feature_matching_penalty=0.0,\n",
    "    referee=None,\n",
    "    name=\"BceGAN_ALP\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMrE8bs952KB"
   },
   "source": [
    "#### Training procedure\n",
    "\n",
    "TBA (fix the following text)\n",
    "\n",
    "To finalize the [`WGAN_ALP`](https://github.com/mbarbetti/pidgan/blob/main/src/pidgan/algorithms/WGAN_ALP.py) model preparation before to run the training procedure, we need to execute the `compile()` method. Differently from the TensorFlow `compile()` method, the [pidgan](https://github.com/mbarbetti/pidgan) one doesn't require to pass the loss function since it is encoded in the customized `gan.fit()` method. The arguments of the `gan.compile()` method follows:\n",
    "\n",
    "* **`metrics`** (_list_ or _None_, default=None) - If not None, list of metrics to be evaluated by the WGAN-ALP model during training and testing. Each of this can be a string (name of a built-in pidgan function) or a [`pidgan.metrics`](https://github.com/mbarbetti/pidgan/tree/main/src/pidgan/metrics) instance.\n",
    "* **`generator_optimizer`** (_optimizer_like_, default=\"rmsprop\") - String (name of optimizer) or optimizer instance for the generator network (see [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)).\n",
    "* **`discriminator_optimizer`** (_optimizer_like_, default=\"rmsprop\") - String (name of optimizer) or optimizer instance for the discriminator network. (see [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)).\n",
    "* **`generator_upds_per_batch`** (_int_, default=1) - Number of the generator weights updates per batch of data.\n",
    "* **`discriminator_upds_per_batch`** (_int_, default=1) - Number of the discriminator weights updates per batch of data.\n",
    "* **`virtual_adv_direction_upds`** (_int_, default=1) - Number of iterations to approximate the virtual adversarial direction per discriminator update.\n",
    "* **`referee_optimizer`** (_optimizer_like_ or _None_, default=None) - If not None, string (name of optimizer) or optimizer instance for the referee network (see [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)).\n",
    "* **`referee_upds_per_batch`** (_int_ or _None_, default=None) - If not None, number of the referee weights updates per batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHL20AxZFNOq"
   },
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    metrics=[\"wass_dist\"],\n",
    "    generator_optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    discriminator_optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    generator_upds_per_batch=1,\n",
    "    discriminator_upds_per_batch=1,\n",
    "    virtual_adv_direction_upds=1,\n",
    "    referee_optimizer=None,\n",
    "    referee_upds_per_batch=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0htLzsSri5xM"
   },
   "source": [
    "The [pidgan](https://github.com/mbarbetti/pidgan) package provides several learning rate scheduling strategies through the [`callbacks.schedulers`](https://github.com/mbarbetti/pidgan/tree/main/src/pidgan/callbacks/schedulers) module. The pidgan schedulers are designed as [custom TensorFlow callbacks](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks) that also allow to trace the learning rate value of each passed optimizer during the training. In this tutorial, we will use an exponential decay schedule both for the generator and discriminator networks:\n",
    "\n",
    "<center>$\\eta(t) = \\eta_0 \\cdot e^{- \\alpha \\cdot t / \\tau}$</center>\n",
    "\n",
    "where $\\eta(t)$ indicates the learning rate value at a specific training step $t$, $\\eta_0$ the initial learning rate, $\\alpha$ the decay rate and $\\tau$ the decay step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0-PqvTDGd1J"
   },
   "outputs": [],
   "source": [
    "from pidgan.callbacks.schedulers import LearnRateExpDecay\n",
    "\n",
    "schedules = list()\n",
    "\n",
    "g_lr_sched = LearnRateExpDecay(\n",
    "    gan.generator_optimizer,\n",
    "    decay_rate=0.1,\n",
    "    decay_steps=75000,\n",
    "    min_learning_rate=1e-6,\n",
    "    verbose=True,\n",
    "    key=\"g_lr\",\n",
    ")\n",
    "schedules.append(g_lr_sched)\n",
    "\n",
    "d_lr_sched = LearnRateExpDecay(\n",
    "    gan.discriminator_optimizer,\n",
    "    decay_rate=0.1,\n",
    "    decay_steps=50000,\n",
    "    min_learning_rate=1e-6,\n",
    "    verbose=True,\n",
    "    key=\"d_lr\",\n",
    ")\n",
    "schedules.append(d_lr_sched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hDNGvV_FI0S"
   },
   "source": [
    "Calling `gan.fit()` allows to perform the **minimax two-player game**, training the generator to reproduce the reference space $y$ on the basis of the conditions $x$ thanks to the discriminator feedbacks. The arguments taken by the pidgan `fit()` method are the same ones traditionally passed to a TensorFlow [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) instance. The schedulers are passed through the `callbacks` argument and, since their verbosity has been enabled (`verbose` and `key` passed), the learning rates of both the generator and discriminator are printed during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1mGZ3_FHMNg",
    "outputId": "fc109c6d-61bc-4682-ef60-f1cb6504c033"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train = gan.fit(\n",
    "    x=x_train_prep,\n",
    "    y=y_train_prep,\n",
    "    batch_size=256,\n",
    "    epochs=250 if not LIVE else 100,\n",
    "    validation_split=0.3,\n",
    "    callbacks=schedules,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "stop = datetime.now()\n",
    "\n",
    "print(f\"Training procedure completed in {stop - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GomKgX8QLeQM"
   },
   "source": [
    "The following code cells report the **learning** and **metric curves** for the training just finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z97AGJog5-jN"
   },
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "-jdGN4aFIhdn",
    "outputId": "cf27bfbc-4fd8-4563-ca9b-003ac855f847"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5), dpi=100)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Learning curves\", fontsize=14)\n",
    "plt.xlabel(\"Training epochs\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.plot(train.history[\"g_loss\"], color=\"#3288bd\", label=\"generator\")\n",
    "plt.plot(train.history[\"d_loss\"], color=\"#fc8d59\", label=\"discriminator\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Learning rate scheduling\", fontsize=14)\n",
    "plt.xlabel(\"Training epochs\", fontsize=12)\n",
    "plt.ylabel(\"Learning rate\", fontsize=12)\n",
    "plt.plot(train.history[\"g_lr\"], color=\"#3288bd\", label=\"generator\")\n",
    "plt.plot(train.history[\"d_lr\"], color=\"#fc8d59\", label=\"discriminator\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "2id4-AK5J-WW",
    "outputId": "c9c32089-dc45-4852-8afa-68256210c247"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5), dpi=100)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Generator learning curves\", fontsize=14)\n",
    "plt.xlabel(\"Training epochs\", fontsize=12)\n",
    "plt.ylabel(gan.loss_name, fontsize=12)\n",
    "plt.plot(train.history[\"g_loss\"], color=\"#d01c8b\", label=\"training set\")\n",
    "plt.plot(train.history[\"val_g_loss\"], color=\"#4dac26\", label=\"validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Discriminator learning curves\", fontsize=14)\n",
    "plt.xlabel(\"Training epochs\", fontsize=12)\n",
    "plt.ylabel(gan.loss_name, fontsize=12)\n",
    "plt.plot(train.history[\"d_loss\"], color=\"#d01c8b\", label=\"training set\")\n",
    "plt.plot(train.history[\"val_d_loss\"], color=\"#4dac26\", label=\"validation set\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5wv91XH6BDC"
   },
   "source": [
    "#### Metric curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "P8rgOZgpKjPj",
    "outputId": "eb6321be-aecb-4af5-f9c1-788322a3ba5c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "plt.title(\"Metric curves\", fontsize=14)\n",
    "plt.xlabel(\"Training epochs\", fontsize=12)\n",
    "plt.ylabel(\"Wasserstein distance\", fontsize=12)\n",
    "plt.plot(train.history[\"wass_dist\"], color=\"#d01c8b\", label=\"training set\")\n",
    "plt.plot(train.history[\"val_wass_dist\"], color=\"#4dac26\", label=\"validation set\")\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Flows\n",
    "\n",
    "#### Architecture of the Flow Model\n",
    "\n",
    "The flow model in this setup is designed for complex distributions and data transformations. It is composed of several key components:\n",
    "\n",
    "- **Base Distribution**: The model begins with a base distribution, specifically a standard normal distribution with a shape of `[5]`. This serves as the starting point for generating more complex distributions through a series of transformations.\n",
    "\n",
    "- **Transformations**:\n",
    "    - **Random Permutation**: At each layer, a random permutation of the features is applied. This helps in correlating the input data, making the model more flexible.\n",
    "    - **Masked Affine Autoregressive Transform**: This transform applies an affine transformation $y = Wx + c$, allowing the model to scale and shift the data in a complex, learned manner. The autoregressive property ensures that the transformation of each dimension depends only on the previous dimensions, making the Jacobian of the transform more tractable.\n",
    "        - The transform uses 3 blocks with 64 hidden features each, and batch normalization is employed to stabilize and speed up training.\n",
    "\n",
    "- **Composite Transform**: All the individual transforms across layers are combined into a single composite transform. This sequential application of transforms allows the model to learn complex, high-dimensional distributions.\n",
    "\n",
    "- **Flow Model**: The final model, termed 'flow', uses the composite transform to mold the base distribution into the target distribution. It is capable of both forward and backward operations, crucial for tasks like density estimation and generative modeling.\n",
    "\n",
    "- **Optimizer**: An Adam optimizer with a learning rate of 0.001 is used to train the model.\n",
    "\n",
    "The use of autoregressive transforms allows for a powerful and flexible modeling of the data while maintaining computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataPreprocessor(object):\n",
    "    def __init__(self, y_train, x_train, scaler_y=None, scaler_x=None, standardize=True):\n",
    "        \"\"\"\n",
    "        Preprocessor for training data.\n",
    "\n",
    "        :param y_train: Genjet features (originally x_train, but swapped for clarity).\n",
    "        :param x_train: Recojet features (originally y_train, but swapped for clarity).\n",
    "        :param scaler_y: Scaler for y_train.\n",
    "        :param scaler_x: Scaler for x_train.\n",
    "        :param standardize: Boolean indicating whether to standardize the data.\n",
    "        \"\"\"\n",
    "        self.scaler_y = scaler_y\n",
    "        self.scaler_x = scaler_x\n",
    "        self.standardize = standardize\n",
    "        self.Y = y_train\n",
    "        self.X = x_train\n",
    "\n",
    "        # Operations on Y (originally X)\n",
    "        self.Y[:, 0] = self.Y[:, 0] / self.X[:, 0]\n",
    "        self.X[:, 4] = np.abs(self.X[:, 4])\n",
    "\n",
    "        # Smearing of N constituents in Y\n",
    "        self.Y[:, 3] = self.Y[:, 3] + (0.5 * np.random.uniform(-1, 1, len(self.Y[:, 3])))\n",
    "\n",
    "        if self.standardize:\n",
    "            if not self.scaler_y and not self.scaler_x:\n",
    "                self.scaler_y = StandardScaler()\n",
    "                self.scaler_y.fit(self.Y)\n",
    "                self.scaler_x = StandardScaler()\n",
    "                self.scaler_x.fit(self.X)\n",
    "\n",
    "            self.Y = self.scaler_y.transform(self.Y)\n",
    "            self.X = self.scaler_x.transform(self.X)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Get the standardized dataset.\n",
    "\n",
    "        :return: Tuple of standardized Y and X arrays.\n",
    "        \"\"\"\n",
    "        return self.Y, self.X\n",
    "\n",
    "    def invert_standardize(self, Y, X):\n",
    "        \"\"\"\n",
    "        Invert the standardization of the dataset.\n",
    "\n",
    "        :param Y: Standardized genjet features.\n",
    "        :param X: Standardized recojet features.\n",
    "        :return: Tuple of original Y and X arrays.\n",
    "        \"\"\"\n",
    "        Y = self.scaler_y.inverse_transform(Y)\n",
    "        X = self.scaler_x.inverse_transform(X)\n",
    "        return Y, X\n",
    "\n",
    "class TestDataPreprocessor(TrainDataPreprocessor):\n",
    "    \"\"\"\n",
    "    Preprocessor for test data, inherits from TrainDataPreprocessor.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for processing\n",
    "batch_size = 2000\n",
    "\n",
    "# Create preprocessors for training and test data\n",
    "train_dataset = TrainDataPreprocessor(y_train, x_train)\n",
    "Y_train, X_train = train_dataset.get_dataset()\n",
    "\n",
    "test_dataset = TestDataPreprocessor(y_test, x_test, scaler_y=train_dataset.scaler_y, scaler_x=train_dataset.scaler_x)\n",
    "Y_test, X_test = test_dataset.get_dataset()\n",
    "\n",
    "# Send data to device (e.g., GPU)\n",
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "Y_train = torch.tensor(Y_train).float().to(device)\n",
    "\n",
    "# Invert standardization for test data\n",
    "Y_test_cpu, X_test_cpu = test_dataset.invert_standardize(Y_test, X_test)\n",
    "Y_test_cpu[:, 0] = Y_test_cpu[:, 0] * X_test_cpu[:, 0]  # Restore pt distribution\n",
    "\n",
    "X_test = torch.tensor(X_test).float().to(device)\n",
    "Y_test = torch.tensor(Y_test).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flow():\n",
    "    num_layers = 10\n",
    "    base_dist = StandardNormal(shape=[5])\n",
    "    transforms = []\n",
    "\n",
    "    # Constructing layers of the flow model\n",
    "    for _ in range(num_layers):\n",
    "        transforms.append(RandomPermutation(features=5))\n",
    "        transforms.append(MaskedAffineAutoregressiveTransform(\n",
    "            features=5,\n",
    "            use_residual_blocks=False,\n",
    "            num_blocks=3,\n",
    "            hidden_features=64,  # Adjusted from 4, 20 for more capacity\n",
    "            use_batch_norm=True,\n",
    "            context_features=5\n",
    "        ))\n",
    "\n",
    "    # Combining all transforms into a composite transform\n",
    "    transform = CompositeTransform(transforms)\n",
    "\n",
    "    # Creating the flow model with the specified transform and base distribution\n",
    "    flow = Flow(transform, base_dist)\n",
    "\n",
    "    # Optimizer for the flow model (LR DEFINED HERE)\n",
    "    optimizer = optim.Adam(flow.parameters(), lr=0.001)\n",
    "\n",
    "    return flow, optimizer\n",
    "\n",
    "# Building the flow model and setting the device\n",
    "flow, optimizer = build_flow()\n",
    "flow = flow.to(device)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in flow.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "start_epoch = 0\n",
    "epochs=10\n",
    "\n",
    "batch_size = 2048\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start epoch: {start_epoch} End epoch: {epochs}\")\n",
    "train_history = []\n",
    "test_history = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    flow.train()\n",
    "\n",
    "    # Print learning rate with precision\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print_lr = param_group[\"lr\"]\n",
    "        print(f\"Current lr is {print_lr:.8f}\")\n",
    "\n",
    "    train_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "    # Loop over batches with a progress bar\n",
    "    total_batches = (len(Y_train) + batch_size - 1) // batch_size\n",
    "    with tqdm(total=total_batches, desc=\"Training\", dynamic_ncols=True) as pbar:\n",
    "        for i in range(0, len(Y_train), batch_size):\n",
    "            Y_batch = Y_train[i : i + batch_size]\n",
    "            X_batch = X_train[i : i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = -flow.log_prob(inputs=Y_batch, context=X_batch).mean()\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    train_loss /= total_batches\n",
    "    train_history.append(train_loss)\n",
    "\n",
    "    # Test phase\n",
    "    with torch.no_grad():\n",
    "        flow.eval()\n",
    "        test_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "        for i in range(0, len(Y_test), batch_size):\n",
    "            Y_batch = Y_test[i : i + batch_size]\n",
    "            X_batch = X_test[i : i + batch_size]\n",
    "            loss = -flow.log_prob(inputs=Y_batch, context=X_batch).mean()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        test_loss /= total_batches\n",
    "        test_history.append(test_loss)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step(train_loss)\n",
    "    print(f\"Epoch: {epoch} Loss: {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and test loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_history, label='Training Loss')\n",
    "plt.plot(test_history, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting sampling\")\n",
    "flow.eval()\n",
    "samples_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=(len(Y_test) + batch_size - 1) // batch_size, desc=\"Sampling\", dynamic_ncols=True) as pbar:\n",
    "        for i in range(0, len(Y_test), batch_size):\n",
    "            X_batch = X_test[i : i + batch_size]  # Getting context batch\n",
    "\n",
    "            samples = flow.sample(1, context=X_batch)\n",
    "            samples_list.append(samples.detach().cpu().numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "# Processing the sampled data\n",
    "samples = np.concatenate(samples_list, axis=0)\n",
    "samples = samples.reshape((-1, Y_test.shape[1]))\n",
    "samples = test_dataset.scaler_y.inverse_transform(samples)\n",
    "\n",
    "# Apply rounding and adjust pt distribution\n",
    "samples[:, 3] = np.rint(samples[:, 3])\n",
    "samples[:, 0] = samples[:, 0] * X_test_cpu[:, 0]\n",
    "\n",
    "# Clipping to physical values based on Y_test_cpu\n",
    "for i in range(samples.shape[1]):\n",
    "    samples[:, i] = np.clip(samples[:, i], Y_test_cpu[:, i].min(), Y_test_cpu[:, i].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation plots\n",
    "\n",
    "TBA (add description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms_and_ratios(samples, Y_test_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= make_corner(Y_test_cpu, samples, title=\"correlations\") # binning for nConst is broken\n",
    "fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVid9HKfLzPW"
   },
   "outputs": [],
   "source": [
    "x_test_prep = x_scaler.transform(x_test[:, :-1])  # input pre-processing\n",
    "\n",
    "int_encoded = label_encoder.transform(x_test[:, -1])\n",
    "onehot_encoded = onehot_encoder.transform(int_encoded[:, None])  # one-hot encoding\n",
    "\n",
    "x_test_prep = np.concatenate([x_test_prep, onehot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_out_test_prep = gan.generate(x_test_prep, seed=None).numpy()\n",
    "gan_out_test_post = y_scaler.inverse_transform(gan_out_test_prep)  # output post-processing\n",
    "\n",
    "gan_out_test = gan_out_test_post.copy()\n",
    "gan_out_test[:, :3] += x_test[:, :3]  # from errors to reco vars\n",
    "gan_out_test[:, 3] = np.round(gan_out_test[:, 3])  # from continous to discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "XM985aPPNPR3",
    "outputId": "fe6dd452-a9b6-4c74-8cbd-3cfa3eb83742"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5), dpi=100)\n",
    "\n",
    "bins = np.linspace(-50.0, 450.0, 101)\n",
    "\n",
    "for i, scale in enumerate([\"linear\", \"log\"]):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.xlabel(\"Reconstructed $p_T$ [GeV/$c$]\", fontsize=12)\n",
    "    plt.ylabel(\"Candidates\", fontsize=12)\n",
    "    plt.hist(y_test[:, 0], bins=bins, color=\"#3288bd\", label=\"MC data\")\n",
    "    plt.hist(gan_out_test[:, 0], bins=bins, histtype=\"step\", lw=2, color=\"#fc8d59\", label=\"GAN-based model\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=10)\n",
    "    plt.yscale(scale)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "KeRJbgTMPl6g",
    "outputId": "46aec229-1b95-43b3-e905-f284ed733346"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "bins = np.linspace(-2*np.pi, 2*np.pi, 101)\n",
    "\n",
    "plt.xlabel(r\"Reconstructed $\\eta$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_test[:, 1], bins=bins, color=\"#3288bd\", label=\"MC data\")\n",
    "plt.hist(gan_out_test[:, 1], bins=bins, histtype=\"step\", lw=2, color=\"#fc8d59\", label=\"GAN-based model\")\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "rMh4nOK3PyEI",
    "outputId": "b0162757-ef15-418e-b2d6-25bc90d17860"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "bins = np.linspace(0.0, 2*np.pi, 101)\n",
    "\n",
    "plt.xlabel(r\"Reconstructed $\\varphi$\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_test[:, 2], bins=bins, color=\"#3288bd\", label=\"MC data\")\n",
    "plt.hist(gan_out_test[:, 2], bins=bins, histtype=\"step\", lw=2, color=\"#fc8d59\", label=\"GAN-based model\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "z3WQRODrQE7H",
    "outputId": "f7bd9d63-a7f7-4d93-a9c4-7af53f97489e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "bins = np.linspace(0.0, 50.0, 101)\n",
    "\n",
    "plt.xlabel(\"Number of particles within the jet\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_test[:, 3], bins=bins, color=\"#3288bd\", alpha=0.6, label=\"MC data\")\n",
    "plt.hist(gan_out_test[:, 3], bins=bins, color=\"#fc8d59\", alpha=0.6, label=\"GAN-based model\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "bins = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "plt.xlabel(\"$b$-tagging\", fontsize=12)\n",
    "plt.ylabel(\"Candidates\", fontsize=12)\n",
    "plt.hist(y_test[:, 4], bins=bins, color=\"#3288bd\", label=\"MC data\")\n",
    "plt.hist(gan_out_test[:, 4], bins=bins, histtype=\"step\", lw=2, color=\"#fc8d59\", label=\"GAN-based model\")\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKqBzg-Fi5xM"
   },
   "source": [
    "## References\n",
    "\n",
    "1. I.J. Goodfellow _et al._, \"Generative Adversarial Networks\", [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)\n",
    "2. M. Arjovsky, L. Bottou, \"Towards Principled Methods for Training Generative Adversarial Networks\", [arXiv:1701.04862](https://arxiv.org/abs/1701.04862)\n",
    "3. X. Mao _et al._, \"Least Squares Generative Adversarial Networks\", [arXiv:1611.04076](https://arxiv.org/abs/1611.04076)\n",
    "4. M. Arjovsky, S. Chintala, L. Bottou, \"Wasserstein GAN\", [arXiv:1701.07875](https://arxiv.org/abs/1701.07875)\n",
    "5. I. Gulrajani _et al._, \"Improved Training of Wasserstein GANs\", [arXiv:1704.00028](https://arxiv.org/abs/1704.00028)\n",
    "6. M.G. Bellemare _et al._, \"The Cramer Distance as a Solution to Biased Wasserstein Gradients\", [arXiv:1705.10743](https://arxiv.org/abs/1705.10743)\n",
    "7. D. Terjék, \"Adversarial Lipschitz Regularization\", [arXiv:1907.05681](https://arxiv.org/abs/1907.05681)\n",
    "8. T. Salimans _et al._, \"Improved Techniques for Training GANs\", [arXiv:1606.03498](https://arxiv.org/abs/1606.03498)\n",
    "9. M. Mirza, S. Osindero, \"Conditional Generative Adversarial Nets\", [arXiv:1411.1784](https://arxiv.org/abs/1411.1784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f26wCo4i5xN"
   },
   "source": [
    "## Credits\n",
    "This hands-on is based on the tutorials provided by the [pidgan](https://github.com/mbarbetti/pidgan) package."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Generative models",
   "language": "python",
   "name": "gen-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
