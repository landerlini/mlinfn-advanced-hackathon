{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d423bb-e164-4cd2-9f3e-ca3e265b3925",
   "metadata": {},
   "source": [
    "# PointNet for particle flow\n",
    "\n",
    "<div class=\"alert alert-block alert-succes\">\n",
    "    \n",
    "This notebook focuses on wandb Artifacts and how they can be used for data and model versioning.\n",
    "\n",
    "**Main changes:** \n",
    "\n",
    "- introduce wandb artifacts for data versions\n",
    "- log artifacts to wandb UI\n",
    "- retrieve artifact from UI as input for processing\n",
    "\n",
    "</div>\n",
    "\n",
    "## Problem\n",
    "\n",
    "This dataset contains a Monte Carlo simulation of $\\rho^{\\pm} \\rightarrow \\pi^{\\pm} + \\pi^0$ decays and the corresponding detector response. Specifically, the data report the measured response of **i) tracker** and **ii) calorimeter**, along with the true pyshical quantitites that generated those measurements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This means that we expect one track per event, with mainly two energy blobs (clusters of cells) in the calorimeter.\n",
    "</div>\n",
    "\n",
    "The final **goal** is to associate the cell signals observed in the calorimeter to the track that caused those energy deposits.\n",
    "\n",
    "## Method\n",
    "\n",
    "The idea is to leverage a **point cloud** data representation to combine tracker and calorimeter information so to associate cell hits to the corresponding track. We will use a [**PointNet**](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) model that is capable of handling this type of data, framed as a **semantic segmentation** approach. More precisely, this means that:\n",
    "- we represent each hit in the detector as a point in the point cloud: x, y, z coordinates + additional features (\"3+\"-dimensional point)\n",
    "- the **learning task** will be binary classification at hit level: for each cell the model learns whether its energy comes mostly from the track (class 1) or not (class 0)\n",
    "\n",
    "## Data structure\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This dataset is organized as follows:\n",
    " - for each event, we create a **sample** (i.e. point cloud)\n",
    " - each sample contains all hits in a cone around a track of the event, called **focal track**\n",
    "     - the cone includes all hits within some $\\Delta R$ distance of the track\n",
    "     - if an event has multiple tracks, then we have more samples per event\n",
    "     - since different samples have possibly different number of hits, **we pad all point clouds to ensure they have same size** (needed since the model requires inputs of same size)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576b6bf-eb45-4742-8f24-1023d6e0cc15",
   "metadata": {},
   "source": [
    "## Settings & config\n",
    "\n",
    "This section collects all configuration variables and training/model hyperparameters. \n",
    "\n",
    "The idea is to put it at the top so that it is easy to find and edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c725801-a326-486d-97ca-5a165b2bbb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 11:04:49.837243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-26 11:04:52.025666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path settings\n",
    "REPO_BASEPATH = Path().cwd().parent\n",
    "DATA_PATH = REPO_BASEPATH / \"pnet_data/raw/rho_small.npz\"\n",
    "CODE_PATH = REPO_BASEPATH / \"src\"\n",
    "sys.path.append(str(CODE_PATH))\n",
    "MODEL_CHECKPOINTS_PATH = REPO_BASEPATH / \"results\" / \"models\" / \"pointnet_baseline.weights.h5\"\n",
    "\n",
    "import wandb\n",
    "from data_viz import *\n",
    "from model_utils import *\n",
    "\n",
    "LABELS = [\"unfocus hit\", \"focus hit\"]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 18\n",
    "set_global_seeds(SEED)\n",
    "\n",
    "# data settings\n",
    "N_TRAIN, N_VAL, N_TEST = 210, 65, 50 # roughly 0.65, 0.2, 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37da7e-33cf-40f2-8bce-f88b092f0ff7",
   "metadata": {},
   "source": [
    "## wandb Artifacts\n",
    "\n",
    "Weights & Biases use `Artifacts` as a tool to store objects we want to track and version. Artifacts are typically inputs or outputs of runs, so they are particularly useful for data and models. \n",
    "\n",
    "    By linking artifacts with runs, it is also possible to track how/when those artifacts were created and when they were used. \n",
    "\n",
    "In brief, artifacts can be handled with a few useful commands:\n",
    "\n",
    "```python\n",
    "# create an artifact for dataset\n",
    "artifact = wandb.Artifact(name = \"example_artifact\", type = \"dataset\")\n",
    "artifact.add_file(local_path = \"./dataset.h5\", name = \"training_dataset\")\n",
    "artifact.save()\n",
    "\n",
    "# reference dataset version used for this experiment\n",
    "artifact = run.use_artifact(\"training_dataset:latest\") #returns a run object using the \"my_data\" artifact\n",
    "\n",
    "# actually download the data\n",
    "datadir = artifact.download() #downloads the full \"my_data\" artifact to the default directory.\n",
    "\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Do **not need to log full dataset!** Data hash is also fine, the objective is mainly to ensure versioning and reproducibility!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7f914-749b-465a-ac4e-1dfd80b882f3",
   "metadata": {},
   "source": [
    "## Create `Artifact` for raw data\n",
    "\n",
    "Initially we can simply track raw data and how we split them into training, validation and test datasets. \n",
    "\n",
    "As before, we choose 65%, 20%, 15% fractions for training, validation and testing data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7873b320-dee0-4c09-82d4-c128727a0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlclissa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/private/mlops-handson/notebooks/wandb/run-20241126_110456-3xpz7w34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/3xpz7w34' target=\"_blank\">dataset-logging</a></strong> to <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/3xpz7w34' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/3xpz7w34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dataset-logging</strong> at: <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/3xpz7w34' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/3xpz7w34</a><br/> View project at: <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241126_110456-3xpz7w34/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"mlops-ai_infn\", entity=\"lclissa\", name=\"dataset-logging\",\n",
    "                job_type=\"data-creation\", config={'DATA_PATH': DATA_PATH,'seed': SEED},\n",
    "                notes=\"Playing with Artifacts ...\") as run:\n",
    "    \n",
    "    # create artifact for raw data\n",
    "    raw_data_artifact = wandb.Artifact(name=\"raw_data\", type=\"dataset\", \n",
    "                              description=\"MC simulation of rho -> pions decays (full data)\"\n",
    "    )\n",
    "    raw_data_artifact.add_file(local_path = str(DATA_PATH), name=\"rho_small.npz\")\n",
    "    wandb.log_artifact(raw_data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03d19b-bd7a-4dd2-be10-dcfdf37d0c9c",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "Once we have our raw data artifact, we can track all processing it is subjected to through wandb.\n",
    "\n",
    "The key is to to either download or just reference it so that wandb knows what artifact is used as input and can track the outputs.\n",
    "\n",
    "Note that Artifacts can store metadata, which is very useful to document how artifacts were created and should be used wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb8d748-ee16-4543-ba61-78300865335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/private/mlops-handson/notebooks/wandb/run-20241126_110502-z8j9biaj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/z8j9biaj' target=\"_blank\">dataset-splitting</a></strong> to <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/z8j9biaj' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/z8j9biaj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dataset-splitting</strong> at: <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/z8j9biaj' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/z8j9biaj</a><br/> View project at: <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241126_110502-z8j9biaj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data splitting\n",
    "def create_and_save_artifact_locally(data, name, desc, meta={}):\n",
    "    data_artifact = wandb.Artifact(name=name, type=\"dataset\", \n",
    "                                     description=desc, metadata=meta)\n",
    "    outpath = DATA_PATH.parent.parent / name\n",
    "    outpath.mkdir(exist_ok=True, parents=True)\n",
    "    outname = str(outpath / DATA_PATH.name)\n",
    "    np.savez(outname, feats=data)\n",
    "    data_artifact.add_file(outname)\n",
    "    # with data_artifact.new_file(outname, mode=\"w\") as file:\n",
    "    #     np.savez(outname, data)\n",
    "    return data_artifact\n",
    "\n",
    "def split(events, n_train, n_val):\n",
    "    all_idx = [*range(events.shape[0])]\n",
    "    \n",
    "    train_idx = np.random.choice(all_idx, n_train, replace=False)\n",
    "    remaining_idx = np.array(list(set(all_idx).difference(train_idx)))\n",
    "    val_idx = np.random.choice(remaining_idx, n_val, replace=False)\n",
    "    test_idx = np.array(list(set(remaining_idx).difference(val_idx)))\n",
    "    \n",
    "    return train_idx, val_idx, test_idx \n",
    "    \n",
    "with wandb.init(project=\"mlops-ai_infn\", entity=\"lclissa\", name=\"dataset-splitting\",\n",
    "                job_type=\"data-split\", config={'DATA_PATH': DATA_PATH,'seed': SEED},\n",
    "                notes=\"Playing with Artifacts ...\") as run:\n",
    "    \n",
    "    # reference data artifact as input of our run\n",
    "    raw_data_artifact = run.use_artifact('raw_data:latest')\n",
    "    \n",
    "    # optionally, we can also download data from wandb\n",
    "    # note: this does not repeat download if already available locally\n",
    "    data_dir = raw_data_artifact.download(root=DATA_PATH.parent)\n",
    "    events = np.load(Path(data_dir) / DATA_PATH.name)[\"feats\"]\n",
    "\n",
    "    # split data\n",
    "    train_idx, val_idx, test_idx = split(events, N_TRAIN, N_VAL)\n",
    "    train_data = events[train_idx, :]\n",
    "    val_data = events[val_idx, :]\n",
    "    test_data = events[test_idx, :]\n",
    "\n",
    "    \n",
    "    # create new artifacts for train, validation and test datasets\n",
    "    meta_dict = {'n_train': N_TRAIN, 'n_val': N_VAL}\n",
    "    train_data_artifact = create_and_save_artifact_locally(\n",
    "        train_data, name=\"train_data\", desc=\"training data\", meta=meta_dict)\n",
    "    val_data_artifact = create_and_save_artifact_locally(\n",
    "        val_data, name=\"val_data\", desc=\"validation data\", meta=meta_dict)\n",
    "    test_data_artifact = create_and_save_artifact_locally(\n",
    "        test_data, name=\"test_data\", desc=\"test data\", meta=meta_dict)\n",
    "    \n",
    "    wandb.log_artifact(train_data_artifact)\n",
    "    wandb.log_artifact(val_data_artifact)\n",
    "    wandb.log_artifact(test_data_artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488aaf02-df88-4c86-afe4-99490e0aad62",
   "metadata": {},
   "source": [
    "## Versioning artifacts\n",
    "\n",
    "In ML projects, we ofter iterate over several times, attempting different preprocessing steps, random split seeds, or feature engineering approaches. \n",
    "Creating entirely new artifacts for our data every time we apply a change would quickly end up with a tone of datasets that are difficult to track and navigate. Instead of doing so, we can leverage W&B artifact versioning to keep conceptually related artifacts together while allowing intuitive tracking and lineage.\n",
    "\n",
    "### Why is it useful?\n",
    "\n",
    "- Maintains clear data lineage and provenance tracking\n",
    "- Makes it easy to reproduce experiments by referencing specific versions\n",
    "- Reduces storage overhead by only tracking changes between versions\n",
    "- Enables easy comparison between different preprocessing approaches\n",
    "- Simplifies rolling back to previous versions if needed\n",
    "- Helps team collaboration by providing a single source of truth with version history\n",
    "\n",
    "### Example: Creating a new version of a dataset artifact\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb run\n",
    "run = wandb.init(project=\"artifact_demo\")\n",
    "\n",
    "# link run to the artifact we want versioning for\n",
    "old_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "\n",
    "# Apply changes and save somewhere, say \"path/to/processed_data\"\n",
    "\n",
    "# Create a new artifact with same name of existing artifact\n",
    "artifact = wandb.Artifact(\"raw_data\", type=\"dataset\")\n",
    "\n",
    "# Add files or data to the artifact\n",
    "artifact.add_file(\"path/to/processed_data\")\n",
    "\n",
    "# Log the artifact - W&B will automatically create a new version\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Later you can reference specific versions using :v0, :v1, etc.\n",
    "# Example: artifact = run.use_artifact('raw_data:v1')\n",
    "```\n",
    "\n",
    "**Note**: Each new version gets an incremental version number (v0, v1, v2, etc.). You can also use aliases like 'latest' to always get the most recent version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbdd28-7d41-41b2-9cc9-fcf3ddf17a70",
   "metadata": {},
   "source": [
    "## Artifact recap\n",
    "\n",
    "Artifacts are useful to store anything that can be seen as input/output of our experiments. Hence, this is particularly useful for:\n",
    " - datasets\n",
    " - models\n",
    "\n",
    "A nice feature is that we can inspect the artifacts' lineage from the wandb UI, as well as track metadata. Also, wandb  takes care of automatically versioning artifacts, so that we have all tools to make sure our results are reproducible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
