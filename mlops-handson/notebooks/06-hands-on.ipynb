{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d423bb-e164-4cd2-9f3e-ca3e265b3925",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# MLOps\n",
    "\n",
    "Now that you've seen the individual components of an MLOps workflow with W&B, it's time to apply everything you've learned in a complete pipeline. You'll combine data versioning, experiment tracking, and hyperparameter optimization in a single comprehensive exercise.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Building on what we covered in previous notebooks, implement a complete MLOps pipeline that:\n",
    "\n",
    "1. **Data Management**\n",
    "   - Filter the dataset to keep only single-track events\n",
    "   - Create versioned artifacts (`train_data:v1` and `val_data:v1`) with \"one-track\" alias, `run.log_artifact(, aliases=[..., \"one-track\"])`\n",
    "   - Document your data processing decisions\n",
    "\n",
    "2. **Training Pipeline**\n",
    "   - Use sample_weights argument of model.fit (ensure dataset is created appropriately, i.e. x, y, sample_weigths format)\n",
    "   - Use proper experiment tracking (metrics, gradients, model checkpoints)\n",
    "   - Save model versions\n",
    "\n",
    "3. **Optimization**\n",
    "   - Design and execute a sweep of your choice\n",
    "   - Must include at least 3 hyperparameters to optimize\n",
    "   - Analyze and document the results\n",
    "\n",
    "**Feel free to explore W&B documentation to use more advanced features!** \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# PointNet for particle flow\n",
    "\n",
    "## Problem\n",
    "\n",
    "This dataset contains a Monte Carlo simulation of $\\rho^{\\pm} \\rightarrow \\pi^{\\pm} + \\pi^0$ decays and the corresponding detector response. Specifically, the data report the measured response of **i) tracker** and **ii) calorimeter**, along with the true pyshical quantitites that generated those measurements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This means that we expect one track per event, with mainly two energy blobs (clusters of cells) in the calorimeter.\n",
    "</div>\n",
    "\n",
    "The final **goal** is to associate the cell signals observed in the calorimeter to the track that caused those energy deposits.\n",
    "\n",
    "## Method\n",
    "\n",
    "The idea is to leverage a **point cloud** data representation to combine tracker and calorimeter information so to associate cell hits to the corresponding track. We will use a [**PointNet**](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) model that is capable of handling this type of data, framed as a **semantic segmentation** approach. More precisely, this means that:\n",
    "- we represent each hit in the detector as a point in the point cloud: x, y, z coordinates + additional features (\"3+\"-dimensional point)\n",
    "- the **learning task** will be binary classification at hit level: for each cell the model learns whether its energy comes mostly from the track (class 1) or not (class 0)\n",
    "\n",
    "## Data structure\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This dataset is organized as follows:\n",
    " - for each event, we create a **sample** (i.e. point cloud)\n",
    " - each sample contains all hits in a cone around a track of the event, called **focal track**\n",
    "     - the cone includes all hits within some $\\Delta R$ distance of the track\n",
    "     - if an event has multiple tracks, then we have more samples per event\n",
    "     - since different samples have possibly different number of hits, **we pad all point clouds to ensure they have same size** (needed since the model requires inputs of same size)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576b6bf-eb45-4742-8f24-1023d6e0cc15",
   "metadata": {},
   "source": [
    "## Settings & config\n",
    "\n",
    "This section collects all configuration variables and training/model hyperparameters. \n",
    "\n",
    "The idea is to put it at the top so that it is easy to find and edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c725801-a326-486d-97ca-5a165b2bbb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 16:27:40.286415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 16:27:42.224175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path settings\n",
    "REPO_BASEPATH = Path().cwd().parent\n",
    "DATA_PATH = REPO_BASEPATH / \"pnet_data/raw/rho_small.npz\"\n",
    "CODE_PATH = REPO_BASEPATH / \"src\"\n",
    "sys.path.append(str(CODE_PATH))\n",
    "MODEL_CHECKPOINTS_PATH = REPO_BASEPATH / \"results\" / \"models\" / \"pointnet_baseline.weights.h5\"\n",
    "\n",
    "import wandb\n",
    "from data_viz import *\n",
    "from model_utils import *\n",
    "\n",
    "LABELS = [\"unfocus hit\", \"focus hit\"]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 18\n",
    "set_global_seeds(SEED)\n",
    "\n",
    "# data settings\n",
    "N_TRAIN, N_VAL, N_TEST = 210, 65, 50 # roughly 0.65, 0.2, 0.15\n",
    "\n",
    "# model settings\n",
    "N_FEATURES = 3\n",
    "INIT_SIZE = 8\n",
    "END_SIZE = 16\n",
    "\n",
    "# training settings\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "INIT_LR = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54eb0a-6709-42c1-8f92-7b1b84e7d9de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model training\n",
    "\n",
    "We proceed with model training:\n",
    "\n",
    "1. split the data\n",
    "1. build our PointNet model using Tensorflow/Keras\n",
    "1. create a dataloader to feed batches into our model\n",
    "1. train\n",
    "1. check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776985d-5603-4049-b25b-92462ac5c187",
   "metadata": {},
   "source": [
    "### PointNet model \n",
    "\n",
    "We use a PointNet model for semantic segmentation. Here is an illustration of its structure:\n",
    "\n",
    "![PointNet architecture](../pnet_data/images/pointnet-architecture.jpg)\n",
    "\n",
    "We have two heads:\n",
    " - classification head (used for point cloud classification)\n",
    " - segmentation head (used for semantic segmentation)\n",
    "\n",
    "We are going to use the **segmentation head** for our problem. The architecture settings we can experiment with are:\n",
    " - `n_features` (the number of input features): original version has only size 3 as it only takes x,y,z coordinates\n",
    " - `init_size` (number of filters of first convolutional layer): original version has 64\n",
    " - `end_size` (number of filters in segmentation head): original version has 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e89ffc-4518-4c47-948c-b91c8bd16e52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Utils for new settings\n",
    "\n",
    "You can try to implement new features alone, but you can check out some reference implementations in case you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb63250a-2125-478d-bf3e-a9ee39e0657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_track_filter(data, labels):\n",
    "#     multiple_samples_event_ids = [826871, 827140, 827188, 827226, 827242, 828437]\n",
    "    \n",
    "#     one_track_mask = ~np.any(np.isin(train_data['event_number'], multiple_samples_event_ids), axis=1)\n",
    "#     filtered_data = data[one_track_mask]\n",
    "#     filtered_labels = labels[one_track_mask]\n",
    "#     return filtered_data, filtered_labels\n",
    "\n",
    "# def augment(point_cloud_batch, label_cloud_batch):\n",
    "#     noise = tf.random.uniform(\n",
    "#         tf.shape(point_cloud_batch[:, :, :3]), -0.001, 0.001, dtype=tf.float64\n",
    "#     )\n",
    "\n",
    "#     noisy_xyz = point_cloud_batch[:, :, :3] + noise\n",
    "#     point_cloud_batch = tf.concat([noisy_xyz, point_cloud_batch[:, :, 3:]], axis=-1)\n",
    "    \n",
    "#     return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "# def get_coords_labels_weights(point_cloud_batch, label_cloud_batch):\n",
    "#     category_mask = point_cloud_batch[:,:,3]\n",
    "#     # assign weight=1 only to cell hits (category=1)\n",
    "#     weights = tf.cast(tf.equal(category_mask, 1), tf.float32)\n",
    "#     return point_cloud_batch[:,:,:3], label_cloud_batch, weights\n",
    "    \n",
    "# def generate_dataset(point_clouds, label_clouds, is_training=True, bs=16, n_points=800, n_features=3, labels=[\"unfocus hit\", \"focus hit\"]):\n",
    "#     # reformat to unstructured array and transform to list of size n_samples, each element of size n_ponints x n_features\n",
    "#     point_clouds = structured_to_unstructured(point_clouds).astype(np.float64)\n",
    "#     point_clouds = [_ for _ in point_clouds]\n",
    "    \n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n",
    "#     dataset = dataset.shuffle(bs * 100) if is_training else dataset\n",
    "#     load_data_with_args = partial(load_data, n_points=n_points, n_features=n_features, labels=labels)\n",
    "#     dataset = dataset.map(load_data_with_args, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.batch(batch_size=bs)\n",
    "#     dataset = (\n",
    "#         dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#         if is_training\n",
    "#         else dataset\n",
    "#     )\n",
    "#     dataset = dataset.map(get_coords_labels_weights, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd2fbe-ac90-41cb-98c4-4b0c5b74a5a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filtering and versioning\n",
    "\n",
    "First step is to filter multi-track events out and update the data artifacts.\n",
    "    \n",
    "```python\n",
    "def filter_and_version(outpath):\n",
    "\n",
    "    with wandb.init(..., job_type=\"preproc\", notes=\"Filtering out multi-track events\") as run:\n",
    "        \n",
    "        # filter data\n",
    "        run.use_artifact(\"train_data:latest\")\n",
    "        run.use_artifact(\"val_data:latest\")\n",
    "\n",
    "        # read train/val datasets\n",
    "        # apply filtering\n",
    "        # save filtered data locally\n",
    "        # create new artifacts with aliases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a664a27-66fd-4f3b-8cbb-e4a450a741a7",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now you need to implement the training function to be passed to the sweep. A basic structure should be the following:\n",
    "\n",
    "```python\n",
    "def train_wrapper():\n",
    "    # initialize session and get config\n",
    "    run = wandb.init(...)\n",
    "    cfg = run.config\n",
    "\n",
    "    # reference old data and create new versions\n",
    "    _ = run.use_artifact(...)\n",
    "    \n",
    "    # include category among inputs so that it can be used for computing weights \n",
    "    input_features = [...]\n",
    "\n",
    "    train_data, train_label_cloud = read_data(\"train\", split_data_path=SPLIT_DATA_PATH)\n",
    "    # filter and create new version: train\n",
    "    \n",
    "    train_point_clouds = train_data[input_features]\n",
    "    total_training_examples = len(train_point_clouds)\n",
    "    \n",
    "    val_data, val_label_cloud = read_data(\"val\", split_data_path=SPLIT_DATA_PATH)\n",
    "    # filter and create new version: validation\n",
    "    \n",
    "    val_point_clouds = val_data[input_features]\n",
    "    \n",
    "    print(\"Num train point clouds:\", len(train_point_clouds))\n",
    "    print(\"Num train point cloud labels:\", len(train_label_cloud))\n",
    "    print(\"Num val point clouds:\", len(val_point_clouds))\n",
    "    print(\"Num val point cloud labels:\", len(val_label_cloud))\n",
    "    \n",
    "    n_points = train_point_clouds[0].shape[0]\n",
    "    n_features = len(train_point_clouds[0].dtype.names)\n",
    "    n_classes = len(LABELS)\n",
    "\n",
    "    # now datasets are in format x, y, sample_weights\n",
    "    train_dataset = generate_dataset(train_point_clouds, train_label_cloud, \n",
    "                                 bs=cfg.batch_size, n_points=n_points, n_features=n_features, labels=LABELS)\n",
    "    val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False, \n",
    "                                   bs=cfg.batch_size, n_points=n_points, n_features=n_features, labels=LABELS)\n",
    "\n",
    "    \n",
    "    # start training\n",
    "    steps_per_epoch = total_training_examples // cfg.batch_size\n",
    "    total_training_steps = steps_per_epoch * EPOCHS\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=cfg.init_lr,\n",
    "        decay_steps=steps_per_epoch * 5,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    # update model creation as we have one more feature\n",
    "    segmentation_model = get_shape_segmentation_model(...)\n",
    "    segmentation_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "        jit_compile=False\n",
    "    )\n",
    "\n",
    "    MODEL_CHECKPOINTS_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # NOTE: same code but now datasets are in format x, y, sample_weights\n",
    "    history = segmentation_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger(log_freq=5),\n",
    "            WandbModelCheckpoint(\n",
    "                       MODEL_CHECKPOINTS_PATH, #.parent / \"model-{epoch:02d}-{val_loss:.2f}.weights.h5\",\n",
    "                       monitor=\"val_loss\",\n",
    "                       save_best_only=True,\n",
    "                       save_weights_only=True,\n",
    "                   )\n",
    "        ],\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0b3fc-14ac-4626-939e-760bc9ef4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_PATH = DATA_PATH.parent.parent\n",
    "NEW_SPLIT_PATH = SPLIT_DATA_PATH / \"one-track\"\n",
    "NEW_SPLIT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def read_data(split, bin_cutoff=0.5, n_classes=2, split_data_path=SPLIT_DATA_PATH): \n",
    "    filepath=str(split_data_path / f\"{split}_data\" / DATA_PATH.name)\n",
    "    data = np.load(filepath)['feats']\n",
    "    target_class = [(energy_fraction > bin_cutoff).astype(np.float32) \n",
    "                    for energy_fraction in data['truth_cell_focal_fraction_energy']]\n",
    "    # target_class = (events[\"truth_cell_focal_fraction_energy\"] > 0.5).reshape(-1)\n",
    "    target_class = keras.utils.to_categorical(target_class, num_classes=n_classes)\n",
    "    return data, target_class\n",
    "\n",
    "def filter_and_version(split_data_path=NEW_SPLIT_PATH):\n",
    "\n",
    "    with wandb.init(project=\"mlops-ai_infn\", entity=\"lclissa\", \n",
    "                job_type=\"preproc\", notes=\"Filtering out multi-track events\") as run:\n",
    "        # filter data\n",
    "        run.use_artifact(\"train_data:latest\")\n",
    "        run.use_artifact(\"val_data:latest\")\n",
    "        \n",
    "        train_data, train_label_cloud = read_data(\"train\", split_data_path=SPLIT_DATA_PATH)\n",
    "        filtered_train_data, filtered_train_label_cloud = one_track_filter(train_data, train_label_cloud)\n",
    "        # save locally\n",
    "        filepath=str(split_data_path / \"train_data\" / DATA_PATH.name)\n",
    "        np.savez(filepath, feats=filtered_train_data)\n",
    "        # create new artifact tracking new version\n",
    "        filtered_train_artifact = wandb.Artifact(name=\"train_data\", type=\"dataset\", description=\"One track only\")\n",
    "        filtered_train_artifact.add_file(local_path = str(filepath))\n",
    "        run.log_artifact(filtered_train_artifact, aliases=[\"one-track\"])\n",
    "        \n",
    "        train_point_clouds = train_data[input_features]\n",
    "        total_training_examples = len(train_point_clouds)\n",
    "        \n",
    "        val_data, val_label_cloud = read_data(\"val\", split_data_path=SPLIT_DATA_PATH)\n",
    "        filtered_val_data, filtered_val_label_cloud = one_track_filter(val_data, val_label_cloud)\n",
    "        # save locally\n",
    "        filepath=str(split_data_path / \"val_data\" / DATA_PATH.name)\n",
    "        np.savez(filepath, feats=filtered_val_data)\n",
    "        # create new artifact tracking new version\n",
    "        filtered_val_artifact = wandb.Artifact(name=\"val_data\", type=\"dataset\", description=\"One track only\")\n",
    "        filtered_val_artifact.add_file(local_path = str(filepath))\n",
    "        run.log_artifact(filtered_val_artifact, aliases=[\"one-track\"])\n",
    "\n",
    "print(f\"Creating new data version at: {NEW_SPLIT_PATH}\")\n",
    "filter_and_version(NEW_SPLIT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
