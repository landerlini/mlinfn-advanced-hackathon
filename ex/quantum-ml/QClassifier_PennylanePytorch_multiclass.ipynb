{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "I9GFpzXfj2Nw",
   "metadata": {
    "id": "I9GFpzXfj2Nw"
   },
   "source": [
    "# A QNN based on Parametric Quantum Circuit for classification of MNIST images\n",
    "## Tutorial - Hands-on ##\n",
    "\n",
    "**Version:** V2.0 <p>\n",
    "**Authors:** Stefano Giagu <stefano.giagu@uniroma1.it>\n",
    "\n",
    "\n",
    "**Scope:**: learn how to design a simple variational PQC and train it for a multiclass classification task using the [pennylane](https://pennylane.ai/) platform with [pytorch](https://pytorch.org/) backend\n",
    "\n",
    "**Libraries:** numpy, matplotlib, pennylane, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0TwhFM53nJks",
   "metadata": {
    "id": "0TwhFM53nJks"
   },
   "outputs": [],
   "source": [
    "# only needed on google colab\n",
    "#!pip install pennylane-lightning-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHc_e-ppmfLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHc_e-ppmfLO",
    "outputId": "53f70a7b-fafa-4369-a119-61b707eccaec"
   },
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pennylane as qml\n",
    "print('Torch version: ', torch.__version__)\n",
    "print('Pennylane version: ', qml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09PFfA5VmTwr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09PFfA5VmTwr",
    "outputId": "56d9667d-a7f9-480a-d895-5276ba6683ea"
   },
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "# Note: the example can also be run on CPU w/o problems\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print('Number of available GPUs: ',torch.cuda.device_count())\n",
    "  for i in range(0,torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "  !nvidia-smi\n",
    "else:\n",
    "  print('No GPU available')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d5db8-ef0f-4cbd-a913-87a4b2b416db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d0d5db8-ef0f-4cbd-a913-87a4b2b416db",
    "outputId": "22a9d2ec-1de3-476b-8b35-ea481b2bef34"
   },
   "outputs": [],
   "source": [
    "# check Pennylane software stack\n",
    "qml.about()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db408de6-b399-4bbf-90f2-1162e7472e69",
   "metadata": {
    "id": "db408de6-b399-4bbf-90f2-1162e7472e69"
   },
   "outputs": [],
   "source": [
    "# Download MNIST dataset (from torchvision repository)\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download MNIST and prepare transforms\n",
    "# we donwsample MNIST images from 28x28 to 16x16 pixels in order to reduce the number of qubit needed to represent each image\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),                # Resize to 16x16\n",
    "                                transforms.ToTensor(),                      # Convert to torch tensors\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "\n",
    "\n",
    "# Filter for 0,1,2,3 digits (in the example we train the QNN for a multiclass classification task, classification of 0,1,2 and 3 digits)\n",
    "data = []\n",
    "targets = []\n",
    "for image, label in mnist_train:\n",
    "    if label in [0, 1, 2, 3]:\n",
    "        data.append(image.squeeze())\n",
    "        targets.append(label)\n",
    "\n",
    "data = torch.stack(data)\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "# Select 0,1,2,3 to implement a simpler multiclass classification problem\n",
    "zeros_indices = (targets == 0)\n",
    "ones_indices = (targets == 1)\n",
    "twos_indices = (targets == 2)\n",
    "threes_indices = (targets == 3)\n",
    "\n",
    "zeros = data[zeros_indices]\n",
    "ones = data[ones_indices]\n",
    "twos = data[twos_indices]\n",
    "threes = data[threes_indices]\n",
    "\n",
    "# take a subsample of the dataset to imit the training time: 1024 0,1,2, and 3s for training and an equal size set for test\n",
    "zeros_train = zeros[:1024]\n",
    "ones_train = ones[:1024]\n",
    "twos_train = twos[:1024]\n",
    "threes_train = threes[:1024]\n",
    "\n",
    "zeros_test = zeros[1024:2048]\n",
    "ones_test = ones[1024:2048]\n",
    "twos_test = twos[1024:2048]\n",
    "threes_test = threes[1024:2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5de814-90ae-4294-9f5a-ae0a717d1e22",
   "metadata": {
    "id": "fc5de814-90ae-4294-9f5a-ae0a717d1e22"
   },
   "outputs": [],
   "source": [
    "# normalize images in [0,1]\n",
    "\n",
    "def normalize(imgs):\n",
    "  maxes, _ = torch.max(imgs.reshape(-1, 16*16), dim = 1)\n",
    "  mins, _ = torch.min(imgs.reshape(-1, 16*16), dim = 1)\n",
    "\n",
    "  mins = mins.unsqueeze(1).unsqueeze(2)\n",
    "  maxes = maxes.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "  return (imgs-mins)/(maxes-mins)\n",
    "\n",
    "zeros_train = normalize(zeros_train)\n",
    "ones_train = normalize(ones_train)\n",
    "twos_train = normalize(twos_train)\n",
    "threes_train = normalize(threes_train)\n",
    "\n",
    "zeros_test = normalize(zeros_test)\n",
    "ones_test = normalize(ones_test)\n",
    "twos_test = normalize(twos_test)\n",
    "threes_testn = normalize(threes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e96272-483d-46cf-8dd8-19735352615f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "15e96272-483d-46cf-8dd8-19735352615f",
    "outputId": "e145b338-5651-4527-9422-fb737c62e74d"
   },
   "outputs": [],
   "source": [
    "# plot two examples of the input images\n",
    "\n",
    "# Select a random sample index\n",
    "zero_idx = np.random.randint(0, zeros_train.shape[0])\n",
    "one_idx = np.random.randint(0, ones_train.shape[0])\n",
    "\n",
    "# Extract the images\n",
    "sample_zero = zeros_train[zero_idx]\n",
    "sample_one = ones_train[one_idx]\n",
    "sample_twos = twos_train[zero_idx]\n",
    "sample_threes = threes_train[one_idx]\n",
    "\n",
    "# Plot the images\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.imshow(sample_zero, cmap='gray')\n",
    "ax1.set_title('Zero')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(sample_one, cmap='gray')\n",
    "ax2.set_title('One')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3.imshow(sample_twos, cmap='gray')\n",
    "ax3.set_title('Two')\n",
    "ax3.axis('off')\n",
    "\n",
    "ax4.imshow(sample_threes, cmap='gray')\n",
    "ax4.set_title('Three')\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0395d-70ac-47a4-b020-a9f0634e5af4",
   "metadata": {
    "id": "bfe0395d-70ac-47a4-b020-a9f0634e5af4"
   },
   "outputs": [],
   "source": [
    "# assert images have min 0 and max 1 within an error of 1e-5\n",
    "assert torch.allclose(zeros_train.min(), torch.tensor(0., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(zeros_train.max(), torch.tensor(1., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(ones_train.min(), torch.tensor(0., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(ones_train.max(), torch.tensor(1., dtype = torch.float32), atol=1e-5)\n",
    "\n",
    "assert torch.allclose(zeros_test.min(), torch.tensor(0., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(zeros_test.max(), torch.tensor(1., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(ones_test.min(), torch.tensor(0., dtype = torch.float32), atol=1e-5)\n",
    "assert torch.allclose(ones_test.max(), torch.tensor(1., dtype = torch.float32), atol=1e-5)\n",
    "\n",
    "# concatenate the 0,1,2, and 3 datasets in just one training dataset\n",
    "zeros_train = zeros_train.flatten(start_dim = 1)\n",
    "ones_train = ones_train.flatten(start_dim = 1)\n",
    "twos_train = twos_train.flatten(start_dim = 1)\n",
    "threes_train = threes_train.flatten(start_dim = 1)\n",
    "dataset_train = torch.cat((zeros_train, ones_train, twos_train, threes_train), dim = 0)\n",
    "\n",
    "# same for test\n",
    "zeros_test = zeros_test.flatten(start_dim = 1)\n",
    "ones_test = ones_test.flatten(start_dim = 1)\n",
    "twos_test = twos_test.flatten(start_dim = 1)\n",
    "threes_test = threes_test.flatten(start_dim = 1)\n",
    "dataset_test = torch.cat((zeros_test, ones_test, twos_test, threes_test), dim = 0)\n",
    "\n",
    "# add labels\n",
    "labels_train = torch.cat(\n",
    "    (torch.zeros((zeros_train.shape[0], 1), dtype=torch.long),\n",
    "     torch.ones((ones_train.shape[0], 1), dtype=torch.long),\n",
    "     torch.full((twos_train.shape[0], 1), 2, dtype=torch.long),\n",
    "     torch.full((threes_train.shape[0], 1),3, dtype=torch.long)),\n",
    "     dim = 0).squeeze()\n",
    "labels_test = torch.cat((torch.zeros((zeros_test.shape[0], 1), dtype=torch.long), torch.ones((ones_test.shape[0], 1), dtype=torch.long), torch.full((twos_test.shape[0], 1), 2, dtype=torch.long), torch.full((threes_test.shape[0], 1), 3, dtype=torch.long)), dim = 0).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8ea52-1cbc-4e8a-a62f-0ab3a2756a5e",
   "metadata": {
    "id": "76e8ea52-1cbc-4e8a-a62f-0ab3a2756a5e"
   },
   "outputs": [],
   "source": [
    "# build torch datasets and dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# build dataset\n",
    "dataset_train = torch.utils.data.TensorDataset(dataset_train, labels_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(dataset_test, labels_test)\n",
    "\n",
    "#build dataloaders\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118ad9e-da76-458a-b548-c4994063b616",
   "metadata": {
    "id": "7118ad9e-da76-458a-b548-c4994063b616"
   },
   "outputs": [],
   "source": [
    "# Define a quantum device (eg the number of qubits in the circuit)\n",
    "\n",
    "\n",
    "NUM_QUBITS = 8 # we need 8 qubits to encode 16x16 features\n",
    "NUM_LAYERS = 3 #n umber of layers in the QNN (each layer is composed by a parametric unitary transformation given by 3 rotation and CNOT gates to entangle qubits)\n",
    "\n",
    "# definition of the quantum parametric circuit\n",
    "# the circuit ansatz is made by NUM_LAYERS of the same unitary block made of a 3 parametric rotations gates(along X, Y and Z) in the block sphere for each qubit, folloeed by a ladder of CNOT\n",
    "# gates to entangle the qubits\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=NUM_QUBITS) #\"default.qubit\" is the default pennylane differentiable quantum device simulator that is capable of backprop derivatives\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit_block(params, state=None):\n",
    "\n",
    "\n",
    "\n",
    "    # Load the initial state if provided\n",
    "    # quantum encoding (using amplitude encoding: Encodes 2𝑛 features into the amplitude vector of 𝑛 qubits), to represent a valid quantum state vector, the L2-norm of features must be one, this can be achieved\n",
    "    # either by manually normalizing the data (ex: state = state / torch.linalg.norm(state, dim=1).view(-1, 1)), or automatically by setting the argument normalize=True\n",
    "    if state is not None: qml.AmplitudeEmbedding(features=state, wires=range(NUM_QUBITS), normalize=True)\n",
    "\n",
    "    # Alternatively as we are in simulation also a simplified encoding as normalized state vector can be used (ok in simulation, allows to skip quantum encoding of the classical\n",
    "    # data, requires the input to be pre-normalized as a quantum state (eg L2 norm = 1.0: state = state / torch.linalg.norm(state, dim=1).view(-1, 1)))\n",
    "    # if state is not None: qml.QubitStateVector(state, wires=range(NUM_QUBITS))\n",
    "\n",
    "    # Quantum circuit\n",
    "    for i in range(NUM_LAYERS):\n",
    "\n",
    "      # Rotation layer\n",
    "      for j in range(NUM_QUBITS):\n",
    "          qml.RX(params[i, j, 0], wires=j)   #params is the vector containing the PQC paraneters: size = (NUM_LAYERS, NUM_QUBITS, NUM_ROTATIONS) --> ex. in our case: 3*8*3=72 parameters\n",
    "          qml.RY(params[i, j, 1], wires=j)\n",
    "          qml.RZ(params[i, j, 2], wires=j)\n",
    "\n",
    "      # Entangling layer (a ladder of CNOT gates)\n",
    "      for j in range(NUM_QUBITS):\n",
    "          qml.CNOT(wires=[j, (j + 1) % NUM_QUBITS]) # note: when j=NUM_QUBITS-1 ->  (j+1)%NUM_QUBITS = 0\n",
    "\n",
    "\n",
    "    # Return the probability of measuring one of the base vectors in the first and second qubits\n",
    "    return qml.probs(wires=[0,1])\n",
    "\n",
    "  # define general circuit\n",
    "def circuit(params, state):\n",
    "\n",
    "    # apply quantum circuit\n",
    "    basis_state_proba = circuit_block(params, state)\n",
    "\n",
    "    # return probability of measuring |00> |10> |01> |11> in the first two qubits\n",
    "    return basis_state_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6155f7b-8a7e-44b2-b063-3ea929f7411c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "e6155f7b-8a7e-44b2-b063-3ea929f7411c",
    "outputId": "b0d9ed0a-fed9-4dec-8e65-25dfb167bb7a"
   },
   "outputs": [],
   "source": [
    "# visualize the qnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "parameters = Variable(torch.normal( mean=0. , std=0.1, size=(NUM_LAYERS, NUM_QUBITS, 3)), requires_grad=True)\n",
    "\n",
    "state,_ = next(iter(dataloader_train))\n",
    "#state = state / torch.linalg.norm(state, dim=1).view(-1, 1) #needed in case the qml.QubitStateVector encoding is used\n",
    "\n",
    "qml.drawer.use_style(\"black_white\")\n",
    "fig, ax = qml.draw_mpl(circuit_block)(parameters, state)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42d9e5-cb04-4eda-bf14-ceecad58a943",
   "metadata": {
    "id": "1c42d9e5-cb04-4eda-bf14-ceecad58a943"
   },
   "outputs": [],
   "source": [
    "def run_exp(batch_size, num_epochs, dataloader, loss_fn):\n",
    "\n",
    "    loss_history = []\n",
    "    \n",
    "\n",
    "    avg_time_per_epoch = 0\n",
    "\n",
    "    # training loop for classification\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        t_accuracy = 0.0\n",
    "\n",
    "        # Initialize tqdm progress bar with description showing the current epoch\n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as tqdm_epoch:\n",
    "            for _, (data, labels) in tqdm_epoch:\n",
    "\n",
    "                #needed in case the qml.QubitStateVector encoding is used\n",
    "                #data = data / torch.linalg.norm(data, dim=1).view(-1, 1)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                data = data.type(torch.float).to(device=device)\n",
    "                labels = labels.type(torch.int64).to(device=device)\n",
    "\n",
    "                # forward pass\n",
    "                output = circuit(params, data).to(device=device)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = loss_fn(output, labels)\n",
    "\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.step()\n",
    "\n",
    "                # multiclass accuracy\n",
    "                pred_class = torch.argmax(output, 1)\n",
    "                acc_tmp = (torch.sum(pred_class == labels).float() / labels.nelement())\n",
    "\n",
    "                # Optionally, update tqdm bar with batch loss\n",
    "                tqdm_epoch.set_postfix(loss=loss.item(), accuracy=acc_tmp.item())\n",
    "\n",
    "        t_accuracy += acc_tmp.item()\n",
    "        \n",
    "        avg_time_per_epoch += time()-t0\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # print the time\n",
    "        print(\"Time per epoch: \", time()-t0)\n",
    "\n",
    "        # print the loss\n",
    "        print(\"Epoch: \", epoch, \"Loss: \", loss.item())\n",
    "\n",
    "        # print the accuracy\n",
    "        print(\"Accuracy: \", t_accuracy)\n",
    "\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "    return avg_time_per_epoch/NUM_EPOCHS, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37dfa9-2fd4-4e8e-ae8f-fdd6b13cb9fe",
   "metadata": {
    "id": "8a37dfa9-2fd4-4e8e-ae8f-fdd6b13cb9fe"
   },
   "outputs": [],
   "source": [
    "def run_test(batch_size, dataloader, loss_fn):\n",
    "\n",
    "    t_loss = 0.0\n",
    "    t_accuracy = 0.0\n",
    "\n",
    "    counter = 0\n",
    "    for data, labels in dataloader:\n",
    "\n",
    "        counter += 1\n",
    "        # normalize\n",
    "        #data = data / torch.linalg.norm(data, dim=1).view(-1, 1)\n",
    "\n",
    "        data = data.type(torch.float).to(device=device)\n",
    "        labels = labels.type(torch.int64).to(device=device)\n",
    "\n",
    "        # forward pass\n",
    "        output = circuit(params, data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        # Multiclass accuracy\n",
    "        pred_class = torch.argmax(output, 1)\n",
    "        accuracy = (torch.sum(pred_class == labels).float() / labels.nelement())\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_accuracy += accuracy.item()\n",
    "\n",
    "    print(\"Test loss: \", t_loss/counter)\n",
    "    print(\"Test accuracy: \", t_accuracy/counter)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c1299-bb08-4695-9505-ab04b060182e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b5c1299-bb08-4695-9505-ab04b060182e",
    "outputId": "e8024c49-a96e-4297-8473-275d1799064c"
   },
   "outputs": [],
   "source": [
    "# parametres\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# define the cost function (Negative LogLikhelihood loss)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "# initialize parameters randomly\n",
    "params = torch.randn((NUM_LAYERS, NUM_QUBITS, 3), requires_grad=True)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam([params], lr=LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "print(f'\\nRunning experiment with batch size {BATCH_SIZE} and layers {NUM_LAYERS}\\n')\n",
    "\n",
    "time_per_epoch, loss_history = run_exp(BATCH_SIZE, NUM_EPOCHS, dataloader_train, loss_fn)\n",
    "\n",
    "print(f'Average time per epoch: {time_per_epoch} - BS: {BATCH_SIZE} - LAYERS: {NUM_LAYERS}\\n')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611a564-d5be-4c1d-8197-652d0d06608b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "4611a564-d5be-4c1d-8197-652d0d06608b",
    "outputId": "e4c2145d-49fd-44c2-e339-a9d2c4c5f2cb"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788f260-628e-4ab6-a42c-c5fba236989f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0788f260-628e-4ab6-a42c-c5fba236989f",
    "outputId": "7a8c7dde-7761-4d6b-a5f1-da1a5413b951"
   },
   "outputs": [],
   "source": [
    "# test performance\n",
    "\n",
    "run_test(BATCH_SIZE, dataloader_test, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
