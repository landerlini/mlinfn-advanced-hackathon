{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfec5abe",
   "metadata": {},
   "source": [
    "# Definition of loss functions and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b4416",
   "metadata": {},
   "source": [
    "The problem we want to solve is a segmentation problem. For this reason we should use a correct loss function and metric to both optimize and evaluate the training. \n",
    "\n",
    "As regards the loss function, we are going to use the Dice Similarity Coefficient (DSC) that measures how much the true and the predicted areas overlap. DSC is defined this way:\n",
    "\n",
    "$$ DSC(M_{true}, M_{pred}) = 2 \\cdot \\frac{M_{true} \\cdot M_{pred}}{M_{true} + M_{pred}} $$\n",
    "\n",
    "Visually:\n",
    "\n",
    "<center><img src=\"../images/DSC.png\" alt=\"\" class=\"bg-primary mb-1\" width=\"500px\"></center>\n",
    "\n",
    "If there is a perfect overlapping, the DSC will be equal to 1. If there is no overlapping, DSC will be equal to 0. Since we want to use the DSC as a loss function, we want that the better overlapping is nearer to 0. So we define the DSC loss funtion in this way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c814e3-4f07-4442-b5d5-804a12913f97",
   "metadata": {},
   "source": [
    "$$ DSC_{loss}(M_{true}, M_{loss}) = 1 - 2 \\cdot \\frac{M_{true} \\cdot M_{pred}}{M_{true} + M_{pred}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30af06",
   "metadata": {},
   "source": [
    "## **Let's write this loss function with Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06217b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import surface_distance.metrics as sd\n",
    "\n",
    "\n",
    "def DSC_loss(y_true, y_pred): ## Probabilistic Dice\n",
    "    elements_per_class=tf.math.reduce_sum(y_true) # we sum all the pixels belonging to lungs on the labels\n",
    "    predicted_per_class=tf.math.reduce_sum(y_pred) # we sum all the pixels belonging to lungs on predicted mask\n",
    "    # We compute the intersection: we multiply the matrices of the predicted and the true masks.\n",
    "    # Then we sum the elements of the resulting matrix to obtain the number of overlapping pixels;\n",
    "    # Lastly, we multiply this result by 2.\n",
    "    intersection=tf.math.scalar_mul(2.0,tf.math.reduce_sum(tf.math.multiply(y_pred,y_true)))\n",
    "    union=elements_per_class+predicted_per_class\n",
    "    acc=(intersection+0.0001)/(union+0.0001) # the correction is needed to not let the algorithm loss going to inf.\n",
    "    return 1.0-acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbb333",
   "metadata": {},
   "source": [
    "We can use many different loss function to solve this problem but for now DSC loss is ok.\n",
    "In the same way, we can define many matrics to measure the performance of our algorithm. For now, we will see only the DSC explained above and the Jaccard coefficient. The Jaccard index or Jaccard Similarity Coefficient is an index that measures the similarity between two samples. It is simply defined as the intersection over union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a81602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(y_true, y_pred): ## Probabilistic Jaccard computed on both background and foreground (It has not been used yet)\n",
    "    acc=0\n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    elements_per_class=tf.math.reduce_sum(y_true)\n",
    "    predicted_per_class=tf.math.reduce_sum(y_pred)\n",
    "    intersection=tf.math.reduce_sum(tf.math.multiply(y_pred,y_true))\n",
    "    union=elements_per_class+predicted_per_class-intersection\n",
    "    acc+=intersection/(union+0.000001)\n",
    "    return acc/2\n",
    "\n",
    "def DSC(y_true, y_pred): ## Dice coefficient computed only on the foreground to evaluate performances on GG segmentation.\n",
    "    acc=0\n",
    "    y_pred = tf.math.round(y_pred) # NB: we want to obtain a binary mask but we cannot perform the\n",
    "    # rounding operation in the loss function. Why?\n",
    "    elements_per_class=tf.math.reduce_sum(y_true) # we sum all the pixels belonging to lungs on the labels\n",
    "    predicted_per_class=tf.math.reduce_sum(y_pred) # we sum all the pixels belonging to lungs on predicted mask\n",
    "    # We compute the intersection: we multiply the matrices of the predicted and the true masks.\n",
    "    # Then we sum the elements of the resulting matrix to obtain the number of overlapping pixels;\n",
    "    # Lastly, we multiply this result by 2.\n",
    "    intersection=tf.math.scalar_mul(2.0,tf.math.reduce_sum(tf.math.multiply(y_pred,y_true)))\n",
    "    union=elements_per_class+predicted_per_class\n",
    "    acc=(intersection+0.0001)/(union+0.0001) # the correction is needed to not let the algorithm loss going to inf.\n",
    "    return acc\n",
    "\n",
    "def border_dice(true,pred,mm_tolerance = 10, spacing_mm=[1.,1.]):\n",
    "    true = np.round(true)\n",
    "    pred = np.round(pred) \n",
    "\n",
    "    true = true.astype(np.bool)\n",
    "    pred = pred.astype(np.bool)\n",
    "\n",
    "    # calcolo della distanza\n",
    "    surface_distances = sd.compute_surface_distances(\n",
    "        true, pred, spacing_mm )\n",
    "    return sd.compute_surface_dice_at_tolerance(surface_distances,mm_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cff991",
   "metadata": {},
   "source": [
    "test per capire metriche e loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
