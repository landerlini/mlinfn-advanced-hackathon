{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exercise-title",
   "metadata": {},
   "source": [
    "# Exercise 3: FPGA Inference and Performance Analysis\n",
    "\n",
    "## Objectives\n",
    "In this exercise, you will:\n",
    "\n",
    "0. Download all the pre-built bitstream files (each of them corresponds to a different hls4ml configuration) of the encoder model\n",
    "1. Run inference on the FPGA hardware\n",
    "2. Collect performance metrics (latency, throughput)\n",
    "3. Compare hardware predictions with software predictions\n",
    "4. Analyze the differences and understand the trade-offs\n",
    "\n",
    "## Instructions\n",
    "Complete the code cells marked with `# TODO` comments. Follow the hints provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Part 0: Download all the pre-built bitstream files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a16632-78f5-4aca-bf1c-7cbddccba96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Array of firmware files\n",
    "firmware_files=(\n",
    "    \"firmware-01_baseline.xclbin\"\n",
    "    \"firmware-02_optimized_precision.xclbin\"\n",
    "    \"firmware-03_extreme_precision.xclbin\"\n",
    "    \"firmware-04_ultra_extreme_precision.xclbin\"\n",
    "    \"firmware-05_reuse_2.xclbin\"\n",
    "    \"firmware-06_reuse_4.xclbin\"\n",
    "    \"firmware-07_reuse_8.xclbin\"\n",
    "    \"firmware-09_resource_strategy.xclbin\"\n",
    "    \"firmware-11_optimized_precision_reuse_2.xclbin\"\n",
    "    \"firmware-12_optimized_precision_reuse_8.xclbin\"\n",
    "    \"firmware-13_extreme_precision_reuse_8.xclbin\"\n",
    "    \"firmware-14_extreme_precision_io_stream.xclbin\"\n",
    "    \"firmware-15_area_optimized.xclbin\",\n",
    "    \"firmware-16_resource_strategy_reuse_8.xclbin\"\n",
    ")\n",
    "\n",
    "base_url=\"https://minio.131.154.98.45.myip.cloud.infn.it/public-data/firmware-hackathon\"\n",
    "\n",
    "# Download each firmware file if it doesn't exist\n",
    "for firmware in \"${firmware_files[@]}\"; do\n",
    "    if [ -f \"$firmware\" ]; then\n",
    "        echo \"Skipping $firmware (already exists)\"\n",
    "    else\n",
    "        echo \"Downloading $firmware...\"\n",
    "        wget -q \"$base_url/$firmware\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5522a54-d3d3-4159-b51e-36e18c78db90",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "Run the following cells to set up the environment (no changes needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00734178-da75-4ce3-bc60-4a7ddb5e7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"]=os.environ[\"PATH\"]+\":\"+os.environ[\"BONDMACHINE_DIR\"]\n",
    "os.environ['XILINX_HLS'] = '/tools/Xilinx/Vitis_HLS/2023.2'\n",
    "os.environ['XILINX_VIVADO'] = '/tools/Xilinx/Vivado/2023.2'\n",
    "os.environ['XILINX_VITIS'] = '/tools/Xilinx/Vitis/2023.2'\n",
    "os.environ['PATH']=os.environ[\"PATH\"]+\":\"+os.environ['XILINX_HLS']+\"/bin:\"+os.environ['XILINX_VIVADO']+\"/bin:\"+os.environ['XILINX_VITIS']+\"/bin:\"\n",
    "os.environ['XILINX_XRT'] = '/opt/xilinx/xrt'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/xilinx/xrt/lib'\n",
    "\n",
    "notebook_directory = os.path.abspath(os.path.dirname((os.environ[\"JPY_SESSION_NAME\"])))\n",
    "os.chdir(notebook_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94df7f-488e-413d-9b8c-4d3fa2c24c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import NeuralNetworkOverlay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "inference_dir = os.path.join(current_dir, 'inference')\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "\n",
    "print(\"Loading test data from file...\")\n",
    "X_test = np.load(os.path.join(current_dir, 'X_test.npy'))\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: FPGA Hardware Inference\n",
    "\n",
    "### Task 2.1: Load the FPGA Overlay\n",
    "Complete the code to load the FPGA overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccafde-c417-4b64-93f8-c25d1eaa4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the FPGA overlay\n",
    "# Hint: Use NeuralNetworkOverlay class with xclbin_name=\"myproject_kernel.xclbin\"\n",
    "# Store the overlay object in a variable called 'ol'\n",
    "\n",
    "print(\"Loading FPGA overlay...\")\n",
    "# YOUR CODE HERE\n",
    "ol = None  # Replace None with the correct code\n",
    "\n",
    "print(\"FPGA overlay loaded...\")\n",
    "print(ol)\n",
    "\n",
    "# You will discover that ol is an instance of the custom class NeuralNetworkOverlay which inherits from the PYNQ Overlay class\n",
    "# You may ask youserlf, what is PYNQ? PYNQ is a Python-based framework for using Xilinx FPGAs and lets you control FPGA hardware directly from Python\n",
    "# It loads the FPGA bitstream file by default and you can ispect the properties of this object by running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69691c09-335a-4886-b2fc-e52f4bbc6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol.ip_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-2",
   "metadata": {},
   "source": [
    "### Task 2.2: Run Hardware Inference with Performance Profiling\n",
    "\n",
    "Use the FPGA overlay to run inference on the test data. The `predict` method returns three values:\n",
    "- Hardware predictions\n",
    "- Latency (time taken)\n",
    "- Throughput (inferences per second)\n",
    "\n",
    "**Important:** Set `profile=True` to enable performance profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63b468-2f0f-4dc8-a2f5-a7f8cbf1d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run inference on FPGA hardware with profiling\n",
    "# Hint: Use ol.predict() with parameters: X_test, output_shape, profile=True\n",
    "# Store the three return values in variables: y_hw, latency, throughput\n",
    "\n",
    "output_shape = (X_test.shape[0], 8) # Why 8? Where does the 8 come from?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y_hw = None       # Replace with correct code\n",
    "latency = None    # Replace with correct code\n",
    "throughput = None # Replace with correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-3",
   "metadata": {},
   "source": [
    "### Task 2.3: Display Hardware Predictions\n",
    "\n",
    "Print the hardware predictions to verify they were computed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daafd1d-f1c7-431f-953e-b7cd986d3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the hardware predictions\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-4",
   "metadata": {},
   "source": [
    "### Task 2.4: Analyze Performance Metrics\n",
    "\n",
    "Print out the latency and throughput information in a clear format.\n",
    "- Latency: time taken for all inferences\n",
    "- Throughput: number of inferences per second\n",
    "- Calculate the average time per single inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print performance metrics\n",
    "# Calculate and print:\n",
    "# 1. Total time for all inferences (latency)\n",
    "# 2. Throughput (inferences per second)\n",
    "# 3. Average time per single inference (in microseconds)\n",
    "\n",
    "print(\"\\n=== FPGA Hardware Performance Metrics ===\")\n",
    "# YOUR CODE HERE\n",
    "# Hint: throughput is already inferences/second\n",
    "# Hint: time per inference = latency / number_of_samples\n",
    "# Hint: Convert to microseconds by multiplying by 1e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "### Cleanup: Free the FPGA Overlay\n",
    "\n",
    "After completing inference, free the FPGA resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09372190-926a-49a2-940c-e2f26c62d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the FPGA overlay (no changes needed)\n",
    "ol.free_overlay()\n",
    "print(\"FPGA overlay freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Software (CPU) Inference\n",
    "\n",
    "### Task 3.1: Load the Software Model and Run Inference\n",
    "\n",
    "Load the Keras model and run inference on the CPU for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c4946-1844-42ef-817c-1a78cac4bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the Keras encoder model\n",
    "# Hint: Use load_model() with filename 'small_encoder.keras'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "encoder = None  # Replace with correct code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b1c9d-e4a9-4684-ba01-0c72c0c9ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run CPU inference\n",
    "# Hint: Use encoder.predict() on X_test\n",
    "# Store the result in y_cpu\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y_cpu = None  # Replace with correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "## Part 4: Compare Hardware vs Software Predictions\n",
    "\n",
    "### Task 4.1: Convert Hardware Predictions to NumPy Array\n",
    "\n",
    "The hardware predictions need to be converted to a NumPy array for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953232d-9076-4f18-ac92-18efdc320712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert y_hw to a NumPy array\n",
    "# Hint: Use np.asarray()\n",
    "# Store the result in y_hls\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y_hls = None  # Replace with correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-2",
   "metadata": {},
   "source": [
    "### Task 4.2: Calculate Comparison Metrics\n",
    "\n",
    "Calculate metrics to compare the hardware and software predictions:\n",
    "- Mean Squared Error (MSE) per sample\n",
    "- Overall MSE\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f1751-ea73-4944-85b7-a77e3c64155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate comparison metrics\n",
    "# 1. MSE per sample: mean of (y_cpu - y_hls)^2 along axis 1\n",
    "# 2. Overall MSE: mean of all MSE per sample\n",
    "# 3. MAE: mean of absolute differences\n",
    "# 4. RMSE: square root of overall MSE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mse_per_sample = None  # Calculate MSE for each sample\n",
    "overall_mse = None     # Calculate overall MSE\n",
    "mae = None             # Calculate MAE\n",
    "rmse = None            # Calculate RMSE\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"\\n=== Software vs Hardware Reconstruction Metrics ===\")\n",
    "print(f\"Overall MSE           : {overall_mse:.6f}\")\n",
    "print(f\"Average MSE per sample: {np.mean(mse_per_sample):.6f}\")\n",
    "print(f\"Min MSE               : {np.min(mse_per_sample):.6f}\")\n",
    "print(f\"Max MSE               : {np.max(mse_per_sample):.6f}\")\n",
    "print(f\"Mean Absolute Error   : {mae:.6f}\")\n",
    "print(f\"RMSE                  : {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-3",
   "metadata": {},
   "source": [
    "### Task 4.3: Visualize the Comparison\n",
    "\n",
    "Create visualizations to compare CPU and HLS predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison for first 3 samples (no changes needed)\n",
    "n_examples = 3\n",
    "fig, axes = plt.subplots(n_examples, 3, figsize=(15, 3*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # CPU predictions\n",
    "    axes[i, 0].plot(y_cpu[i])\n",
    "    axes[i, 0].set_title(f'CPU Prediction {i}')\n",
    "    axes[i, 0].set_ylabel('Amplitude')\n",
    "    axes[i, 0].grid(True)\n",
    "    \n",
    "    # HLS predictions\n",
    "    axes[i, 1].plot(y_hls[i])\n",
    "    axes[i, 1].set_title(f'HLS Prediction {i}')\n",
    "    axes[i, 1].grid(True)\n",
    "    \n",
    "    # Difference\n",
    "    axes[i, 2].plot(y_cpu[i] - y_hls[i])\n",
    "    axes[i, 2].set_title(f'Error (MSE: {mse_per_sample[i]:.4f})')\n",
    "    axes[i, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cpu_hls_comparison.png')\n",
    "print(\"\\nComparison plot saved as 'cpu_hls_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution (no changes needed)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse_per_sample, bins=20, edgecolor='black')\n",
    "plt.xlabel('MSE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(mse_per_sample)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Distribution (Boxplot)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png')\n",
    "print(\"Error distribution plot saved as 'error_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "## Part 5: Latency Comparison - FPGA vs CPU\n",
    "\n",
    "Now let's compare the inference latencies between FPGA and CPU to understand the performance benefits of hardware acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-1",
   "metadata": {},
   "source": [
    "### Task 5.1: Measure CPU Inference Latency\n",
    "\n",
    "Measure the time it takes for the CPU to perform inference on the same test data.\n",
    "We'll use Python's `time` module to measure the elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpu-latency-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure CPU inference latency\n",
    "# 1. Import the time module\n",
    "# 2. Record start time before prediction\n",
    "# 3. Run encoder.predict(X_test) again\n",
    "# 4. Record end time after prediction\n",
    "# 5. Calculate cpu_latency = end_time - start_time\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Measuring CPU inference latency...\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "start_time = None  # Record start time using time.time()\n",
    "# Run prediction here\n",
    "end_time = None    # Record end time using time.time()\n",
    "\n",
    "cpu_latency = None # Calculate the difference\n",
    "\n",
    "print(f\"CPU inference completed in {cpu_latency:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-2",
   "metadata": {},
   "source": [
    "### Task 5.2: Calculate Performance Metrics\n",
    "\n",
    "Calculate key performance metrics to compare FPGA and CPU:\n",
    "- Speedup factor (how many times faster is FPGA)\n",
    "- Throughput for both platforms\n",
    "- Time per inference for both platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate comparison metrics\n",
    "# 1. Calculate speedup = cpu_latency / fpga_latency (use 'latency' variable from Part 2)\n",
    "# 2. Calculate cpu_throughput = num_samples / cpu_latency\n",
    "# 3. Calculate cpu_time_per_inference = (cpu_latency / num_samples) * 1e6 (in microseconds)\n",
    "# 4. Calculate fpga_time_per_inference = (latency / num_samples) * 1e6 (in microseconds)\n",
    "\n",
    "num_samples = X_test.shape[0]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "speedup = None                    # Calculate speedup\n",
    "cpu_throughput = None             # Calculate CPU throughput\n",
    "cpu_time_per_inference = None     # Calculate CPU time per inference in μs\n",
    "fpga_latency = latency            # FPGA latency from Part 2\n",
    "fpga_throughput = throughput      # FPGA throughput from Part 2\n",
    "fpga_time_per_inference = None    # Calculate FPGA time per inference in μs\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LATENCY COMPARISON: FPGA vs CPU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<30} {'FPGA':<20} {'CPU':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Total Latency (seconds)':<30} {fpga_latency:<20.6f} {cpu_latency:<20.6f}\")\n",
    "print(f\"{'Throughput (inferences/sec)':<30} {fpga_throughput:<20.2f} {cpu_throughput:<20.2f}\")\n",
    "print(f\"{'Time per Inference (μs)':<30} {fpga_time_per_inference:<20.4f} {cpu_time_per_inference:<20.4f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'SPEEDUP FACTOR':<30} {speedup:<20.2f}x {'1.00x':<20}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nThe FPGA is {speedup:.2f}x faster than the CPU for this inference task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-3",
   "metadata": {},
   "source": [
    "### Task 5.3: Visualize Latency Comparison\n",
    "\n",
    "Create visualizations to compare the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison visualizations\n",
    "# Create bar charts comparing FPGA and CPU performance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Total Latency Comparison\n",
    "platforms = ['FPGA', 'CPU']\n",
    "latencies = [fpga_latency, cpu_latency]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "axes[0].bar(platforms, latencies, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Latency (seconds)', fontsize=12)\n",
    "axes[0].set_title('Total Inference Latency', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(latencies):\n",
    "    axes[0].text(i, v, f'{v:.4f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Throughput Comparison\n",
    "throughputs = [fpga_throughput, cpu_throughput]\n",
    "axes[1].bar(platforms, throughputs, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Throughput (inferences/sec)', fontsize=12)\n",
    "axes[1].set_title('Inference Throughput', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(throughputs):\n",
    "    axes[1].text(i, v, f'{v:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Time per Inference Comparison\n",
    "times_per_inf = [fpga_time_per_inference, cpu_time_per_inference]\n",
    "axes[2].bar(platforms, times_per_inf, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Time per Inference (μs)', fontsize=12)\n",
    "axes[2].set_title('Time per Single Inference', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(times_per_inf):\n",
    "    axes[2].text(i, v, f'{v:.2f}μs', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nLatency comparison plot saved as 'latency_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speedup-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speedup visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create a horizontal bar showing speedup\n",
    "y_pos = [0, 1]\n",
    "performance = [1.0, speedup]\n",
    "labels = ['CPU (Baseline)', f'FPGA ({speedup:.2f}x faster)']\n",
    "colors_speedup = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax.barh(y_pos, performance, color=colors_speedup, alpha=0.7, edgecolor='black', height=0.6)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels, fontsize=12)\n",
    "ax.set_xlabel('Relative Performance (Speedup Factor)', fontsize=12)\n",
    "ax.set_title('FPGA Speedup vs CPU', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, performance)):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "            f'  {val:.2f}x', \n",
    "            ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speedup_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Speedup comparison plot saved as 'speedup_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "## Part 6: Simulated vs Real Latency Comparison\n",
    "\n",
    "In this final part, you will compare the **simulated latency** (from HLS synthesis) with the **real measured latency** from the FPGA.\n",
    "\n",
    "The simulation gives you the number of **clock cycles** required for the computation. To compare with real latency, you need to convert clock cycles to time using the FPGA clock frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-1",
   "metadata": {},
   "source": [
    "### Task 6.1: Get Simulated Latency and Clock Frequency\n",
    "\n",
    "From your previous HLS synthesis notebooks, you should have the simulated latency in clock cycles.\n",
    "\n",
    "**Note**: The FPGA clock frequency is typically available as a property of the overlay or is a known constant (e.g., 200 MHz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulated-latency-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Input your simulated latency from HLS synthesis\n",
    "# This value should come from your synthesis report (e.g., 52 clock cycles)\n",
    "\n",
    "simulated_clock_cycles = None  # Replace with your value (e.g., 52)\n",
    "\n",
    "# TODO: Get the FPGA clock frequency\n",
    "# Option 1: Try to get it from the overlay object\n",
    "# Check if ol has a clock frequency property (common properties: clock_freq, frequency_mhz)\n",
    "# Option 2: Use a known constant (typically 100-300 MHz for these boards)\n",
    "\n",
    "# Try to get frequency from overlay (uncomment and try different property names)\n",
    "# fpga_clock_freq_mhz = ol.clock_freq  # or ol.frequency_mhz, or ol.clock_frequency\n",
    "\n",
    "# Or set it manually if you know the frequency:\n",
    "fpga_clock_freq_mhz = None  # Replace with your FPGA clock frequency in MHz (e.g., 200)\n",
    "\n",
    "print(f\"Simulated latency: {simulated_clock_cycles} clock cycles\")\n",
    "print(f\"FPGA clock frequency: {fpga_clock_freq_mhz} MHz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-2",
   "metadata": {},
   "source": [
    "### Task 6.2: Calculate Theoretical Latency from Simulation\n",
    "\n",
    "Convert the simulated clock cycles to time (microseconds).\n",
    "\n",
    "**Formula**: \n",
    "- Clock period = 1 / frequency\n",
    "- Time = clock_cycles × clock_period\n",
    "- If frequency is in MHz, then: Time (μs) = clock_cycles / frequency_MHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the theoretical latency per inference from simulation\n",
    "# Formula: theoretical_latency_us = simulated_clock_cycles / fpga_clock_freq_mhz\n",
    "\n",
    "# YOUR CODE HERE\n",
    "theoretical_latency_us = None  # Calculate theoretical latency in microseconds\n",
    "\n",
    "print(f\"\\nTheoretical latency (from simulation): {theoretical_latency_us:.4f} μs per inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-3",
   "metadata": {},
   "source": [
    "### Task 6.3: Compare with Real Measured Latency\n",
    "\n",
    "Compare the theoretical latency with the actual measured latency from Part 2.\n",
    "Calculate the overhead factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-comparison-sim-real",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare simulated vs real latency\n",
    "# Use fpga_time_per_inference from Part 5\n",
    "# Calculate overhead_factor = real_latency / theoretical_latency\n",
    "\n",
    "real_latency_us = fpga_time_per_inference  # From Part 5\n",
    "\n",
    "# YOUR CODE HERE\n",
    "overhead_factor = None  # Calculate the overhead factor\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SIMULATED vs REAL LATENCY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<35} {'Value':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Simulated clock cycles':<35} {simulated_clock_cycles:<20}\")\n",
    "print(f\"{'FPGA clock frequency (MHz)':<35} {fpga_clock_freq_mhz:<20}\")\n",
    "print(f\"{'Theoretical latency (μs)':<35} {theoretical_latency_us:<20.4f}\")\n",
    "print(f\"{'Real measured latency (μs)':<35} {real_latency_us:<20.4f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Overhead factor':<35} {overhead_factor:<20.2f}x\")\n",
    "print(f\"{'Additional latency (μs)':<35} {(real_latency_us - theoretical_latency_us):<20.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nThe real latency is {overhead_factor:.2f}x higher than the simulated latency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-4",
   "metadata": {},
   "source": [
    "### Task 6.4: Visualize Simulated vs Real Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sim-real-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of simulated vs real latency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Latency comparison\n",
    "categories = ['Simulated\\n(Theoretical)', 'Real\\n(Measured)']\n",
    "latencies = [theoretical_latency_us, real_latency_us]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "bars = axes[0].bar(categories, latencies, color=colors, alpha=0.7, edgecolor='black', width=0.6)\n",
    "axes[0].set_ylabel('Latency per Inference (μs)', fontsize=12)\n",
    "axes[0].set_title('Simulated vs Real Latency', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, latencies)):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}μs',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Breakdown of latency components\n",
    "components = ['Computation\\n(Simulated)', 'Overhead\\n(Data Transfer,\\nProtocol, etc.)']\n",
    "values = [theoretical_latency_us, real_latency_us - theoretical_latency_us]\n",
    "colors_breakdown = ['#3498db', '#f39c12']\n",
    "\n",
    "bars2 = axes[1].bar(components, values, color=colors_breakdown, alpha=0.7, edgecolor='black', width=0.6)\n",
    "axes[1].set_ylabel('Latency (μs)', fontsize=12)\n",
    "axes[1].set_title('Latency Breakdown', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars2, values)):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}μs',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('simulated_vs_real_latency.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nSimulated vs real latency plot saved as 'simulated_vs_real_latency.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (hls4ml-ml)",
   "language": "python",
   "name": "hls4ml-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
