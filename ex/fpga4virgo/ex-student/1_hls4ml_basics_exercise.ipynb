{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56e0b3b",
   "metadata": {},
   "source": [
    "# Exercise 1: Train a Small Encoder and Introduction to _hls4ml_\n",
    "\n",
    "## Objectives\n",
    "In this exercise, you will:\n",
    "1. Train a simplified version of the Autoencoder\n",
    "2. Learn the basics of _hls4ml_\n",
    "\n",
    "## Instructions\n",
    "Complete the code cells marked with `# TODO` comments. Follow the hints provided.\n",
    "\n",
    "## Part 1: Environment Setup and Data Loading\n",
    "Run the following cells to set up the environment (no changes needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558b04b-4236-4ee8-8f92-2d230b7959ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries and import packages (no changes needed)\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "\n",
    "# Numpy and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TimeSeries to hold our data\n",
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "# Keras for the model\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape, Input, InputLayer\n",
    "from keras.optimizers import Nadam, Adam, SGD\n",
    "from keras import regularizers\n",
    "\n",
    "# Local file with some useful methods\n",
    "from utils import *\n",
    "\n",
    "# hls4ml\n",
    "import hls4ml\n",
    "\n",
    "# Set the correct libraries path for hls4ml\n",
    "os.environ['XILINX_HLS']    = '/opt/tools/Xilinx/Vitis_HLS/2023.2'\n",
    "os.environ['XILINX_VIVADO'] = '/opt/tools/Xilinx/Vivado/2023.2'\n",
    "os.environ['XILINX_VITIS']  = '/opt/tools/Xilinx/Vitis/2023.2'\n",
    "os.environ['PATH'] = os.environ[\"PATH\"] + \":\" \\\n",
    "                   + os.environ['XILINX_HLS'] + \"/bin:\" \\\n",
    "                   + os.environ['XILINX_VIVADO'] + \"/bin:\" \\\n",
    "                   + os.environ['XILINX_VITIS'] + \"/bin:\"\n",
    "\n",
    "print(\"Environment setup correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d171d-e502-493f-8835-1e7073421308",
   "metadata": {},
   "source": [
    "Load the data needed to train the encoder.\n",
    "\n",
    "In the first exercise you analyzed 1 full second of data, now we only want to analyze 64 input datapoints and compress them down to dimension 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the input data\n",
    "chunks = None  # Replace with correct code\n",
    "\n",
    "# Hint: We want to train a small encoder that goes from 64 inputs down to 8 otuput, so the shape of the data must be adjusted accordingly.\n",
    "# Hint: You can reuse the code from the previous exercise or look in utils.py if you find a useful method...\n",
    "# Hint: remember the input directory is \"/data/input_data/AI_INFN/gwdata\".\n",
    "\n",
    "print(f\"Loaded {chunks.shape[0]} samples with shape: {chunks[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacd5a7-9ab8-49f5-a39b-dee4b57a8c2e",
   "metadata": {},
   "source": [
    "## Part 2: Train a small encoder\n",
    "FPGAs have limited resources and the more resources are required by your design, the more time _hls4ml_ will need to produce a firmware. <br>\n",
    "Additionally, the FPGA board that we are targetting for this exercise (`AMD Alveo U55c`) is quite large, increasing further the compilation time.\n",
    "\n",
    "### Task 2.1: Define the Autoencoder\n",
    "In order to fit within the hands-on session today we will focus on a small encoder of fully connected dense layers (no Convolution):\n",
    "- 64 inputs\n",
    "- Few hidden layers\n",
    "- 8 \"encoded\" outputs\n",
    "\n",
    "**Important:** Remember to name all your layers, it will be useful when optimizing the model! <br>\n",
    "Example:\n",
    "```python\n",
    "model.Add(\n",
    "    Dense(\n",
    "            64,                 # N Neurons\n",
    "            activation='tanh',  # Activation\n",
    "            name=\"encoder1\"     # Name of the layer\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ab918-0292-44d4-bdad-cf8b17567c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a \"small\" autoencoder which has 64 inputs and latent dimension of 8\n",
    "\n",
    "# Hint: You can re-use the same model structure seen in the previous exercise...\n",
    "#       but remmemer to update the layers and the neurons to get a small model!\n",
    "\n",
    "def autoencoder_model( '''YOUR CODE HERE''' ):\n",
    "\n",
    "    # ====== Encoder ======\n",
    "    encoder = Sequential(name=\"encoder\")\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # ====== Decoder ======\n",
    "    decoder = Sequential(name=\"decoder\")\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # ====== Full Autoencoder ======\n",
    "    # YOUR CODE HERE\n",
    "    autoencoder = Model(inputs=input_signal, outputs=decoded, name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-4), loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Return both the full model and the encoder/decoder models\n",
    "    return autoencoder, decoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e6e9b-d25b-4c48-9b97-6712b7be088f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: declare your model and visualize it\n",
    "\n",
    "# Define model\n",
    "# YOUR CODE HERE\n",
    "(autoencoder, decoder, encoder) = None  # Replace with correct code\n",
    "\n",
    "# And visualize it\n",
    "print(\"\\n - AUTOENCODER -\")\n",
    "autoencoder.summary()\n",
    "print(\"\\n - ENCODER -\")\n",
    "encoder.summary()\n",
    "print(\"\\n - DECODER -\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547e099-06da-440f-a751-878b719c6e63",
   "metadata": {},
   "source": [
    "### Task 2.2: Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e68261-b085-4774-a787-b39284f7e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train the model\n",
    "\n",
    "# Hint: we can limit the training to few epochs (~30) as we don't particularly care about\n",
    "#       the results, we only want to compare its performace when run on CPU and on FPGA\n",
    "\n",
    "# Actual training\n",
    "history = autoencoder.fit( ''' YOUR CODE HERE ''' ) # Replace with correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d42b8a-974a-4b2d-bb78-237b2a2cede0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Task 2.3: Save the Autoencoder\n",
    "We will check its performance later, for now we just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2772e-93da-4b30-b663-6ac644308266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure output directory exists (no changes needed)\n",
    "if not os.path.exists('small_model'):\n",
    "    os.makedirs('small_model')\n",
    "\n",
    "# Save all three models\n",
    "encoder    .save(\"small_model/small_encoder.keras\"    , save_format=\"keras_v3\")\n",
    "decoder    .save(\"small_model/small_decoder.keras\"    , save_format=\"keras_v3\")\n",
    "autoencoder.save(\"small_model/small_autoencoder.keras\", save_format=\"keras_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16e4bb-9cf2-4786-946a-f4fd8443fe6c",
   "metadata": {},
   "source": [
    "We also save a small subset of the input data that we can use later to test our converted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec3817-c865-421f-997d-886cc76fb98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save 2000 events for testing (no changes needed)\n",
    "X_test = chunks[:2000]\n",
    "np.save('X_test.npy', X_test)\n",
    "\n",
    "print(f\"{X_test.shape[0]} test events (with shape {X_test[0].shape}) have been saved to X_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30adbb6-db10-48ed-b1a8-88e462f421c7",
   "metadata": {},
   "source": [
    "## Part 3: _hls4ml_ Basics\n",
    "We will now use the _hls4ml_ library to convert our encoder to a low latency firmware to be run on the FPGA.\n",
    "This process has few steps:\n",
    "-  Define an hls4ml config\n",
    "-  Convert the model using the config\n",
    "-  Compile\n",
    "-  Build the firmware\n",
    "\n",
    "For additional details:\n",
    "- here is the official documentation: https://fastmachinelearning.org/hls4ml/index.html\n",
    "- here is the GitHub page: https://github.com/fastmachinelearning/hls4ml\n",
    "\n",
    "### Task 3.1: _hls4ml_ Configuration Files\n",
    "We first define the _hls4ml_ config files needed to translate our model into a firmware. <br>\n",
    "We need two configs:\n",
    "- `hls_config`: to control the conversion options of the model\n",
    "   - here is where we will mostly optimize our framework \n",
    "- `main_config`: which contains\n",
    "   - to specify the details of the FPGA board we are targeting\n",
    "   - the hls_config\n",
    "   - the model\n",
    "   - some I/O options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6fdf-8da1-412b-8ede-b9fb5a0652f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a default hls config and inspect it (no changes needed)\n",
    "hls_config = hls4ml.utils.config_from_keras_model(encoder, granularity='model')\n",
    "\n",
    "print(\"=\"*20, \"HLS Config\", \"=\"*20)\n",
    "print_dict(hls_config)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# You will see a few parameters that can be customized:\n",
    "#  - `Precision`: the bit-wise representation of all the numbers in the model\n",
    "#  - `ReuseFactor`: a mechanism to tune out firmware parallelism\n",
    "#  - `Strategy`: how _hls4ml_ should do the conversion, if targeting resources- or latency-optimization\n",
    "# In the next exercise we will explore some of them and verify what is their impact the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01c2e8-2547-4ccf-85df-6cc33adb22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix the missing parameters in the main_config\n",
    "\n",
    "# Main config\n",
    "main_cfg = hls4ml.converters.create_config(\n",
    "    board = 'alveo-u55c',            # Target boad                           - DO NOT CHANGE\n",
    "    part = 'xcu55c-fsvh2892-2L-e',   # Target FPGA part                      - DO NOT CHANGE\n",
    "    clock_period = 3,                # Clock period in ns (i.e. ~333 MHz)\n",
    "    backend = 'VivadoAccelerator'    # Backend to convert the NN in firmware - DO NOT CHANGE\n",
    ")\n",
    "\n",
    "# Few more customizations\n",
    "# YOUR CODE HERE\n",
    "main_cfg['HLSConfig'] = None                                                      # Replace with correct config\n",
    "main_cfg['IOType'] = 'io_parallel'                                                # DO NOT CHANGE\n",
    "main_cfg['AcceleratorConfig']['Platform'] = 'xilinx_u55c_gen3x16_xdma_3_202210_1' # DO NOT CHANGE\n",
    "main_cfg['KerasModel'] = None                                                     # Replace with correct model\n",
    "main_cfg['OutputDir'] = 'small_model/' + None                                     # Replace with output name\n",
    "\n",
    "# Inspect final config\n",
    "print(\"=\"*20, \"Main Config\", \"=\"*20)\n",
    "print_dict(main_cfg)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80406e9a-57c5-4620-bcc8-7b8093c0acfc",
   "metadata": {},
   "source": [
    "### Task 3.2: HLS Model and Inference\n",
    "We are now ready to define an hls model, compile it and run some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159de25-2cd2-4c22-bd0b-67dbadb836d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: declare the HLS model and run inference on the test data\n",
    "\n",
    "# Define the hls model\n",
    "hls_model = hls4ml.converters.keras_v2_to_hls(main_cfg)\n",
    "\n",
    "# Compile it\n",
    "hls_model.compile()\n",
    "\n",
    "# Run inference\n",
    "# Hint: use the hls4ml `predict(X)` method, just like in _Keras_, this will emulate\n",
    "#       the performance of the model converted into firmware with hls4ml\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y_cpu = None # Replace with correct code\n",
    "y_hls = None # Replace with correct code\n",
    "\n",
    "# Save predictions for later comparisons\n",
    "np.save('y_cpu.npy', y_cpu)\n",
    "np.save('y_hls.npy', y_hls)\n",
    "print(\"Software predictions saved as 'y_cpu.npy' and 'y_hls.npy'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5a1dc-0a0c-4869-a6fe-7b4f36abdf86",
   "metadata": {},
   "source": [
    "### Task 3.3: Calculate Comparison Metrics\n",
    "Calculate metrics to compare the hardware and software predictions:\n",
    "- Mean Squared Error (MSE) per sample\n",
    "- Overall MSE\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097bb8d-ceea-49ac-b638-e075b1d85dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate comparison metrics\n",
    "# 1. MSE per sample: mean of (y_cpu - y_hls)^2 along axis 1\n",
    "# 2. Overall MSE: mean of all MSE per sample\n",
    "# 3. MAE: mean of absolute differences\n",
    "# 4. RMSE: square root of overall MSE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mse_per_sample = None  # Calculate MSE for each sample\n",
    "overall_mse = None     # Calculate overall MSE\n",
    "mae = None             # Calculate MAE\n",
    "rmse = None            # Calculate RMSE\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"\\n=== Software vs Hardware Reconstruction Metrics ===\")\n",
    "print(f\"Overall MSE           : {overall_mse:.6f}\")\n",
    "print(f\"Average MSE per sample: {np.mean(mse_per_sample):.6f}\")\n",
    "print(f\"Min MSE               : {np.min(mse_per_sample):.6f}\")\n",
    "print(f\"Max MSE               : {np.max(mse_per_sample):.6f}\")\n",
    "print(f\"Mean Absolute Error   : {mae:.6f}\")\n",
    "print(f\"RMSE                  : {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632322de-6843-4e50-a7d0-fea69a01cce4",
   "metadata": {},
   "source": [
    "### Task 3.4: Visualize the Comparison\n",
    "Create visualizations to compare CPU and HLS predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e8e51-c642-4f6f-9f19-5edd38316f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison for first 3 samples (no changes needed)\n",
    "n_examples = 3\n",
    "fig, axes = plt.subplots(n_examples, 3, figsize=(15, 3*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # CPU predictions\n",
    "    axes[i, 0].plot(y_cpu[i])\n",
    "    axes[i, 0].set_title(f'CPU Prediction {i}')\n",
    "    axes[i, 0].set_ylabel('Amplitude')\n",
    "    axes[i, 0].grid(True)\n",
    "    \n",
    "    # HLS predictions\n",
    "    axes[i, 1].plot(y_hls[i])\n",
    "    axes[i, 1].set_title(f'HLS Prediction {i}')\n",
    "    axes[i, 1].grid(True)\n",
    "    \n",
    "    # Difference\n",
    "    axes[i, 2].plot(y_cpu[i] - y_hls[i])\n",
    "    axes[i, 2].set_title(f'Error (MSE: {mse_per_sample[i]:.4f})')\n",
    "    axes[i, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cpu_hls_comparison.png')\n",
    "print(\"\\nComparison plot saved as 'cpu_hls_emulation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaab20e-6fc7-4202-8e17-d50c1f8b0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution (no changes needed)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse_per_sample, bins=20, edgecolor='black')\n",
    "plt.xlabel('MSE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(mse_per_sample)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Distribution (Boxplot)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png')\n",
    "print(\"Error distribution plot saved as 'emulation_error_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dd7b8-bbae-4b75-b2e4-3a4a15bfae01",
   "metadata": {},
   "source": [
    "### Task 4.4: Synthesize the Model\n",
    "The last step of this exercise is to run the synthesis of the model.\n",
    "\n",
    "In this step `hls4ml` uses the Vivado/Vitis_HLS libraries to convert the Neural Network into the electrical circuit that will be loaded on the FPGA. After this step we will have a first estimate of how many resources our project will need and what will be the latency.\n",
    "\n",
    "**Important:** This step takes 5-10 minutes depending on how many layers/neurons you have used, so fix the cell below and run it now!\n",
    "\n",
    "### Bonus:\n",
    "The `build ` method potentially takes many more parameters:\n",
    "```python\n",
    "hls_model.build(\n",
    "    csim          = False, # Run C++ emulation\n",
    "    synth         = True,  # Run Synthesis\n",
    "    export        = True,  # Run Synthesis + Implementation + packaging into custo IP\n",
    "    export_xo     = True,  # Build an .xo file for integration in larger project\n",
    "    export_bitfile= True   # Build an .xlcbin file for direct board deployment\n",
    ")\n",
    "```\n",
    "Building a full `.xlcbin` file might require several hours! In the last exercise today (`notebook3`) you will use some pre-compiled firmwares to run inference on the real `Alveo U55c` boards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e33238-7cf9-4bf8-8629-1af8bde5d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the same output dir that you have used in the main_cfg\n",
    "\n",
    "# Run the synthesis\n",
    "hls_model.build(csim=False) # -- DO NOT CHANGE\n",
    "\n",
    "#and print the reports\n",
    "print(\"Resource usage and latency:\")\n",
    "print_report(''' YOUR CODE HERE ''')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.10 (hls4ml-ml)",
   "language": "python",
   "name": "hls4ml-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
