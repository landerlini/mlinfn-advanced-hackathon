{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56e0b3b",
   "metadata": {},
   "source": [
    "# Exercise 2: Optimize _hls4ml_ Conversion\n",
    "Optimizing the conversion of a Neural Network into firmware is always a trade-off between how many resources we can occupy on the hardware, how fast we want our algorithm to run and how accurate we want the performance to be.\n",
    "\n",
    "In this exercise we will explore a few options to control how _hls4ml_ translates the Neural Network into a firmware.\n",
    "\n",
    "## Objectives\n",
    "In this exercise, you will:\n",
    "1. Explore different ways of optimizing the _hls4ml_ conversion\n",
    "2. Compare the optimized models\n",
    "\n",
    "## Instructions\n",
    "Complete the code cells marked with `# TODO` comments. Follow the hints provided.\n",
    "\n",
    "## Part 1: Environment Setup and Data Loading\n",
    "Run the following cells to set up the environment (no changes needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558b04b-4236-4ee8-8f92-2d230b7959ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries and import packages (no changes needed)\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "\n",
    "# Numpy and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TimeSeries to hold our data\n",
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "# Keras, numpy and matplotlib\n",
    "from keras.models import load_model\n",
    "\n",
    "# Local file with some useful methods\n",
    "from utils import *\n",
    "\n",
    "# hls4ml\n",
    "import hls4ml\n",
    "from hls4ml.model.profiling import numerical, get_ymodel_keras\n",
    "\n",
    "# Set the correct libraries path for hls4ml\n",
    "os.environ['XILINX_HLS']    = '/opt/tools/Xilinx/Vitis_HLS/2023.2'\n",
    "os.environ['XILINX_VIVADO'] = '/opt/tools/Xilinx/Vivado/2023.2'\n",
    "os.environ['XILINX_VITIS']  = '/opt/tools/Xilinx/Vitis/2023.2'\n",
    "os.environ['PATH'] = os.environ[\"PATH\"] + \":\" \\\n",
    "                   + os.environ['XILINX_HLS'] + \"/bin:\" \\\n",
    "                   + os.environ['XILINX_VIVADO'] + \"/bin:\" \\\n",
    "                   + os.environ['XILINX_VITIS'] + \"/bin:\"\n",
    "\n",
    "print(\"Environment setup correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf686f-8931-4fed-bc16-e9d56bf397b5",
   "metadata": {},
   "source": [
    "Load the model and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2b3ce-84f4-4730-9ab1-97eb1801bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the \"small encoder\" you trained in the previous exercise\n",
    "\n",
    "# Load the encoder model previously trained\n",
    "# YOUR CODE HERE\n",
    "model = load_model(''' YOUR CODE HERE ''')  # Update with correct name\n",
    "\n",
    "# Load the test data previously saved\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "print(f\"Loaded {X_test.shape[0]} test data with shape {X_test[0].shape}\")\n",
    "\n",
    "# Make sure the output directory to store plots and results exists:\n",
    "if not os.path.exists('inference'):\n",
    "    os.makedirs('inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faacd5a7-9ab8-49f5-a39b-dee4b57a8c2e",
   "metadata": {},
   "source": [
    "## Part 2: Optimization\n",
    "\n",
    "### Task 2.1: Precision\n",
    "You can control how the model parameters are implemented (bit-representation) via the `Precision` parameter in the `hls_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79248dde-dc65-4792-aa0e-fd46877f0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a config with a custom precision and explore its performance\n",
    "\n",
    "# Define a default hls config\n",
    "precision_config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "\n",
    "# YOUR CODE HERE: change the Model precision\n",
    "# Hint: the config is basically a python dictionary, how do you update its keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b21ab-2c7b-4d81-8a37-86a1000b2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define the HLS model\n",
    "\n",
    "# Main config\n",
    "main_cfg = hls4ml.converters.create_config(\n",
    "    board = 'alveo-u55c',            # Target boad and FPGA part\n",
    "    part = 'xcu55c-fsvh2892-2L-e',   #\n",
    "    clock_period = 3,                # Clock period in ns -> 3 ns = ~333 MHz\n",
    "    backend = 'VivadoAccelerator'    # Backend to convert NN -> firmware\n",
    ")\n",
    "\n",
    "# Few more customizations\n",
    "# YOUR CODE HERE\n",
    "main_cfg['HLSConfig'] = None                                                      # Use the updated config\n",
    "main_cfg['IOType'] = 'io_parallel'\n",
    "main_cfg['AcceleratorConfig']['Platform'] = 'xilinx_u55c_gen3x16_xdma_3_202210_1'\n",
    "main_cfg['KerasModel'] = None                                                     # Use the loaded model\n",
    "main_cfg['OutputDir'] = None                                                      # Change the output name\n",
    "\n",
    "print(\"=\"*20, \"Main Config\", \"=\"*20)\n",
    "plotting.print_dict(main_cfg)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the hls model\n",
    "# YOUR CODE HERE\n",
    "precision_model = None   # Replace with correct code\n",
    "\n",
    "# Compile it\n",
    "precision_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884caa02-8bd7-4111-ad89-5a32176476e3",
   "metadata": {},
   "source": [
    "You can now run inference with this updated HLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1727e9-721a-4077-8fb5-72be4373ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y_precision = None # Replace with correct code\n",
    "\n",
    "# Save predictions for later comparisons\n",
    "np.save('y_precision.npy', y_precision)\n",
    "print(\"Precision config predictions saved as 'y_precision.npy'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d90b19-5b54-4a4a-9688-1dea7ddc7063",
   "metadata": {},
   "source": [
    "and compare its performance with the keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d654219-5652-48ac-84dc-6070a8940f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and visualize comparison metrics\n",
    "# 1. MSE per sample: mean of (y_cpu - y_hls)^2 along axis 1\n",
    "# 2. Overall MSE: mean of all MSE per sample\n",
    "# 3. MAE: mean of absolute differences\n",
    "# 4. RMSE: square root of overall MSE\n",
    "\n",
    "# Load the previously saved inferences\n",
    "y_cpu = np.load(\"y_cpu.npy\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mse_per_sample = None  # Calculate MSE for each sample\n",
    "overall_mse = None     # Calculate overall MSE\n",
    "mae = None             # Calculate MAE\n",
    "rmse = None            # Calculate RMSE\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"\\n=== Software vs Precision Reconstruction Metrics ===\")\n",
    "print(f\"Overall MSE           : {overall_mse:.6f}\")\n",
    "print(f\"Average MSE per sample: {np.mean(mse_per_sample):.6f}\")\n",
    "print(f\"Min MSE               : {np.min(mse_per_sample):.6f}\")\n",
    "print(f\"Max MSE               : {np.max(mse_per_sample):.6f}\")\n",
    "print(f\"Mean Absolute Error   : {mae:.6f}\")\n",
    "print(f\"RMSE                  : {rmse:.6f}\")\n",
    "\n",
    "# Visualize comparison for first 3 samples (no changes needed)\n",
    "n_examples = 3\n",
    "fig, axes = plt.subplots(n_examples, 3, figsize=(15, 3*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # CPU predictions\n",
    "    axes[i, 0].plot(y_cpu[i])\n",
    "    axes[i, 0].set_title(f'CPU Prediction {i}')\n",
    "    axes[i, 0].set_ylabel('Amplitude')\n",
    "    axes[i, 0].grid(True)\n",
    "    \n",
    "    # HLS predictions\n",
    "    axes[i, 1].plot(y_precision[i])\n",
    "    axes[i, 1].set_title(f'Precision Prediction {i}')\n",
    "    axes[i, 1].grid(True)\n",
    "    \n",
    "    # Difference\n",
    "    axes[i, 2].plot(y_cpu[i] - y_precision[i])\n",
    "    axes[i, 2].set_title(f'Error (MSE: {mse_per_sample[i]:.4f})')\n",
    "    axes[i, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cpu_hls_comparison.png')\n",
    "print(\"\\nComparison plot saved as 'cpu_hls_precision_comparison.png'\")\n",
    "\n",
    "# Visualize error distribution (no changes needed)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse_per_sample, bins=20, edgecolor='black')\n",
    "plt.xlabel('MSE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(mse_per_sample)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Distribution (Boxplot)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_error_distribution.png')\n",
    "print(\"Error distribution plot saved as 'precision_error_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb945a-dcbf-4aa6-a9cb-9c412daf5324",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "1. How does the new model compare with the keras model?\n",
    "2. How does the new model compare with the previous HLS config?\n",
    "3. Is this what you expected based on the Precision chosen?\n",
    "\n",
    "### Task 2.2: Profiling\n",
    "When the `granularity` parameter of the config is set to `name`, _hls4ml_ allows to finely control the bit representation of all the outputs, biases and weights for each layer singularly.\n",
    "\n",
    "By default all values is set to `auto`, which is an overly-conservative value chosen in order to avoid overflow and truncation issues.\n",
    "We can profit of the _hls4ml_ \"Profiling\" method to manually adjust the configuration and explicitly set the specific widths.\n",
    "\n",
    "Let's define and explore a config with `name` granularity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64097a-7d98-48c0-bfad-e471f47d8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an hls config with \"name\" granularity (no changes needed)\n",
    "profiling_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "print(\"=\"*20, \"Profiling Config\", \"=\"*20)\n",
    "plotting.print_dict(profiling_config)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c267ca6-425c-4699-ba78-62b725ebf960",
   "metadata": {},
   "source": [
    "We can now use the \"Profiling\" tool to explore how well our hls model bit-representation actually \"covers\" the model parameters:\n",
    "\n",
    "> This method plots the distribution of the weights (and biases) as a box and whisker plot. The grey boxes show the values which can be represented with the data types used in the hls_model. Generally, you need the box to overlap completely with the whisker ‘to the right’ (large values) otherwise you’ll get saturation & wrap-around issues. It can be okay for the box not to overlap completely ‘to the left’ (small values), but finding how small you can go is a matter of trial-and-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966fefd-5fd0-45fc-90bc-cbc70c81597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Profiling\n",
    "\n",
    "# Define a minimal model to explore the profiling method\n",
    "# YOUR CODE HERE\n",
    "profiling_model = hls4ml.converters.convert_from_keras_model(\n",
    "    None,            # Your Keras model\n",
    "    hls_config=None  # Your Profiling config\n",
    ")\n",
    "\n",
    "# Run the numerical profiling\n",
    "# YOUR CODE HERE\n",
    "prof = numerical(model=None, hls_model=None) # Your Keras model + Your Profiling model\n",
    "\n",
    "# Bonus:\n",
    "# To also see the layers' Activation Functions bit representation:\n",
    "#  - activate tracing in the layers\n",
    "#  - pass the input data to the numerical() methos\n",
    "\n",
    "#for layer in profiling_config['LayerName'].keys():\n",
    "#    profiling_config['LayerName'][layer]['Trace'] = True\n",
    "\n",
    "#prof = numerical(model=model, hls_model=hls_model, X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a5649-6d61-4eaf-9e83-d12a7015451a",
   "metadata": {},
   "source": [
    "**Excercise:**\n",
    "- Adjust the weights and biases of the layers\n",
    "- Re-run the profiling to check if the new definition is correct\n",
    "- Repeat iteratively untill you are satisfied with the config\n",
    "\n",
    "Hint: to update the layer you have to modify the profiling config, which is still a python dictionary...\n",
    "\n",
    "**Questions:**\n",
    "1. Do you remember the `ap_fixed` notation?\n",
    "2. Will the optimized model use more or less resources with respect the \"deafult config\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b164dc-b43c-4515-bace-2456942d89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare performances with previous models\n",
    "\n",
    "# Main config\n",
    "main_cfg = hls4ml.converters.create_config(\n",
    "    board = 'alveo-u55c',            # Target boad and FPGA part\n",
    "    part = 'xcu55c-fsvh2892-2L-e',   #\n",
    "    clock_period = 3,                # Clock period in ns -> 3 ns = ~333 MHz\n",
    "    backend = 'VivadoAccelerator'    # Backend to convert NN -> firmware\n",
    ")\n",
    "\n",
    "# Few more customizations\n",
    "# YOUR CODE HERE\n",
    "main_cfg['HLSConfig'] = None                                                      # Use the updated config\n",
    "main_cfg['IOType'] = 'io_parallel'\n",
    "main_cfg['AcceleratorConfig']['Platform'] = 'xilinx_u55c_gen3x16_xdma_3_202210_1'\n",
    "main_cfg['KerasModel'] = None                                                     # Use the loaded model\n",
    "main_cfg['OutputDir'] = None                                                      # Change the output name\n",
    "\n",
    "print(\"=\"*20, \"Main Config\", \"=\"*20)\n",
    "plotting.print_dict(main_cfg)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the hls model\n",
    "# YOUR CODE HERE\n",
    "profiled_model = None   # Replace with correct code\n",
    "\n",
    "# Compile it\n",
    "profiled_model.compile()\n",
    "\n",
    "# Run inference\n",
    "# YOUR CODE HERE\n",
    "y_profiled = None   # Replace with correct code\n",
    "\n",
    "# Save predictions for later comparisons\n",
    "np.save('y_profiled.npy', y_profiled)\n",
    "print(\"Profiled config predictions saved as 'y_profiled.npy'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af70dc-306f-4c2d-87f0-90979aa983e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and visualize comparison metrics\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mse_per_sample = None  # Calculate MSE for each sample\n",
    "overall_mse = None     # Calculate overall MSE\n",
    "mae = None             # Calculate MAE\n",
    "rmse = None            # Calculate RMSE\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"\\n=== Software vs Profiled Reconstruction Metrics ===\")\n",
    "print(f\"Overall MSE           : {overall_mse:.6f}\")\n",
    "print(f\"Average MSE per sample: {np.mean(mse_per_sample):.6f}\")\n",
    "print(f\"Min MSE               : {np.min(mse_per_sample):.6f}\")\n",
    "print(f\"Max MSE               : {np.max(mse_per_sample):.6f}\")\n",
    "print(f\"Mean Absolute Error   : {mae:.6f}\")\n",
    "print(f\"RMSE                  : {rmse:.6f}\")\n",
    "\n",
    "# Visualize comparison for first 3 samples (no changes needed)\n",
    "n_examples = 3\n",
    "fig, axes = plt.subplots(n_examples, 3, figsize=(15, 3*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # CPU predictions\n",
    "    axes[i, 0].plot(y_cpu[i])\n",
    "    axes[i, 0].set_title(f'CPU Prediction {i}')\n",
    "    axes[i, 0].set_ylabel('Amplitude')\n",
    "    axes[i, 0].grid(True)\n",
    "    \n",
    "    # HLS predictions\n",
    "    axes[i, 1].plot(y_profiled[i])\n",
    "    axes[i, 1].set_title(f'Precision Prediction {i}')\n",
    "    axes[i, 1].grid(True)\n",
    "    \n",
    "    # Difference\n",
    "    axes[i, 2].plot(y_cpu[i] - y_profiled[i])\n",
    "    axes[i, 2].set_title(f'Error (MSE: {mse_per_sample[i]:.4f})')\n",
    "    axes[i, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cpu_hls_comparison.png')\n",
    "print(\"\\nComparison plot saved as 'cpu_hls_profiled_comparison.png'\")\n",
    "\n",
    "# Visualize error distribution (no changes needed)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse_per_sample, bins=20, edgecolor='black')\n",
    "plt.xlabel('MSE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(mse_per_sample)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Distribution (Boxplot)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('profiled_error_distribution.png')\n",
    "print(\"Error distribution plot saved as 'profiled_error_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150dab55-42c6-452b-ac5f-c0058243b3f2",
   "metadata": {},
   "source": [
    "### Bonus - Tracing\n",
    "Use the \"Trace\" method to collect the model outputs at each layer and identify if/where a too aggressive bit-representation was applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14670341-7568-42ef-9294-9637e1f8abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HLS predictions and trace\n",
    "hls4ml_pred, hls4ml_trace = profiled_model.trace(X_test[:1000])\n",
    "\n",
    "# Get trace from keras using get_ymodel_keras\n",
    "# YOUR CODE HERE\n",
    "keras_trace = get_ymodel_keras('''YOUR CODE HERE''', X_test[:1000]) # Fix name of the loaded keras model\n",
    "\n",
    "# Print and compare\n",
    "print(\"Keras layer 'encoder1', first sample:\")\n",
    "print(keras_trace['encoder1'][0])\n",
    "print(\"hls4ml layer 'encoder1', first sample:\")\n",
    "print(hls4ml_trace['encoder1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8d49a-6503-458c-a297-45546ce1bf02",
   "metadata": {},
   "source": [
    "### Task 2.3: Other Model Optimizations\n",
    "There are other \"Model-Level\" optimization parameters that you can explore:\n",
    "- `Strategy`: refers to the implementation of core matrix-vector multiplication routine, which can be latency-oriented, resource-saving oriented, or specialized.\n",
    "  - Possible values: `\"Latency\"`, `\"Resource\"` \n",
    "- `ReuseFactor`: this defines the pipeline interval or initiation interval, i.e. how many times each resource can/will be re-used in the implementation\n",
    "  - The ReuseFactor can also be defined on a per-layer base\n",
    "  - Possible values: powers of `2`\n",
    "- `BramFactor`: Contols which layers will be implemented as BRAM elements\n",
    "  - Example: setting `BramFactor=100`, only layers with more than 100 weights will be exposed as external BRAM.\n",
    "\n",
    "All these parameters are usually customized together to get to the desired firmware implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c230e-e55c-4cfd-8bdc-f4a544e2a582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: explore the Strategy/ReuseFactor/BramFactor parameters\n",
    "\n",
    "# Hint: Follow the same flow we used so far:\n",
    "\n",
    "# 1. Define a new config\n",
    "# YOUR CODE HERE\n",
    "optimal_config = None  # Use correct code here\n",
    "\n",
    "# Hint: remember we need 1 hls config + 1 main config!\n",
    "\n",
    "# 2. Update one (or more) parameter\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Deine a new HLS model\n",
    "# YOUR CODE HERE\n",
    "optimal_model = None  # Use correct code here\n",
    "\n",
    "# 4. Run inference and \n",
    "y_optimal = optimal_model.predict(X_test)\n",
    "\n",
    "# 5. Compare with previous results (metrics, plots...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30adbb6-db10-48ed-b1a8-88e462f421c7",
   "metadata": {},
   "source": [
    "## Part 4: Check Resources\n",
    "So far, we have only checked how the accuracy of the translated model was, but a crucial point of optimizing the HLS configs is ensuring that we can fit our model into the FPGA resources and that it is running fast enough!  \n",
    "\n",
    "**Exercise**\n",
    "Choose a few of the models you explored above and run the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6fdf-8da1-412b-8ede-b9fb5a0652f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: syntesize model and check the report\n",
    "\n",
    "# Hints and suggestions:\n",
    "# - Remember to define a different output directory for each model\n",
    "# - Remember that each synthesis will take 5-10 minutes, so choose carefully!\n",
    "\n",
    "# Run the synthesis\n",
    "# YOUR CODE HERE\n",
    "my_model.build(csim=False) # -- Fix the model to be syntesised\n",
    "\n",
    "# Print the reports\n",
    "# YOUR CODE HERE\n",
    "print(\"Resource usage and latency:\")\n",
    "print_report(''' YOUR CODE HERE ''') # -- use the chose model output directory"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.10 (hls4ml-ml)",
   "language": "python",
   "name": "hls4ml-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
