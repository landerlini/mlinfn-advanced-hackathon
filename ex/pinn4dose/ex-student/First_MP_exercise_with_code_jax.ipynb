{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437a346a",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Network\n",
    "Physics Informed Neural Networks (PINNs) are a class of deep learning models that incorporate physical laws, typically expressed as partial differential equations (PDEs), into the training process. By embedding these equations into the loss function, PINNs enable neural networks to learn solutions that are consistent with known physics, even when data is scarce or noisy. This approach is particularly useful for solving forward and inverse problems in science, where traditional numerical methods may be computationally expensive or limited by incomplete information.\n",
    "\n",
    "To solve a PDE you need (i) a domain (D) definition, (ii) an initial (or constrined) condition (IC), and (iii) boundary condition (BC). These requirements define a boundary value problem (BVP).\n",
    "Since the aim in a PINN experiment is to find an approximate solution to a BVP, the inputs to these networks are spatiotemporal coordinates and the outputs are the solution values at those points and times.\\\n",
    "During training, we can distinguish a supervised component ‚Äî the data-driven part (e.g., experimental data embedded in the IC) ‚Äî and an unsupervised component ‚Äî the equation-driven part provided by the PDE and the BC.\n",
    "\n",
    "In this exercise we implement a PINN for a simple BVP that admits an analytic solution. This will provide the fundamental tools used subsequently for more complex problems.\n",
    "\n",
    "**NB:** No specific Python library is suggested. Students should write the code using whichever libraries they prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8a01d",
   "metadata": {},
   "source": [
    "## BVP to solve\n",
    "For simplicity (and to not spoil the fun of trying to do better), we choose the 1D heat equation:\n",
    "\n",
    "$\\frac{\\partial u}{\\partial t} = D \\frac{\\partial^2 u}{\\partial x^2}, \\qquad x \\in [0,L],\\ t \\in [0,T]$\n",
    "\n",
    "IC: $u(x,0) = \\sin\\!\\left(\\frac{\\pi x}{L}\\right)$\n",
    "\n",
    "BC: $u(0,t) = u(L,t) = 0 \\quad \\forall t > 0$\n",
    "\n",
    "The analytical solution is:\n",
    "\n",
    "$u(x,t) = \\sin\\!\\left(\\frac{\\pi x}{L}\\right) \\exp\\!\\left(-\\frac{\\pi^2 D}{L^2} t\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdfbd2e",
   "metadata": {},
   "source": [
    "## üìù Exercise Instructions\n",
    "\n",
    "This notebook contains a series of exercises to implement a Physics-Informed Neural Network (PINN) for solving the 1D heat equation. Throughout the notebook, you'll find:\n",
    "\n",
    "- **TODO comments**: Indicating code you need to implement\n",
    "- **Hints**: Providing guidance on implementation details  \n",
    "- **Test cells**: Automated tests to validate your implementations (using `test_pinn.py`)\n",
    "\n",
    "**How to use this notebook:**\n",
    "\n",
    "1. Read each task description carefully\n",
    "2. Implement the required code in the cells marked with `TODO` comments\n",
    "3. Run the test cells to validate your implementation\n",
    "4. Fix any issues before moving to the next section\n",
    "5. All tests should pass before you can successfully train the PINN\n",
    "\n",
    "**Test Suite:**\n",
    "\n",
    "A test suite (`test_pinn.py`) is provided to automatically validate your implementations. Test cells will:\n",
    "- ‚úÖ Show **PASSED** tests in green\n",
    "- ‚ùå Show **FAILED** tests with error messages in red\n",
    "- ‚ö†Ô∏è Show **WARNINGS** for optional features or potential issues\n",
    "\n",
    "Good luck! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f279b51",
   "metadata": {},
   "source": [
    "## 1. Domain definition\n",
    "\n",
    "To solve this equation using a PINN model, you need to define the computational domain and sample points from it.\n",
    "\n",
    "**Task 1.1:** Create a Python `Class` called `Geometry` with the following specifications:\n",
    "\n",
    "**Initialization:**\n",
    "- Input: geometric and topological information of the domain (spatial domain `[0, L]`, temporal domain `[t_start, t_end]`)\n",
    "- Store these parameters for later use\n",
    "\n",
    "**Required Methods:**\n",
    "\n",
    "1. **Method `sample_interior(N1)`** - Sample interior domain points\n",
    "   - Input: `N1` (integer) - number of points to sample\n",
    "   - Output: Array of shape `(N1, 2)` containing random `(x, t)` pairs where `x ‚àà (0, L)` and `t ‚àà (t_start, t_end)`\n",
    "   - Note: Boundary points should be excluded (i.e., `x ‚â† 0` and `x ‚â† L`)\n",
    "\n",
    "2. **Method `sample_boundary(N2)`** - Sample boundary points\n",
    "   - Input: `N2` (integer) - number of points to sample\n",
    "   - Output: Array of shape `(N2, 2)` containing random `(x, t)` pairs where `x ‚àà {0, L}` and `t ‚àà (t_start, t_end)`\n",
    "   - Note: These points should be at the spatial boundaries only\n",
    "\n",
    "3. **Method `sample_initial(N3)`** - Sample initial condition points\n",
    "   - Input: `N3` (integer) - number of points to sample\n",
    "   - Output: Array of shape `(N3, 2)` containing random `(x, 0)` pairs where `x ‚àà [0, L]` and `t = 0`\n",
    "   - Note: These points are for enforcing the initial condition\n",
    "\n",
    "4. **Method `sample_at_time(N4, t1)`** (Optional but recommended)\n",
    "   - Input: `N4` (integer) - number of points, `t1` (float) - specific time\n",
    "   - Output: Array of shape `(N4, 2)` containing random `(x, t1)` pairs\n",
    "   - Note: Useful for future exercises and visualization\n",
    "\n",
    "**‚ö†Ô∏è Important:** Each call to these methods should return a *different* set of random points. Make sure to handle randomness appropriately in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "from scipy.interpolate import interp1d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geometry:\n",
    "    \"\"\"\n",
    "    Geometry class for defining the computational domain and sampling points.\n",
    "    \n",
    "    Domain: x ‚àà [0, L], t ‚àà [t_start, t_end]\n",
    "    \"\"\"\n",
    "    def __init__(self, L=10.0, N=100, t_start=0.0, t_end=1.0, Nt=100):\n",
    "        \"\"\"\n",
    "        Initialize the geometry.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        L : float\n",
    "            Length of the spatial domain\n",
    "        N : int\n",
    "            Number of grid points for visualization\n",
    "        t_start : float\n",
    "            Start time\n",
    "        t_end : float\n",
    "            End time\n",
    "        Nt : int\n",
    "            Number of time points for visualization\n",
    "        \"\"\"\n",
    "        # TODO: Store the domain parameters\n",
    "        # Hint: Save L, t_start, t_end, etc. as instance attributes\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end\n",
    "        self.Nt = Nt\n",
    "        self.dx = L / N\n",
    "        self.dt = (t_end - t_start) / Nt\n",
    "        # Grid points for visualization\n",
    "        self.x = jnp.linspace(0, L, N)\n",
    "        self.t = jnp.linspace(t_start, t_end, Nt)\n",
    "\n",
    "    def sample_interior(self, key, N1):\n",
    "        \"\"\"\n",
    "        Sample N1 random points in the interior of the domain (boundaries excluded).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        key : jax.random.PRNGKey\n",
    "            Random key for JAX random number generation\n",
    "        N1 : int\n",
    "            Number of points to sample\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        points : array of shape (N1, 2)\n",
    "            Array containing (x, t) pairs where x ‚àà (0, L) and t ‚àà (t_start, t_end)\n",
    "        \"\"\"\n",
    "        # TODO: Sample N1 random points in the interior domain\n",
    "        # Hint: Use random.uniform() to sample x in (0, L) and t in (t_start, t_end)\n",
    "        # Hint: Use random.split() to create separate keys for x and t\n",
    "        # Hint: Stack x and t horizontally using jnp.hstack()\n",
    "        raise NotImplementedError(\"Implement sample_interior method\")\n",
    "    \n",
    "    def sample_boundary(self, key, N2):\n",
    "        \"\"\"\n",
    "        Sample N2 random points on the spatial boundaries x=0 and x=L.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        key : jax.random.PRNGKey\n",
    "            Random key for JAX random number generation\n",
    "        N2 : int\n",
    "            Number of points to sample (should be even for equal distribution)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        points : array of shape (N2, 2)\n",
    "            Array containing (x, t) pairs where x ‚àà {0, L} and t ‚àà (t_start, t_end)\n",
    "        \"\"\"\n",
    "        # TODO: Sample N2 random points on the boundaries (x=0 and x=L)\n",
    "        # Hint: Split N2 points between left (x=0) and right (x=L) boundaries\n",
    "        # Hint: Use jnp.zeros() and jnp.full() to create boundary x-coordinates\n",
    "        # Hint: Sample random t values using random.uniform()\n",
    "        raise NotImplementedError(\"Implement sample_boundary method\")\n",
    "    \n",
    "    def sample_initial(self, key, N3):\n",
    "        \"\"\"\n",
    "        Sample N3 random points at the initial time t=0.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        key : jax.random.PRNGKey\n",
    "            Random key for JAX random number generation\n",
    "        N3 : int\n",
    "            Number of points to sample\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        points : array of shape (N3, 2)\n",
    "            Array containing (x, 0) pairs where x ‚àà [0, L] and t = 0\n",
    "        \"\"\"\n",
    "        # TODO: Sample N3 random points at t=0\n",
    "        # Hint: Sample random x values in [0, L]\n",
    "        # Hint: Set all t values to 0 using jnp.zeros()\n",
    "        raise NotImplementedError(\"Implement sample_initial method\")\n",
    "    \n",
    "    def sample_at_time(self, key, N4, t1):\n",
    "        \"\"\"\n",
    "        Sample N4 random points at a specific time t1.\n",
    "        (Optional method - useful for inverse problems and visualization)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        key : jax.random.PRNGKey\n",
    "            Random key for JAX random number generation\n",
    "        N4 : int\n",
    "            Number of points to sample\n",
    "        t1 : float\n",
    "            Specific time value\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        points : array of shape (N4, 2)\n",
    "            Array containing (x, t1) pairs where x ‚àà [0, L]\n",
    "        \"\"\"\n",
    "        # TODO (OPTIONAL): Sample N4 random points at time t1\n",
    "        # Hint: Sample random x values in [0, L]\n",
    "        # Hint: Set all t values to t1 using jnp.full()\n",
    "        raise NotImplementedError(\"Implement sample_at_time method (optional)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803dd6c4",
   "metadata": {},
   "source": [
    "## 1.5 Test Your Geometry Implementation\n",
    "\n",
    "Run the following cell to test your `Geometry` class implementation. The test suite will check:\n",
    "- Proper initialization\n",
    "- Correct output shapes\n",
    "- Points sampled in the right domains\n",
    "- Randomness of sampling\n",
    "\n",
    "Fix any issues before proceeding to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Geometry class\n",
    "from test_pinn import test_geometry\n",
    "\n",
    "test_geometry(Geometry, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9ee3b",
   "metadata": {},
   "source": [
    "## 2. Domain Visualization & Validation\n",
    "\n",
    "**Task 2.1:** Verify your `Geometry` class implementation\n",
    "\n",
    "Create a visualization that displays sampled points in the `(x, t)` plane with:\n",
    "- Interior points (from `sample_interior`) in one color\n",
    "- Boundary points (from `sample_boundary`) in another color  \n",
    "- Initial condition points (from `sample_initial`) in a third color\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Does your visualization show that points are sampled from the correct regions?\n",
    "2. Are the boundary points only at `x = 0` and `x = L`?\n",
    "3. Are the initial condition points only at `t = 0`?\n",
    "4. What would be a good choice of `(N1, N2, N3)` to adequately sample the domain? Consider:\n",
    "   - The relative importance of each constraint (PDE, BC, IC)\n",
    "   - The domain size and dimensionality\n",
    "   - Computational cost vs. accuracy trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain parameters\n",
    "L = 10.0          # Spatial domain length\n",
    "t_start = 0.0     # Start time\n",
    "t_end = 1.0       # End time\n",
    "\n",
    "# Create geometry object\n",
    "geo = Geometry(L=L, N=100, t_start=t_start, t_end=t_end, Nt=100)\n",
    "\n",
    "# Sample points for visualization\n",
    "key = random.PRNGKey(0)\n",
    "N1 = 1000  # Interior points\n",
    "N2 = 200   # Boundary points\n",
    "N3 = 200   # Initial condition points\n",
    "\n",
    "try:\n",
    "    # Generate different types of points\n",
    "    key, subkey1, subkey2, subkey3 = random.split(key, 4)\n",
    "    points_interior = geo.sample_interior(subkey1, N1)\n",
    "    points_boundary = geo.sample_boundary(subkey2, N2)\n",
    "    points_initial = geo.sample_initial(subkey3, N3)\n",
    "\n",
    "    # Visualize the sampled points\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(points_interior[:, 0], points_interior[:, 1], \n",
    "                label='Interior (PDE)', alpha=0.5, s=10)\n",
    "    plt.scatter(points_boundary[:, 0], points_boundary[:, 1], \n",
    "                label='Boundary (BC)', alpha=0.7, s=20)\n",
    "    plt.scatter(points_initial[:, 0], points_initial[:, 1], \n",
    "                label='Initial (IC)', alpha=0.7, s=20)\n",
    "    plt.xlim(-0.5, L+0.5)\n",
    "    plt.ylim(-0.05, t_end+0.05)\n",
    "    plt.xlabel('x (spatial coordinate)')\n",
    "    plt.ylabel('t (time)')\n",
    "    plt.title('Sampled Points in the (x,t) Domain')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "except NotImplementedError as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot visualize: {e}\")\n",
    "    print(\"Please implement the Geometry methods first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e421b",
   "metadata": {},
   "source": [
    "## 3. Neural Network Architecture\n",
    "\n",
    "**Task 3.1:** Define a Neural Network that will approximate the solution `u(x, t)`\n",
    "\n",
    "**Network Requirements:**\n",
    "\n",
    "1. **Architecture constraints:**\n",
    "   - Input layer: dimension = 2 (representing `x` and `t`)\n",
    "   - Hidden layers: number and width are yours to choose (see tips below)\n",
    "   - Output layer: dimension = 1 (representing `u(x, t)`)\n",
    "   - Must satisfy the universal approximation theorem\n",
    "\n",
    "2. **Activation function:**\n",
    "   - Must be continuously differentiable (the network needs to compute derivatives)\n",
    "   - Common choices: `tanh`, `sigmoid`, `softplus`, `GELU`\n",
    "   - Avoid: `ReLU` (non-differentiable at 0, though often works in practice)\n",
    "\n",
    "3. **Input/Output specification:**\n",
    "   - Input: Array of shape `(N, 2)` containing `(x, t)` coordinates\n",
    "   - Output: Array of shape `(N, 1)` containing predicted `u(x, t)` values\n",
    "\n",
    "**üí° Tips:**\n",
    "\n",
    "1. **Model capacity:** Start simple, then increase complexity if needed\n",
    "   - For this 1D heat equation, try starting with 2-3 hidden layers of 20-50 neurons each\n",
    "   - If the model doesn't learn well, consider increasing depth or width\n",
    "   - If training is slow or unstable, try reducing complexity\n",
    "\n",
    "2. **Parameter initialization:** This is critical for PINN training!\n",
    "   - **Problem:** Random normal initialization often leads to trivial solutions (e.g., `u ‚âà 0` everywhere)\n",
    "   - **Solutions:**\n",
    "     - Use **Glorot/Xavier uniform initialization** (strongly recommended)\n",
    "     - Alternatively, use careful loss weighting (covered later)\n",
    "   - Most deep learning libraries provide Glorot initialization (e.g., `glorot_uniform` in Keras/JAX)\n",
    "\n",
    "3. **Implementation tips:**\n",
    "   - Your network should be a callable function/class that takes `(x, t)` coordinates and returns `u` predictions\n",
    "   - Ensure you can access intermediate gradients (needed for computing derivatives in the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7893fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn.initializers import glorot_uniform\n",
    "\n",
    "class Pinn:\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for approximating the solution u(x, t).\n",
    "    \n",
    "    The network takes (x, t) coordinates as input and outputs u(x, t) predictions.\n",
    "    Uses Glorot uniform initialization to avoid trivial solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, key, layers):\n",
    "        \"\"\"\n",
    "        Initialize the PINN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        key : jax.random.PRNGKey\n",
    "            Random key for parameter initialization\n",
    "        layers : list of int\n",
    "            Architecture specification, e.g., [2, 20, 20, 20, 1]\n",
    "            - First element: input dimension (2 for x and t)\n",
    "            - Middle elements: hidden layer widths\n",
    "            - Last element: output dimension (1 for u)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.params = self.initialize_params(key, layers)\n",
    "\n",
    "    def initialize_params(self, key, layers):\n",
    "        \"\"\"\n",
    "        Initialize network parameters using Glorot uniform initialization.\n",
    "        This helps avoid convergence to trivial solutions (u ‚âà 0 everywhere).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        params : list of tuples\n",
    "            Each tuple contains (weights, biases) for one layer\n",
    "        \"\"\"\n",
    "        # TODO: Initialize network parameters with Glorot uniform initialization\n",
    "        # Hint: Use random.split() to create separate keys for each layer\n",
    "        # Hint: Use glorot_uniform()(key, shape) to initialize weights\n",
    "        # Hint: Initialize biases to zero using jnp.zeros()\n",
    "        # Hint: Store each layer's (W, b) as a tuple in a list\n",
    "        # Hint: You need len(layers)-1 sets of parameters (one per connection)\n",
    "        raise NotImplementedError(\"Implement initialize_params method\")\n",
    "\n",
    "    def neural_net(self, params, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : list of tuples\n",
    "            Network parameters (weights and biases)\n",
    "        x : array of shape (N, 2)\n",
    "            Input coordinates (x, t)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        output : array of shape (N, 1)\n",
    "            Predicted values u(x, t)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass through the network\n",
    "        # Hint: Loop through all layers except the last one\n",
    "        # Hint: Apply tanh activation for hidden layers: jnp.tanh(jnp.dot(x, W) + b)\n",
    "        # Hint: Last layer has no activation (linear output)\n",
    "        # Hint: Each layer transforms: x = activation(x @ W + b)\n",
    "        raise NotImplementedError(\"Implement neural_net method\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make the network callable: pinn(x) instead of pinn.neural_net(pinn.params, x)\n",
    "        \"\"\"\n",
    "        return self.neural_net(self.params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2932b9",
   "metadata": {},
   "source": [
    "## 4. Neural Network Validation\n",
    "\n",
    "**Task 4.1:** Test your network implementation\n",
    "\n",
    "Before moving to training, verify that:\n",
    "1. Your network accepts inputs of shape `(N, 2)` and produces outputs of shape `(N, 1)`\n",
    "2. You can generate sample points using your `Geometry` class and pass them through the network\n",
    "3. The output values are reasonable (not `NaN` or extremely large)\n",
    "\n",
    "Run the test cell below to validate your PINN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the PINN implementation\n",
    "from test_pinn import test_pinn\n",
    "\n",
    "layers = [2, 20, 20, 20, 1]  # Architecture: 2 inputs -> 3 hidden layers (20 neurons each) -> 1 output\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "try:\n",
    "    pinn = Pinn(key, layers)\n",
    "    test_pinn(Pinn, verbose=True)\n",
    "    \n",
    "    # Additional manual test\n",
    "    print(\"\\nManual forward pass test:\")\n",
    "    x_test = jnp.array([[1.0, 0.5],   # (x=1.0, t=0.5)\n",
    "                         [2.0, 0.5]])   # (x=2.0, t=0.5)\n",
    "    u_test = pinn(x_test)\n",
    "    print(\"Test inputs (x, t):\")\n",
    "    print(x_test)\n",
    "    print(\"\\nTest outputs u(x, t):\")\n",
    "    print(u_test)\n",
    "except NotImplementedError as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot test PINN: {e}\")\n",
    "    print(\"Please implement the Pinn methods first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba086ca",
   "metadata": {},
   "source": [
    "## 5. Loss Functions\n",
    "\n",
    "**Task 5.1:** Implement the physics-informed loss function\n",
    "\n",
    "The loss function is what makes this network \"physics-informed\". It enforces the PDE, boundary conditions, and initial conditions. We split it into three components:\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 PDE Residual Loss: $\\mathcal{L}_{PDE}$\n",
    "\n",
    "This enforces that the network satisfies the heat equation at interior points.\n",
    "\n",
    "**Mathematical definition:**\n",
    "\n",
    "$$\\mathcal{L}_{PDE}= \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\left(\\frac{\\partial}{\\partial t}NN(x_i,t_i)-D\\frac{\\partial^2}{\\partial x^2}NN(x_i,t_i)\\right)^2$$\n",
    "\n",
    "**Implementation requirements:**\n",
    "- **Input:** \n",
    "  - Interior points `(x, t)` from `sample_interior(N1)` - shape `(N1, 2)`\n",
    "  - Network prediction function `NN(x, t)`\n",
    "  - Diffusion coefficient `D`\n",
    "- **Steps:**\n",
    "  1. For each point `(xi, ti)`, compute `u = NN(xi, ti)`\n",
    "  2. Compute time derivative: `‚àÇu/‚àÇt` using automatic differentiation\n",
    "  3. Compute second spatial derivative: `‚àÇ¬≤u/‚àÇx¬≤` using automatic differentiation\n",
    "  4. Calculate residual: `r = ‚àÇu/‚àÇt - D¬∑‚àÇ¬≤u/‚àÇx¬≤`\n",
    "  5. Return mean squared residual: `mean(r¬≤)`\n",
    "- **Output:** Scalar value representing PDE residual loss\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Boundary Condition Loss: $\\mathcal{L}_{BC}$\n",
    "\n",
    "This enforces Dirichlet boundary conditions: `u(0, t) = u(L, t) = 0` for all `t > 0`.\n",
    "\n",
    "**Mathematical definition:**\n",
    "\n",
    "$$\\mathcal{L}_{BC}= \\frac{1}{N_2} \\sum_{i=1}^{N_2} (NN(x_i, t_i))^2 \\quad \\text{where } x_i \\in \\{0, L\\}$$\n",
    "\n",
    "**Implementation requirements:**\n",
    "- **Input:**\n",
    "  - Boundary points from `sample_boundary(N2)` - shape `(N2, 2)`, where `x ‚àà {0, L}`\n",
    "  - Network prediction function `NN(x, t)`\n",
    "- **Steps:**\n",
    "  1. Evaluate network at boundary points: `u_boundary = NN(x_boundary, t_boundary)`\n",
    "  2. Return mean squared predictions: `mean(u_boundary¬≤)`\n",
    "- **Output:** Scalar value representing boundary condition loss\n",
    "- **Note:** Since the boundary condition is `u = 0`, we simply penalize any non-zero predictions\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Initial Condition Loss: $\\mathcal{L}_{IC}$\n",
    "\n",
    "This is the *supervised* component, enforcing the initial condition: `u(x, 0) = sin(œÄx/L)`.\n",
    "\n",
    "**Mathematical definition:**\n",
    "\n",
    "$$\\mathcal{L}_{IC}=\\frac{1}{N_3} \\sum_{i=1}^{N_3} (NN(x_i,0)-\\sin(\\pi x_i/L))^2$$\n",
    "\n",
    "**Implementation requirements:**\n",
    "- **Input:**\n",
    "  - Initial condition points from `sample_initial(N3)` - shape `(N3, 2)`, where `t = 0`\n",
    "  - Network prediction function `NN(x, t)`\n",
    "  - Domain length `L`\n",
    "- **Steps:**\n",
    "  1. Evaluate network at initial points: `u_pred = NN(x_initial, 0)`\n",
    "  2. Compute exact initial condition: `u_exact = sin(œÄ¬∑x_initial/L)`\n",
    "  3. Return mean squared error: `mean((u_pred - u_exact)¬≤)`\n",
    "- **Output:** Scalar value representing initial condition loss\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Total Loss\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{PDE} + \\mathcal{L}_{BC} + \\mathcal{L}_{IC}$$\n",
    "\n",
    "Optionally, you can use weighted losses: $\\mathcal{L}_{total} = w_1\\mathcal{L}_{PDE} + w_2\\mathcal{L}_{BC} + w_3\\mathcal{L}_{IC}$\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Implementation Tips:**\n",
    "\n",
    "1. **Automatic differentiation:** Use your framework's built-in tools\n",
    "   - TensorFlow: `tf.GradientTape` (set `persistent=True` to compute multiple derivatives)\n",
    "   - PyTorch: `torch.autograd.grad`\n",
    "   - JAX: `jax.grad` or `jax.jacfwd`/`jax.jacrev`\n",
    "\n",
    "2. **Computing higher-order derivatives:**\n",
    "   - For `‚àÇ¬≤u/‚àÇx¬≤`, you need to differentiate twice with respect to `x`\n",
    "   - Ensure your automatic differentiation context can handle multiple derivative operations\n",
    "\n",
    "3. **Debugging:** Test each loss component separately with known simple functions before combining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "def loss_pde(params, pinn, points, D):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual loss: L_PDE.\n",
    "    \n",
    "    Enforces the heat equation: ‚àÇu/‚àÇt = D * ‚àÇ¬≤u/‚àÇx¬≤\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : list\n",
    "        Network parameters\n",
    "    pinn : Pinn\n",
    "        PINN object\n",
    "    points : array of shape (N1, 2)\n",
    "        Interior points (x, t)\n",
    "    D : float\n",
    "        Diffusion coefficient\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Mean squared PDE residual\n",
    "    \"\"\"\n",
    "    def pde_residual(point):\n",
    "        \"\"\"Compute PDE residual at a single point.\"\"\"\n",
    "        x, t = point[0], point[1]\n",
    "        \n",
    "        # Define function u(x, t) for automatic differentiation\n",
    "        def u_fn(x_val, t_val):\n",
    "            input_point = jnp.array([[x_val, t_val]])\n",
    "            return pinn.neural_net(params, input_point)[0, 0]\n",
    "        \n",
    "        # TODO: Compute derivatives using JAX automatic differentiation\n",
    "        # Hint: Use grad(u_fn, argnums=1)(x, t) for ‚àÇu/‚àÇt\n",
    "        # Hint: Use grad(grad(u_fn, argnums=0), argnums=0)(x, t) for ‚àÇ¬≤u/‚àÇx¬≤\n",
    "        # Hint: PDE residual is: ‚àÇu/‚àÇt - D * ‚àÇ¬≤u/‚àÇx¬≤\n",
    "        # Hint: Return the squared residual\n",
    "        raise NotImplementedError(\"Implement PDE residual computation\")\n",
    "    \n",
    "    # Vectorize the residual computation over all points\n",
    "    residuals = vmap(pde_residual)(points)\n",
    "    \n",
    "    return jnp.mean(residuals)\n",
    "\n",
    "\n",
    "def loss_bc(params, pinn, points):\n",
    "    \"\"\"\n",
    "    Compute the boundary condition loss: L_BC.\n",
    "    \n",
    "    Enforces Dirichlet BC: u(0, t) = u(L, t) = 0\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : list\n",
    "        Network parameters\n",
    "    pinn : Pinn\n",
    "        PINN object\n",
    "    points : array of shape (N2, 2)\n",
    "        Boundary points where x ‚àà {0, L}\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Mean squared boundary predictions (should be zero)\n",
    "    \"\"\"\n",
    "    # TODO: Implement boundary condition loss\n",
    "    # Hint: Evaluate the network at boundary points using pinn.neural_net(params, points)\n",
    "    # Hint: Since BC is u=0, compute mean((predictions)¬≤)\n",
    "    raise NotImplementedError(\"Implement loss_bc\")\n",
    "\n",
    "\n",
    "def loss_ic(params, pinn, points, L):\n",
    "    \"\"\"\n",
    "    Compute the initial condition loss: L_IC.\n",
    "    \n",
    "    Enforces IC: u(x, 0) = sin(œÄx/L)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : list\n",
    "        Network parameters\n",
    "    pinn : Pinn\n",
    "        PINN object\n",
    "    points : array of shape (N3, 2)\n",
    "        Initial condition points where t = 0\n",
    "    L : float\n",
    "        Domain length\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Mean squared error between predictions and exact IC\n",
    "    \"\"\"\n",
    "    # TODO: Implement initial condition loss\n",
    "    # Hint: Get network predictions at t=0 using pinn.neural_net(params, points)\n",
    "    # Hint: Compute exact initial condition: sin(œÄx/L) where x = points[:, 0:1]\n",
    "    # Hint: Return mean squared error between predictions and exact values\n",
    "    raise NotImplementedError(\"Implement loss_ic\")\n",
    "\n",
    "\n",
    "def total_loss(params, pinn, points_interior, points_boundary, points_initial, D, L):\n",
    "    \"\"\"\n",
    "    Compute the total physics-informed loss.\n",
    "    \n",
    "    L_total = L_PDE + L_BC + L_IC\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : list\n",
    "        Network parameters\n",
    "    pinn : Pinn\n",
    "        PINN object\n",
    "    points_interior : array\n",
    "        Interior domain points\n",
    "    points_boundary : array\n",
    "        Boundary points\n",
    "    points_initial : array\n",
    "        Initial condition points\n",
    "    D : float\n",
    "        Diffusion coefficient\n",
    "    L : float\n",
    "        Domain length\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    total : float\n",
    "        Total loss value\n",
    "    components : tuple\n",
    "        Individual loss components (l_pde, l_bc, l_ic) for monitoring\n",
    "    \"\"\"\n",
    "    # TODO: Implement total loss\n",
    "    # Hint: Compute each loss component using the functions above\n",
    "    # Hint: Sum them to get total loss\n",
    "    # Hint: Return (total, (l_pde, l_bc, l_ic)) for monitoring\n",
    "    raise NotImplementedError(\"Implement total_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b87f9a",
   "metadata": {},
   "source": [
    "## 6. Loss Function Validation\n",
    "\n",
    "**Task 6.1:** Test your loss implementations\n",
    "\n",
    "Before training, verify that each loss component works correctly:\n",
    "\n",
    "1. **Test with simple inputs:**\n",
    "   - Sample a small number of points (e.g., `N1=10`, `N2=10`, `N3=10`)\n",
    "   - Compute each loss term separately\n",
    "   - Check that all values are finite (not `NaN` or `Inf`)\n",
    "\n",
    "2. **Sanity checks:**\n",
    "   - For a randomly initialized network, all losses should be non-zero\n",
    "   - `L_BC` should be relatively small if the network happens to predict values close to zero at boundaries\n",
    "   - `L_IC` depends on how far the random predictions are from `sin(œÄx/L)`\n",
    "\n",
    "3. **Test derivatives:**\n",
    "   - Ensure automatic differentiation is computing gradients correctly\n",
    "   - Try evaluating `‚àÇu/‚àÇt` and `‚àÇ¬≤u/‚àÇx¬≤` at a single point and verify they are computable\n",
    "\n",
    "**Debugging tip:** If you encounter errors, test each loss function independently before combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68076e",
   "metadata": {},
   "source": [
    "## 6.5 Run All Tests\n",
    "\n",
    "Before proceeding to training, run this comprehensive test to ensure all components are working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf13449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive test suite\n",
    "from test_pinn import run_all_tests\n",
    "\n",
    "try:\n",
    "    all_passed = run_all_tests(\n",
    "        Geometry=Geometry,\n",
    "        Pinn=Pinn,\n",
    "        loss_pde=loss_pde,\n",
    "        loss_bc=loss_bc,\n",
    "        loss_ic=loss_ic,\n",
    "        total_loss=total_loss,\n",
    "        train_pinn=None,  # We'll test training separately\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"‚úÖ All core components are implemented correctly!\")\n",
    "        print(\"You can now proceed to implementing the training function.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Please fix the failing tests before proceeding to training.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running tests: {e}\")\n",
    "    print(\"Make sure all required functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loss functions with test suite\n",
    "from test_pinn import test_loss_functions\n",
    "\n",
    "try:\n",
    "    test_loss_functions(loss_pde, loss_bc, loss_ic, total_loss, Pinn, Geometry, verbose=True)\n",
    "    \n",
    "    # Additional manual test\n",
    "    print(\"\\nManual loss computation test:\")\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Sample test points\n",
    "    N1_test = 100\n",
    "    N2_test = 20\n",
    "    N3_test = 20\n",
    "    \n",
    "    key, subkey1, subkey2, subkey3 = random.split(key, 4)\n",
    "    points_interior = geo.sample_interior(subkey1, N1_test)\n",
    "    points_boundary = geo.sample_boundary(subkey2, N2_test)\n",
    "    points_initial = geo.sample_initial(subkey3, N3_test)\n",
    "    \n",
    "    # Compute each loss component\n",
    "    D_test = 3\n",
    "    l_pde = loss_pde(pinn.params, pinn, points_interior, D_test)\n",
    "    l_bc = loss_bc(pinn.params, pinn, points_boundary)\n",
    "    l_ic = loss_ic(pinn.params, pinn, points_initial, geo.L)\n",
    "    \n",
    "    print(\"Loss components for untrained network:\")\n",
    "    print(f\"  L_PDE (PDE residual):        {l_pde:.6f}\")\n",
    "    print(f\"  L_BC  (Boundary conditions): {l_bc:.6f}\")\n",
    "    print(f\"  L_IC  (Initial condition):   {l_ic:.6f}\")\n",
    "    print(f\"  L_total:                     {l_pde + l_bc + l_ic:.6f}\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot test losses: {e}\")\n",
    "    print(\"Please implement the loss functions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80c70e",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "**Task 7.1:** Implement the training procedure\n",
    "\n",
    "Now that all components are ready, implement the optimization loop to train your PINN.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.1 Core Training Operations\n",
    "\n",
    "Your training loop should perform these steps at each iteration:\n",
    "\n",
    "1. **Sample points:** Generate new random sets of:\n",
    "   - Interior points `(x, t)` using `sample_interior(N1)`\n",
    "   - Boundary points `(x, t)` using `sample_boundary(N2)`\n",
    "   - Initial condition points `(x, 0)` using `sample_initial(N3)`\n",
    "\n",
    "2. **Forward pass & loss computation:**\n",
    "   - Evaluate the network at all sampled points\n",
    "   - Compute `L_PDE`, `L_BC`, `L_IC`, and `L_total`\n",
    "\n",
    "3. **Backward pass & optimization:**\n",
    "   - Compute gradients of `L_total` with respect to network parameters\n",
    "   - Update parameters using an optimizer (Adam recommended)\n",
    "\n",
    "4. **Monitoring:**\n",
    "   - Print/log loss values periodically\n",
    "   - Track the best model (lowest total loss)\n",
    "   - Save checkpoints when a new minimum is reached\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 Implementation Suggestions\n",
    "\n",
    "For clean, readable code, consider organizing as follows:\n",
    "\n",
    "**Option A: Unified PDE function**\n",
    "- Create a function `compute_pde_loss(model, interior_points, boundary_points, initial_points, D, L)` that:\n",
    "  - Takes all inputs and hyperparameters\n",
    "  - Returns all loss components `(L_total, L_PDE, L_BC, L_IC)`\n",
    "- Your training loop then only needs:\n",
    "  1. Sample points from `Geometry` class\n",
    "  2. Call `compute_pde_loss()` within gradient computation context\n",
    "  3. Apply optimizer step\n",
    "  4. Monitor and save\n",
    "\n",
    "**Option B: Separate components**\n",
    "- Keep loss functions separate as implemented in Task 5\n",
    "- Call them individually in the training loop\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 Training Hyperparameters\n",
    "\n",
    "**Optimizer:**\n",
    "- **Recommended:** Adam with learning rate `1e-3` to `1e-4`\n",
    "- **Alternative:** Try L-BFGS for fine-tuning after initial Adam training (see tips below)\n",
    "\n",
    "**Number of epochs:**\n",
    "- Start with 5,000-10,000 epochs\n",
    "- Monitor loss curves to determine if more training is needed\n",
    "\n",
    "**Sampling sizes:**\n",
    "- Typical values: `N1=1000`, `N2=200`, `N3=200`\n",
    "- Adjust based on computational resources and convergence behavior\n",
    "\n",
    "---\n",
    "\n",
    "### 7.4 Tips & Best Practices\n",
    "\n",
    "**1. Loss weighting (if needed):**\n",
    "   - If one loss component dominates, consider weighting: `L_total = w1¬∑L_PDE + w2¬∑L_BC + w3¬∑L_IC`\n",
    "   - Start with equal weights (1, 1, 1) and adjust if necessary\n",
    "\n",
    "**2. Advanced optimization - L-BFGS:**\n",
    "   - Some PINN literature recommends [L-BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) (second-order optimizer)\n",
    "   - **Strategy:** Train with Adam first, then fine-tune with L-BFGS\n",
    "   - Not required but can improve convergence\n",
    "\n",
    "**3. Monitoring:**\n",
    "   - Print each loss component separately to diagnose issues\n",
    "   - If one component doesn't decrease, check its implementation\n",
    "\n",
    "**4. Stopping criterion:**\n",
    "   - **Key question:** *Without a validation set, how do you decide when to stop training?*\n",
    "   - Consider:\n",
    "     - Has the total loss plateaued?\n",
    "     - Are all three loss components sufficiently small?\n",
    "     - Does the solution look physically reasonable?\n",
    "     - Compare with analytical solution (if available, as in this exercise)\n",
    "\n",
    "**5. Checkpointing:**\n",
    "   - Save the model parameters whenever `L_total` reaches a new minimum\n",
    "   - This protects against divergence in later epochs\n",
    "\n",
    "---\n",
    "\n",
    "### 7.5 Expected Behavior\n",
    "\n",
    "During training, you should observe:\n",
    "- All loss components decreasing over time\n",
    "- `L_IC` typically decreases fastest (it's supervised)\n",
    "- `L_PDE` often decreases more slowly\n",
    "- Total loss should eventually plateau at a low value (e.g., < 0.01 or lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965cd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pinn(pinn, geo, D, L, n_epochs=5000, N1=1000, N2=200, N3=200, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Train the Physics-Informed Neural Network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pinn : Pinn\n",
    "        PINN object to train\n",
    "    geo : Geometry\n",
    "        Geometry object for sampling points\n",
    "    D : float\n",
    "        Diffusion coefficient\n",
    "    L : float\n",
    "        Domain length\n",
    "    n_epochs : int\n",
    "        Number of training iterations\n",
    "    N1, N2, N3 : int\n",
    "        Number of interior, boundary, and initial condition points per epoch\n",
    "    learning_rate : float\n",
    "        Adam optimizer learning rate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loss_history : list\n",
    "        Total loss at each epoch\n",
    "    loss_components : dict\n",
    "        History of individual loss components\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop\n",
    "    # Hint: Initialize Adam optimizer using optax.adam(learning_rate)\n",
    "    # Hint: Initialize optimizer state with optimizer.init(pinn.params)\n",
    "    # Hint: Track best_loss and best_params to save the best model\n",
    "    # Hint: Create loss_history list and loss_components dict to track losses\n",
    "    \n",
    "    # TODO: For each epoch:\n",
    "    #   1. Sample new random points using geo.sample_interior/boundary/initial\n",
    "    #   2. Compute loss and gradients using jax.value_and_grad(total_loss, has_aux=True)\n",
    "    #   3. Update parameters: updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    #                        pinn.params = optax.apply_updates(pinn.params, updates)\n",
    "    #   4. Store loss values in history\n",
    "    #   5. Print progress every 500 epochs\n",
    "    \n",
    "    # TODO: After training, restore best parameters\n",
    "    # TODO: Return loss_history and loss_components\n",
    "    \n",
    "    raise NotImplementedError(\"Implement train_pinn function\")\n",
    "\n",
    "\n",
    "# Train the PINN\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING PHYSICS-INFORMED NEURAL NETWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define problem parameters\n",
    "L = 10.0\n",
    "D = 3\n",
    "\n",
    "# Create a fresh PINN\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "try:\n",
    "    pinn = Pinn(key, layers)\n",
    "    \n",
    "    # Train with reduced epochs for exercise (increase for better results)\n",
    "    loss_history, loss_components = train_pinn(\n",
    "        pinn, geo, D, L, \n",
    "        n_epochs=1000,  # Increase to 5000-10000 for better results\n",
    "        N1=1000, N2=200, N3=200,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Total loss\n",
    "    axes[0].plot(loss_history, linewidth=1.5)\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Total Loss')\n",
    "    axes[0].set_title('Training Loss History')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Individual components\n",
    "    axes[1].plot(loss_components['pde'], label='L_PDE', linewidth=1.5)\n",
    "    axes[1].plot(loss_components['bc'], label='L_BC', linewidth=1.5)\n",
    "    axes[1].plot(loss_components['ic'], label='L_IC', linewidth=1.5)\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss Components')\n",
    "    axes[1].set_title('Individual Loss Components')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except NotImplementedError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cannot train: {e}\")\n",
    "    print(\"Please implement the train_pinn function first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20f20a",
   "metadata": {},
   "source": [
    "## 8. Performance Evaluation\n",
    "\n",
    "**Task 8.1:** Evaluate and analyze your trained PINN\n",
    "\n",
    "Now that your model is trained, assess its performance and understand its behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1 Quantitative Metrics\n",
    "\n",
    "**Primary metric - Mean Absolute Error (MAE):**\n",
    "\n",
    "Since we have the analytical solution for this problem:\n",
    "\n",
    "$$u_{exact}(x,t) = \\sin\\!\\left(\\frac{\\pi x}{L}\\right) \\exp\\!\\left(-\\frac{\\pi^2 D}{L^2} t\\right)$$\n",
    "\n",
    "Compute:\n",
    "- **Absolute Error:** `|u_pred(x,t) - u_exact(x,t)|`\n",
    "- **Mean Absolute Error:** Average absolute error over the domain\n",
    "- **Maximum Error:** Worst-case pointwise error\n",
    "- **Relative Error:** `|u_pred - u_exact| / |u_exact|` (where `u_exact ‚â† 0`)\n",
    "\n",
    "**Questions to answer:**\n",
    "1. What is the average error across the entire domain?\n",
    "2. Where is the error largest? (boundaries? interior? early/late times?)\n",
    "3. Is the error acceptable for your application?\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2 Qualitative Assessment\n",
    "\n",
    "**Visualization tasks:**\n",
    "\n",
    "1. **Solution comparison:**\n",
    "   - Plot `u_pred(x,t)` as a heatmap/contour plot\n",
    "   - Plot `u_exact(x,t)` as a heatmap/contour plot\n",
    "   - Visually compare: Does the PINN capture the correct behavior?\n",
    "\n",
    "2. **Error visualization:**\n",
    "   - Plot absolute error `|u_pred - u_exact|` as a heatmap\n",
    "   - Identify regions of high/low error\n",
    "\n",
    "3. **Time slices:**\n",
    "   - Plot `u(x, t)` vs `x` at several fixed times (e.g., `t = 0, 0.25, 0.5, 0.75, 1.0`)\n",
    "   - Compare PINN predictions with analytical solution\n",
    "   - Does the solution decay correctly over time?\n",
    "\n",
    "4. **Spatial profiles:**\n",
    "   - Plot `u(x, t)` vs `t` at several fixed locations (e.g., `x = L/4, L/2, 3L/4`)\n",
    "   - Verify exponential decay in time\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3 Comparison with Classical Methods\n",
    "\n",
    "**Task 8.2:** Compare PINN with numerical methods\n",
    "\n",
    "To understand PINNs' strengths and limitations, compare with traditional PDE solvers:\n",
    "\n",
    "**Suggested method - FTCS (Forward-Time Central-Space):**\n",
    "- Simple explicit finite difference scheme for the heat equation\n",
    "- Discretize in space and time: `u[n+1, i] = u[n, i] + D¬∑dt/dx¬≤¬∑(u[n,i+1] - 2u[n,i] + u[n,i-1])`\n",
    "- **Note:** Check stability condition: `D¬∑dt/dx¬≤ ‚â§ 0.5`\n",
    "\n",
    "**Comparison criteria:**\n",
    "\n",
    "1. **Accuracy:**\n",
    "   - Which method has lower error compared to the analytical solution?\n",
    "   - How does error scale with grid resolution (for FTCS) vs network size (for PINN)?\n",
    "\n",
    "2. **Computational cost:**\n",
    "   - Training time for PINN vs solve time for FTCS\n",
    "   - Memory requirements\n",
    "\n",
    "3. **Flexibility:**\n",
    "   - How easy is it to change domain size, boundary conditions, or initial conditions?\n",
    "   - Which method generalizes better to new scenarios?\n",
    "\n",
    "4. **Physical constraints:**\n",
    "   - Does the PINN naturally satisfy physical properties (e.g., positivity, conservation)?\n",
    "   - Compare with FTCS in this regard\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 Reflection Questions\n",
    "\n",
    "1. **When would you prefer a PINN over classical methods?**\n",
    "   - Consider: sparse data, inverse problems, complex geometries, parameter estimation\n",
    "\n",
    "2. **What are the main challenges you encountered?**\n",
    "   - Hyperparameter tuning? Training stability? Computational cost?\n",
    "\n",
    "3. **How would this approach scale to higher dimensions (2D/3D)?**\n",
    "   - Think about: computational complexity, sampling strategies, network architecture\n",
    "\n",
    "4. **What happens if you reduce the number of training points?**\n",
    "   - Try reducing `N1`, `N2`, or `N3` and observe the effect on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE EVALUATION: Compare PINN predictions with analytical solution\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    # Create a grid for evaluation\n",
    "    x = geo.x\n",
    "    t = geo.t\n",
    "    X, T = jnp.meshgrid(x, t)\n",
    "\n",
    "    # Flatten for network evaluation\n",
    "    XT = jnp.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "    # PINN predictions\n",
    "    u_pred = pinn(XT).reshape(X.shape)\n",
    "\n",
    "    # Analytical solution: u(x,t) = sin(œÄx/L) * exp(-œÄ¬≤Dt/L¬≤)\n",
    "    u_exact = jnp.sin(jnp.pi * X / L) * jnp.exp(-D * (jnp.pi / L)**2 * T)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 8.1 Quantitative Metrics\n",
    "    # ============================================================================\n",
    "\n",
    "    mae = jnp.mean(jnp.abs(u_pred - u_exact))\n",
    "    max_error = jnp.max(jnp.abs(u_pred - u_exact))\n",
    "    rel_error = jnp.mean(jnp.abs(u_pred - u_exact) / (jnp.abs(u_exact) + 1e-10))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUANTITATIVE PERFORMANCE METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Mean Absolute Error (MAE):     {mae:.6e}\")\n",
    "    print(f\"Maximum Absolute Error:        {max_error:.6e}\")\n",
    "    print(f\"Mean Relative Error:           {rel_error:.6e}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 8.2 Qualitative Assessment: Visualizations\n",
    "    # ============================================================================\n",
    "\n",
    "    # 1. Solution comparison (heatmaps)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    # PINN prediction\n",
    "    im1 = axes[0].pcolormesh(X, T, u_pred, shading='auto', cmap='viridis')\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('t')\n",
    "    axes[0].set_title('PINN Prediction: u(x,t)')\n",
    "    plt.colorbar(im1, ax=axes[0], label='u')\n",
    "\n",
    "    # Analytical solution\n",
    "    im2 = axes[1].pcolormesh(X, T, u_exact, shading='auto', cmap='viridis')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylabel('t')\n",
    "    axes[1].set_title('Analytical Solution: u(x,t)')\n",
    "    plt.colorbar(im2, ax=axes[1], label='u')\n",
    "\n",
    "    # Absolute error\n",
    "    residual = jnp.abs(u_pred - u_exact)\n",
    "    im3 = axes[2].pcolormesh(X, T, residual, shading='auto', cmap='inferno')\n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].set_ylabel('t')\n",
    "    axes[2].set_title('Absolute Error: |u_pred - u_exact|')\n",
    "    plt.colorbar(im3, ax=axes[2], label='Error')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================================\n",
    "    # 2. Time slices: u(x, t) at fixed times\n",
    "    # ============================================================================\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    times = [0.0, 0.25, 0.5, 1.0]\n",
    "\n",
    "    for idx, t_slice in enumerate(times):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Find closest time index\n",
    "        t_idx = jnp.argmin(jnp.abs(t - t_slice))\n",
    "        \n",
    "        # Extract predictions and exact solution at this time\n",
    "        u_pred_slice = u_pred[t_idx, :]\n",
    "        u_exact_slice = u_exact[t_idx, :]\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(x, u_pred_slice, label='PINN', color='blue', linewidth=2)\n",
    "        ax.plot(x, u_exact_slice, label='Analytical', color='red', \n",
    "                linestyle='--', linewidth=2)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('u')\n",
    "        # set y-axis limits from -0.1 to 1.1\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.set_title(f'Solution at t = {t[t_idx]:.3f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================================\n",
    "    # 3. Spatial profiles: u(x, t) at fixed locations\n",
    "    # ============================================================================\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    x_locations = [L/4, L/2, 3*L/4]\n",
    "\n",
    "    for idx, x_loc in enumerate(x_locations):\n",
    "        # Find closest x index\n",
    "        x_idx = jnp.argmin(jnp.abs(x - x_loc))\n",
    "        \n",
    "        # Extract predictions and exact solution at this location\n",
    "        u_pred_profile = u_pred[:, x_idx]\n",
    "        u_exact_profile = u_exact[:, x_idx]\n",
    "        \n",
    "        # Plot temporal evolution at this location\n",
    "        axes[idx].plot(t, u_pred_profile, label='PINN', color='blue', linewidth=2)\n",
    "        axes[idx].plot(t, u_exact_profile, label='Analytical', color='red', \n",
    "                       linestyle='--', linewidth=2)\n",
    "        axes[idx].set_xlabel('t')\n",
    "        axes[idx].set_ylabel('u')\n",
    "        axes[idx].set_title(f'Temporal Evolution at x = {x[x_idx]:.2f}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except (NotImplementedError, NameError) as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cannot evaluate: {e}\")\n",
    "    print(\"Please complete the implementation and training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa3a4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ ADVANCED / OPTIONAL EXERCISES\n",
    "\n",
    "The following sections are **optional** and explore more advanced topics:\n",
    "- **Section 8.3**: Comparison with classical numerical methods (FTCS)\n",
    "- **Section 9**: Inverse problem - reconstructing initial conditions from measurements\n",
    "\n",
    "These are excellent extensions if you want to deepen your understanding, but are **not required** for completing the core exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25df8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8.3 OPTIONAL: Comparison with Classical Method: FTCS (Forward-Time Central-Space)\n",
    "# ============================================================================\n",
    "\n",
    "def solve_heat_ftcs(L, T, D, nx, nt, ic_func):\n",
    "    \"\"\"\n",
    "    Solve the 1D heat equation using the FTCS explicit finite difference method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    L : float\n",
    "        Domain length\n",
    "    T : float\n",
    "        Final time\n",
    "    D : float\n",
    "        Diffusion coefficient\n",
    "    nx : int\n",
    "        Number of spatial grid points\n",
    "    nt : int\n",
    "        Number of time steps\n",
    "    ic_func : function\n",
    "        Initial condition function u(x, 0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    x : array\n",
    "        Spatial grid\n",
    "    t : array\n",
    "        Time grid\n",
    "    u : array\n",
    "        Solution u(x, t)\n",
    "    \"\"\"\n",
    "    # Grid setup\n",
    "    dx = L / (nx - 1)\n",
    "    dt = T / (nt - 1)\n",
    "    x = np.linspace(0, L, nx)\n",
    "    t = np.linspace(0, T, nt)\n",
    "    \n",
    "    # Check stability condition: D*dt/dx¬≤ ‚â§ 0.5\n",
    "    stability_param = D * dt / dx**2\n",
    "    print(f\"FTCS stability parameter (should be ‚â§ 0.5): {stability_param:.4f}\")\n",
    "    \n",
    "    if stability_param > 0.5:\n",
    "        print(\"WARNING: FTCS scheme may be unstable!\")\n",
    "    \n",
    "    # Initialize solution\n",
    "    u = np.zeros((nt, nx))\n",
    "    u[0, :] = ic_func(x)\n",
    "    \n",
    "    # Time integration\n",
    "    for n in range(0, nt-1):\n",
    "        for i in range(1, nx-1):\n",
    "            # FTCS formula: u[n+1,i] = u[n,i] + D*dt/dx¬≤ * (u[n,i+1] - 2*u[n,i] + u[n,i-1])\n",
    "            u[n+1, i] = u[n, i] + stability_param * (u[n, i+1] - 2*u[n, i] + u[n, i-1])\n",
    "        \n",
    "        # Boundary conditions: u(0,t) = u(L,t) = 0\n",
    "        u[n+1, 0] = 0\n",
    "        u[n+1, -1] = 0\n",
    "    \n",
    "    return x, t, u\n",
    "\n",
    "\n",
    "# Initial condition function\n",
    "def ic_sin(x):\n",
    "    return np.sin(np.pi * x / L)\n",
    "\n",
    "# Solve using FTCS\n",
    "print(\"\\nSolving with FTCS method...\")\n",
    "x_ftcs, t_ftcs, u_ftcs = solve_heat_ftcs(L, t_end, D, nx=100, nt=200, ic_func=ic_sin)\n",
    "\n",
    "# Convert to JAX arrays for comparison\n",
    "u_ftcs_jax = jnp.array(u_ftcs)\n",
    "X_ftcs, T_ftcs = jnp.meshgrid(x_ftcs, t_ftcs)\n",
    "\n",
    "# Compute analytical solution on FTCS grid\n",
    "u_exact_ftcs = jnp.sin(jnp.pi * X_ftcs / L) * jnp.exp(-D * (jnp.pi / L)**2 * T_ftcs)\n",
    "\n",
    "# Compute errors\n",
    "ftcs_mae = jnp.mean(jnp.abs(u_ftcs_jax - u_exact_ftcs))\n",
    "ftcs_max_error = jnp.max(jnp.abs(u_ftcs_jax - u_exact_ftcs))\n",
    "\n",
    "print(f\"\\nFTCS Method Errors:\")\n",
    "print(f\"  Mean Absolute Error: {ftcs_mae:.6e}\")\n",
    "print(f\"  Maximum Error:       {ftcs_max_error:.6e}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  PINN MAE:  {mae:.6e}\")\n",
    "print(f\"  FTCS MAE:  {ftcs_mae:.6e}\")\n",
    "print(f\"  Winner:    {'PINN' if mae < ftcs_mae else 'FTCS'} (lower MAE)\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# PINN error\n",
    "im1 = axes[0].pcolormesh(X, T, jnp.abs(u_pred - u_exact), shading='auto', cmap='inferno')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('t')\n",
    "axes[0].set_title(f'PINN Error (MAE={mae:.2e})')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# FTCS error\n",
    "im2 = axes[1].pcolormesh(X_ftcs, T_ftcs, jnp.abs(u_ftcs_jax - u_exact_ftcs), \n",
    "                         shading='auto', cmap='inferno')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('t')\n",
    "axes[1].set_title(f'FTCS Error (MAE={ftcs_mae:.2e})')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Error comparison at t=0.5\n",
    "t_compare = 0.5\n",
    "t_idx_pinn = jnp.argmin(jnp.abs(t - t_compare))\n",
    "t_idx_ftcs = np.argmin(np.abs(t_ftcs - t_compare))\n",
    "\n",
    "axes[2].plot(x, jnp.abs(u_pred[t_idx_pinn, :] - u_exact[t_idx_pinn, :]), \n",
    "             label='PINN', linewidth=2)\n",
    "axes[2].plot(x_ftcs, np.abs(u_ftcs[t_idx_ftcs, :] - u_exact_ftcs[t_idx_ftcs, :]), \n",
    "             label='FTCS', linewidth=2, linestyle='--')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('Absolute Error')\n",
    "axes[2].set_title(f'Error Profile at t = {t_compare}')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e1d56",
   "metadata": {},
   "source": [
    "## 9. üî¨ ADVANCED: Inverse Problem Challenge\n",
    "\n",
    "\n",
    "### 9.1 Diffusion in Fricke Gel ‚Äì 1D Inverse Problem\n",
    "\n",
    "The simplest diffusion experiment involving Fricke hydrogel consists of a small rod that is half-irradiated, as shown in the figure below. In this case, a chelating agent‚Äîwhich induces color changes in accordance with the amount of ferric ions‚Äîwas added to the gel. It follows that dose estimation can be performed through optical measurements of absorbance.  \n",
    "In other words, absorbance (what you measure) is proportional to the amount of ferric ions, which in turn is proportional to the absorbed dose of ionizing radiation.\n",
    "\n",
    "![1d_fricke_gel](./images/fricke_gel_irr.png)\n",
    "\n",
    "After irradiation, ferric ions diffuse within the gel matrix, causing the loss of the initial shape, as shown in the animation below.\n",
    "\n",
    "![1d_evolution](./images/1D_evolution.gif)\n",
    "\n",
    "Since ions cannot move outside the gel, the concentration flux at the boundaries is zero. This means that:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}A(x,t) = 0 \\quad \\text{with} \\quad x = \\{0, L\\}\n",
    "$$  \n",
    "(*Von Neumann boundary condition*)\n",
    "\n",
    "where $A(x,t)$ is the measured absorbance at position $x$ and time $t$.  \n",
    "\n",
    "**The main physical consequence is that the total amount of ferric ions does not change, and consequently, the total absorbed dose remains constant as well.**\n",
    "\n",
    "\n",
    "**Task 9.1:** Recover the initial distribution from a later measurement (inverse problem)\n",
    "\n",
    "This advanced exercise explores one of PINNs' most powerful capabilities: solving inverse problems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 Problem Setup\n",
    "\n",
    "In [experimental_data folder](./experimental_data/) 4 sets at $t = \\{1,4,7,10\\}\\ h$ of Absorbance as a function of position are given.\n",
    "\n",
    "- **First experimental distribution:** `A(x, t)` at time `t = 1.0 h` (Initial distribution cannot be measured)\n",
    "- **Diffused distribution:** `A(x, t)` at later times `t = t_measurement = [4.0, 7.0, 10.0] h`\n",
    "- **Diffusion coefficient:** it was experimentally determined as `D = 0.249 mm¬≤/h`\n",
    "- **Boundary conditions:** `‚àÇA/‚àÇx(0, t) = ‚àÇA/‚àÇx(L, t) = 0` for all `t > 0`\n",
    "\n",
    "**Your goal:** Starting from the measurement at `t = t_measurement`, reconstruct the initial distribution at `t = 1.0 h` (predicting `t = 0.0 h` is possible but ground truth is not provided).\n",
    "\n",
    "This is an **inverse problem** (going backward in time) and is generally ill-posed for the heat equation due to its irreversible nature.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 Implementation Strategy\n",
    "\n",
    "**Modifications to your PINN:**\n",
    "\n",
    "1. **Change the supervised data:**\n",
    "   - Instead of using the initial condition `A(x, 0) = sin(œÄx/L)` as training data...\n",
    "   - Use the measurement at `t = t_measurement` as your supervised constraint\n",
    "   - New loss: `L_data = mean((NN(x, t_measurement) - u_measured(x, t_measurement))¬≤)`\n",
    "\n",
    "2. **Predict backward:**\n",
    "   - Train the network to satisfy the PDE throughout `t ‚àà [0, t_measurement]`\n",
    "   - The network should learn to \"reverse\" the diffusion process\n",
    "   - After training, evaluate `NN(x, 1.0)` to get the reconstructed initial condition\n",
    "\n",
    "3. **Handle discrete measurements:**\n",
    "   - **Challenge:** You only have discrete measurement points, not a continuous function\n",
    "   - **Solution:** Use interpolation to evaluate `u_measured` at arbitrary points\n",
    "   - Suggested: [Scipy interpolation](https://docs.scipy.org/doc/scipy/reference/interpolate.html) (e.g., `scipy.interpolate.interp1d` or `scipy.interpolate.CubicSpline`)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.4 Modified Loss Function\n",
    "\n",
    "Your new total loss should be:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{PDE} + \\mathcal{L}_{BC} + \\mathcal{L}_{data}$$\n",
    "\n",
    "Where:\n",
    "- `L_PDE`: Same as before (enforces heat equation throughout the domain)\n",
    "- `L_BC`: Must be modified for Neumann BCs: `‚àÇu/‚àÇx(0, t) = ‚àÇu/‚àÇx(L, t) = 0`\n",
    "- `L_data`: New supervised loss using measurements at `t = t_measurement`\n",
    "\n",
    "Note: You're replacing `L_IC` (initial condition supervision) with `L_data` (measurement supervision).\n",
    "\n",
    "---\n",
    "\n",
    "### 9.5 Key Questions to Explore\n",
    "\n",
    "**Question 9.1:** *How far back in time can you reliably reconstruct?*\n",
    "\n",
    "Experiment with different values of `t_measurement`:\n",
    "- Start with small values (e.g., `t_measurement = 4.0 h`)\n",
    "- Gradually increase to larger values (e.g., `7.0 h`, `10.0 h`)\n",
    "- Additional measurements at longer times can be generated using the FTCS scheme\n",
    "- At what point does reconstruction fail or become very inaccurate?\n",
    "\n",
    "**Factors to investigate:**\n",
    "1. **Reconstruction error vs. time:**\n",
    "   - Plot reconstruction error as a function of `t_measurement`\n",
    "   - Observe when error grows unacceptably large\n",
    "\n",
    "2. **Role of noise:** (can be skipped with real experimental data)\n",
    "   - Add Gaussian noise to the measurement data: `A_noisy = A_measured + Œµ¬∑N(0,1)`\n",
    "   - How does noise affect reconstruction quality?\n",
    "   - Try different noise levels\n",
    "\n",
    "3. **Regularization:**\n",
    "   - Can you improve reconstruction by adding regularization terms to the loss?\n",
    "   - Example: Penalize roughness of the initial condition\n",
    "\n",
    "4. **Physical intuition:**\n",
    "   - Why does the heat equation make backward reconstruction difficult?\n",
    "   - Think about information loss during diffusion\n",
    "   - How does this relate to the second law of thermodynamics?\n",
    "\n",
    "---\n",
    "\n",
    "### 9.6 Expected Challenges\n",
    "\n",
    "1. **Training instability:**\n",
    "   - Inverse problems are typically more difficult to train\n",
    "   - You may need: smaller learning rates, more iterations, better initialization\n",
    "\n",
    "2. **Non-uniqueness:**\n",
    "   - Multiple initial conditions might produce similar measurements\n",
    "   - The PINN might converge to different solutions with different random seeds\n",
    "\n",
    "3. **Sensitivity to hyperparameters:**\n",
    "   - Loss weighting becomes more critical\n",
    "   - Network architecture may need adjustment\n",
    "\n",
    "**Tips:**\n",
    "- Start with small `t_measurement` where the problem is easier\n",
    "- Use multiple random seeds and compare results\n",
    "- Visualize intermediate solutions during training to debug issues\n",
    "\n",
    "---\n",
    "\n",
    "### 9.7 Extension Ideas\n",
    "\n",
    "If you complete the basic inverse problem, consider:\n",
    "1. **Unknown diffusion coefficient:** Simultaneously learn `D` and initial condition\n",
    "2. **Sparse measurements:** Use only a few spatial points at `t_measurement`\n",
    "3. **Multiple time measurements:** Use data at several different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. ADVANCED/OPTIONAL: INVERSE PROBLEM\n",
    "# ============================================================================\n",
    "# Goal: Recover initial condition from a measurement at later time\n",
    "\n",
    "#loading data\n",
    "data_0=np.load('experimental_data/data_0.npz') #experimental data at t=1.0h\n",
    "data_1=np.load('experimental_data/data_1.npz') #experimental data at t=4.0h\n",
    "data_2=np.load('experimental_data/data_2.npz') #experimental data at t=7.0h\n",
    "data_3=np.load('experimental_data/data_3.npz') #experimental data at t=10.0h\n",
    "\n",
    "#plotting data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(data_0['x'], data_0['y'], label='t=1.0h')\n",
    "plt.plot(data_1['x'], data_1['y'], label='t=4.0h')\n",
    "plt.plot(data_2['x'], data_2['y'], label='t=7.0h')\n",
    "plt.plot(data_3['x'], data_3['y'], label='t=10.0h')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absorbance')\n",
    "plt.title('Experimental Data at Different Times')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. ADVANCED/OPTIONAL: INVERSE PROBLEM\n",
    "# ============================================================================\n",
    "# Goal: Recover initial condition from a measurement at later time\n",
    "\n",
    "#loading data\n",
    "data_0=np.load('experimental_data/data_0.npz') #experimental data at t=1.0h\n",
    "data_1=np.load('experimental_data/data_1.npz') #experimental data at t=4.0h\n",
    "data_2=np.load('experimental_data/data_2.npz') #experimental data at t=7.0h\n",
    "data_3=np.load('experimental_data/data_3.npz') #experimental data at t=10.0h\n",
    "\n",
    "#plotting data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(data_0['x'], data_0['y'], label='t=1.0h')\n",
    "plt.plot(data_1['x'], data_1['y'], label='t=4.0h')\n",
    "plt.plot(data_2['x'], data_2['y'], label='t=7.0h')\n",
    "plt.plot(data_3['x'], data_3['y'], label='t=10.0h')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absorbance')\n",
    "plt.title('Experimental Data at Different Times')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67740f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of measurement in hours choose one of the available times [4.0,7.0,10.0]\n",
    "t_measurement = 7.0\n",
    "\n",
    "D = 0.249 # diffusion coefficient from experimental analysis\n",
    "\n",
    "x_ini , A_ini = data_0['x'] , data_0['y']  # initial condition at t=1.0h \n",
    "\n",
    "if t_measurement == 4.0:\n",
    "    x_measure, A_measure = data_1['x'] , data_1['y']\n",
    "elif t_measurement == 7.0:\n",
    "    x_measure, A_measure = data_2['x'] , data_2['y']\n",
    "elif t_measurement == 10.0:\n",
    "    x_measure, A_measure = data_3['x'] , data_3['y']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INVERSE PROBLEM: RECONSTRUCTING INITIAL CONDITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(f\"  Measurement points: {len(x)} at t={t_measurement}h,  Diffusion Coeff.: D={D}mm¬≤/h\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.plot(x_ini, A_ini, 'o-', label='Initial condition t=1.0h', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absorbance')\n",
    "plt.plot(x_measure, A_measure, 'o-', label=f'Distribution at {t_measurement}h', linewidth=2)\n",
    "plt.xlabel('x (mm)')\n",
    "plt.ylabel('Absorbance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Experimental Data')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab987ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Create interpolation function for measurements\n",
    "# ============================================================================\n",
    "\n",
    "# Create interpolation function (cubic interpolation)\n",
    "# Convert JAX arrays to NumPy for scipy compatibility\n",
    "u_measured_interp = interp1d(...)\n",
    "\n",
    "\n",
    "# TODO: implement a cubic-spline object to get values out of the defined grid\n",
    "# Hint: Use the scipy documentation for more details \n",
    "# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create loss data function, modified BC_loss and PDE loss\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def loss_data(params, pinn, points_data, u_measured_values):\n",
    "    \"\"\"\n",
    "    Loss for matching measurement data at t = t_measurement.\n",
    "    Replaces L_IC in the inverse problem.\n",
    "    \"\"\"\n",
    "    # TODO:Implement your loss data function\n",
    "    # Hint: Modify your working IC_loss\n",
    "    raise NotImplementedError(\"Implement loss_data\")\n",
    "\n",
    "\n",
    "def loss_bc_inverse(params, pinn, points):\n",
    "    \"\"\"\n",
    "    Compute the boundary condition loss: L_BC.\n",
    "\n",
    "    Enforces Dirichlet BC: u_x(0, t) = u_x(L, t) = 0\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : list\n",
    "        Network parameters\n",
    "    pinn : Pinn\n",
    "        PINN object\n",
    "    points : array of shape (N2, 2)\n",
    "        Boundary points where x ‚àà {0, L}\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Mean squared boundary predictions (should be zero)\n",
    "    \"\"\"\n",
    "\n",
    "    def border_flux(point):\n",
    "        x, t = point[0], point[1]\n",
    "\n",
    "        # TODO: Define function for forward propagation\n",
    "        def u_fn(x_val, t_val):\n",
    "            #Hint: cast input in jnp.array\n",
    "            raise NotImplementedError(\"Implement u_fn for boundary flux computation\")\n",
    "\n",
    "        # TODO:Compute derivatives using JAX grad function\n",
    "        # First derivative: ‚àÇu/‚àÇx\n",
    "        \n",
    "\n",
    "        # Since BC is u_0=0, we penalize any non-zero predictions\n",
    "        return u_x**2\n",
    "\n",
    "    loss = vmap(border_flux)(points)\n",
    "\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "\n",
    "def total_loss_inverse(\n",
    "    params, pinn, points_interior, points_boundary, points_data, u_measured_values, D\n",
    "):\n",
    "    \"\"\"\n",
    "    Total loss for inverse problem: L_total = L_PDE + L_BC + L_data\n",
    "    \"\"\"\n",
    "    l_pde = loss_pde(params, pinn, points_interior, D)\n",
    "    l_bc = loss_bc_inverse(params, pinn, points_boundary)\n",
    "    l_data = loss_data(params, pinn, points_data, u_measured_values)\n",
    "\n",
    "    total = l_pde + l_bc + l_data\n",
    "\n",
    "    return total, (l_pde, l_bc, l_data)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Train PINN for inverse problem with JIT-compiled training step\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def train_pinn_inverse(\n",
    "    pinn,\n",
    "    geo,\n",
    "    D,\n",
    "    t_measurement,\n",
    "    u_interp,\n",
    "    n_epochs=1000,\n",
    "    N1=1000,\n",
    "    N2=200,\n",
    "    N_data=200,\n",
    "    learning_rate=5e-4,\n",
    "):\n",
    "    \"\"\"Train PINN for the inverse problem with JIT-compiled training step.\"\"\"\n",
    "\n",
    "    # TODO: Modify your training loop to solve the inverse problem\n",
    "    # Hint: Remember, you defined a new total_loss function\n",
    "    # Hint: The procedure is the same as before, except for the point you sample (Now you sample at t=t_measure)\n",
    "    raise NotImplementedError(\"Implement your inverse training loop\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new PINN for inverse problem\n",
    "print(\"\\nInitializing new PINN for inverse problem...\")\n",
    "layers_inverse = []  # Slightly larger network\n",
    "key_inverse = random.PRNGKey(999)\n",
    "pinn_inverse = Pinn(key_inverse, layers_inverse)\n",
    "\n",
    "L = x_measure[-1] - x_measure[0]\n",
    "print(L)\n",
    "\n",
    "# Update geometry to only go up to t_measurement\n",
    "geo_inverse = Geometry(L=L, N=100, t_start=0.0, t_end=t_measurement, Nt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PINN\n",
    "loss_history_inv, loss_components_inv = train_pinn_inverse(\n",
    "    pinn_inverse,\n",
    "    geo_inverse,\n",
    "    D,\n",
    "    t_measurement,\n",
    "    u_measured_interp,\n",
    "    n_epochs=5000,\n",
    "    N1=20000,\n",
    "    N2=1000,\n",
    "    N_data=10000,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(loss_history_inv, linewidth=1.5)\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Total Loss\")\n",
    "axes[0].set_title(\"Inverse Problem: Training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(loss_components_inv[\"pde\"], label=\"L_PDE\", linewidth=1.5)\n",
    "axes[1].plot(loss_components_inv[\"bc\"], label=\"L_BC\", linewidth=1.5)\n",
    "axes[1].plot(loss_components_inv[\"data\"], label=\"L_data\", linewidth=1.5)\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss Components\")\n",
    "axes[1].set_title(\"Individual Loss Components\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20829971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Evaluate reconstruction\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECONSTRUCTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reconstruct initial condition\n",
    "x_recon = x_ini  # use initial x points for reconstruction\n",
    "t_ini = 1.0  # initial time corresponding to A_ini\n",
    "t_zero = np.ones_like(x_recon)* t_ini\n",
    "points_initial_recon = np.stack((x_recon, t_zero),-1)\n",
    "\n",
    "u_initial_reconstructed = pinn_inverse(points_initial_recon)\n",
    "u_initial_reconstructed = u_initial_reconstructed.reshape(x_recon.shape[-1])\n",
    "\n",
    "\n",
    "# Compute reconstruction error\n",
    "recon_mae = np.mean(np.abs(u_initial_reconstructed - A_ini))\n",
    "recon_max_error = np.max(np.abs(u_initial_reconstructed - A_ini))\n",
    "\n",
    "print(f\"\\nReconstruction at t = 0 (going back {t_measurement} in time):\")\n",
    "print(f\"  Mean Absolute Error: {recon_mae:.6e}\")\n",
    "print(f\"  Maximum Error:       {recon_max_error:.6e}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize reconstruction\n",
    "fig, axes = plt.subplots(3, 1, figsize=(7, 15))\n",
    "\n",
    "# 1. Initial condition reconstruction\n",
    "axes[0].plot(x_recon, A_ini, 'k-', \n",
    "                label='True initial', linewidth=3, alpha=0.7)\n",
    "axes[0].plot(x_recon, u_initial_reconstructed, 'b--', \n",
    "                label='PINN reconstruction', linewidth=2)\n",
    "axes[0].plot(x_measure, A_measure, 'ro', \n",
    "                label=f'Measured at t={t_measurement}', markersize=4)\n",
    "axes[0].set_xlabel('x(mm)')\n",
    "axes[0].set_ylabel('A(x, 0)')\n",
    "axes[0].set_title('Initial Condition Reconstruction')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# 3. Full solution in (x,t) space\n",
    "x_full = geo_inverse.x\n",
    "t_full = geo_inverse.t\n",
    "\n",
    "X_full, T_full = np.meshgrid(x_full, t_full[25:])\n",
    "XT_full = np.hstack((X_full.flatten()[:, None], T_full.flatten()[:, None]))\n",
    "u_inverse_full = np.array(pinn_inverse(XT_full)).reshape(X_full.shape)\n",
    "\n",
    "im = axes[1].pcolormesh(X_full, T_full, u_inverse_full, shading='auto', cmap='viridis')\n",
    "axes[1].axhline(t_measurement, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f't = {t_measurement} (measurement)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('t')\n",
    "axes[1].set_title('Reconstructed Solution A(x,t)')\n",
    "axes[1].legend()\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "# 4. Reconstruction error\n",
    "error_recon = np.abs(u_initial_reconstructed - A_ini)\n",
    "axes[2].plot(x_recon, error_recon, 'r-', linewidth=2)\n",
    "axes[2].fill_between(x_recon, 0, error_recon, alpha=0.3)\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('|A_recon - A_true|')\n",
    "axes[2].set_title(f'Reconstruction Error at t = {t_ini}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Try changing t_measurement to see how reconstruction quality degrades!\")\n",
    "print(f\"   Current value: {t_measurement}\")\n",
    "print(f\"   Suggested experiments: 7, 10, ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86014e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. üî¨ ADVANCED: 2D Fricke Gel Inverse Problem\n",
    "\n",
    "In this final advanced exercise, we extend our PINN capabilities to solve a realistic **2D inverse problem** inspired by medical physics: reconstructing the initial dose distribution in a Fricke gel dosimeter after diffusion has occurred.\n",
    "\n",
    "## 10.1 Physical Background\n",
    "\n",
    "**Fricke gel dosimeters** are used in radiation therapy to measure 3D dose distributions:\n",
    "- Iron ions ($Fe^{2+}$) in gel are oxidized to $Fe^{3+}$ by ionizing radiation\n",
    "- $Fe^{3+}$ concentration is proportional to absorbed dose\n",
    "- MRI or optical scanning can map the $Fe^{3+}$ distribution\n",
    "\n",
    "**The Challenge:** \n",
    "- Diffusion blurs the dose distribution between irradiation and measurement\n",
    "- We measure a *diffused* distribution, but need the *original* dose for treatment verification\n",
    "- This is an **inverse problem**: going backward in time to recover initial conditions\n",
    "\n",
    "**Governing Equation (2D Heat/Diffusion):**\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = D \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)$$\n",
    "\n",
    "Where:\n",
    "- $u(x,y,t)$ is the $Fe^{3+}$ concentration (proportional to dose)\n",
    "- $D$ is the diffusion coefficient\n",
    "- Domain: $(x,y) \\in [0, L_x] \\times [0, L_y]$, $t \\in [0, T_{max}]$\n",
    "- Boundary conditions: Neumann (no-flux) at all boundaries\n",
    "\n",
    "**Our Goal:** Given $u(x,y,T_{measure})$ (measurement after diffusion), reconstruct $u(x,y,0)$ (original dose distribution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10.2 Generate Synthetic 2D Data using FTCS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"2D FRICKE GEL INVERSE PROBLEM - DATA GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from FTCS.FTCS_scheme import diffusion_2D\n",
    "from FTCS.Initial_condition import rectangle\n",
    "\n",
    "# Problem parameters\n",
    "Lx = 1.0      # domain size in x\n",
    "Ly = 1.0      # domain size in y\n",
    "Tmax = 0.5    # maximum time\n",
    "D_2d = 0.005  # diffusion coefficient\n",
    "dx = 0.01     # spatial resolution x\n",
    "dy = 0.01     # spatial resolution y\n",
    "\n",
    "# Rectangle initial condition parameters (dose distribution)\n",
    "xc = 0.5      # center x\n",
    "yc = 0.5      # center y\n",
    "W = 0.3       # width\n",
    "H = 0.2       # height\n",
    "\n",
    "print(f\"\\nDomain: {Lx} x {Ly}\")\n",
    "print(f\"Diffusion coefficient: D = {D_2d}\")\n",
    "print(f\"Spatial resolution: dx = {dx}, dy = {dy}\")\n",
    "\n",
    "# Initialize FTCS solver\n",
    "geo_ftcs = diffusion_2D(Lx, Ly, Tmax, dx, dy, D_2d)\n",
    "\n",
    "# Create smooth rectangular initial condition\n",
    "ini_2d = rectangle(geo_ftcs, xc, yc, H, W, T_evo=0.3)\n",
    "geo_ftcs.set_initial_condition(ini_2d)\n",
    "\n",
    "print(f\"Grid size: {ini_2d.shape}\")\n",
    "\n",
    "# Visualize initial condition\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ini_2d, cmap='hot', origin='lower', extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "plt.colorbar(label='Dose (normalized)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('True Initial Distribution (t=0)\\n[To be reconstructed]')\n",
    "\n",
    "# Evolve to measurement time\n",
    "t_measure_2d = 0.2  # measurement time\n",
    "geo_ftcs_measure = diffusion_2D(Lx, Ly, t_measure_2d, dx, dy, D_2d)\n",
    "geo_ftcs_measure.set_initial_condition(ini_2d)\n",
    "final_2d = geo_ftcs_measure.Neumann_evo()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(final_2d, cmap='hot', origin='lower', extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "plt.colorbar(label='Dose (normalized)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Diffused Distribution (t={t_measure_2d})\\n[Measured data]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData generated successfully!\")\n",
    "print(f\"Initial distribution range: [{ini_2d.min():.4f}, {ini_2d.max():.4f}]\")\n",
    "print(f\"Diffused distribution range: [{final_2d.min():.4f}, {final_2d.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bfc9b1",
   "metadata": {},
   "source": [
    "## 10.3 2D Geometry Class for PINN\n",
    "\n",
    "We need to extend our geometry class to handle 2D spatial domains. The main differences from 1D:\n",
    "- Input coordinates: $(x, y, t)$ instead of $(x, t)$\n",
    "- Domain boundaries: 4 edges (left, right, bottom, top) instead of 2 points\n",
    "- Neumann boundary conditions: $\\frac{\\partial u}{\\partial n} = 0$ at all boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2D Geometry class for spatial domain [0, Lx] x [0, Ly] x [0, T] TODO\n",
    "# ============================================================================\n",
    "\n",
    "#That you developed for 1D case can be easely adapted for a 2D geometry\n",
    "#Keep in mind that for Von Neumann condition you have:\n",
    "# n ¬∑ ‚àáu=0, with n normal versor along the border.\n",
    "# n is different at each border!\n",
    "# in particular on the left you will have:\n",
    "# ‚àÇu/‚àÇx = 0       but ‚àÇu/‚àÇy is not necessary 0!\n",
    "\n",
    "\n",
    "class Geometry2D:\n",
    "    \"\"\"\n",
    "    Geometry class for 2D spatial + 1D temporal domain.\n",
    "    Domain: (x,y) ‚àà [0, Lx] x [0, Ly], t ‚àà [t_start, t_end]\n",
    "    \"\"\"\n",
    "    def __init__(self, Lx=1.0, Ly=1.0, Nx=50, Ny=50, t_start=0.0, t_end=1.0, Nt=50):\n",
    "        self.Lx = Lx\n",
    "        self.Ly = Ly\n",
    "        self.Nx = Nx\n",
    "        self.Ny = Ny\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end\n",
    "        self.Nt = Nt\n",
    "        \n",
    "        # Create grids for visualization\n",
    "        self.x = jnp.linspace(0, Lx, Nx)\n",
    "        self.y = jnp.linspace(0, Ly, Ny)\n",
    "        self.t = jnp.linspace(t_start, t_end, Nt)\n",
    "    \n",
    "    def sample_interior(self, key, N):\n",
    "        \"\"\"Sample N random points in the interior domain.\"\"\"\n",
    "        key_x, key_y, key_t = random.split(key, 3)\n",
    "        \n",
    "        # TODO: Use your geometry implementation and adapt it to 2D geometry\n",
    "        # Hint: you need just to add y-coordinates sampling\n",
    "        # Hint: return a np.array, shape=(n_point, 3), of (x,y,t) coordinates\n",
    "\n",
    "    \n",
    "    def sample_boundary(self, key, N):\n",
    "        \"\"\"\n",
    "        Return a dict, size=4, of borer points.\n",
    "        keys:\n",
    "        - left\n",
    "        - right\n",
    "        - down\n",
    "        - up\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        #left border jnp.array, shape (n_point//4,3) (x,y,t) x=0, y ‚àä (0,Ly), t‚àä(0,t_measure)\n",
    "\n",
    "        left=...\n",
    "\n",
    "        #right border jnp.array, shape (n_point//4,3) (x,y,t) x=Lx, y ‚àä (0,Ly), t‚àä(0,t_measure)\n",
    "\n",
    "        right=...\n",
    "        \n",
    "        #bottom border jnp.array, shape (n_point//4,3) (x,y,t) x‚àä (0,Lx) , y=0 , t‚àä(0,t_measure)\n",
    "\n",
    "        down=...\n",
    "        \n",
    "        #top border jnp.array, shape (n_point//4,3) (x,y,t) x‚àä (0,Lx) , y=Ly , t‚àä(0,t_measure)\n",
    "\n",
    "        up=...\n",
    "\n",
    "        border={'left': left, 'right':right, 'down':down, 'up':up}\n",
    "\n",
    "        # TODO: Use the suggestions to build a sample_boundary function\n",
    "        # Hint: Take inspiration from the 1D geometry you have already implemented\n",
    "    \n",
    "    def sample_initial(self, key, N):\n",
    "        \"\"\"Sample N random points at initial time t=0.\"\"\"\n",
    "        key_x, key_y = random.split(key)\n",
    "\n",
    "        # TODO: Use your geometry implementation and adapt it to 2D geometry\n",
    "        # Hint: you need just to add y-coordinates sampling\n",
    "        # Hint: return a jnp.array, shape=(n_point, 3), of (x,y,t) coordinates, t=0\n",
    "    \n",
    "    def sample_at_time(self, key, N, t_val):\n",
    "        \"\"\"Sample N random points at specific time t_val.\"\"\"\n",
    "        key_x, key_y = random.split(key)\n",
    "\n",
    "        # TODO: Use your geometry implementation and adapt it to 2D geometry\n",
    "        # Hint: you need just to add y-coordinates sampling\n",
    "        # Hint: return a jnp.array, shape=(n_point, 3), of (x,y,t) coordinates, t=T\n",
    "\n",
    "print(\"‚úì Geometry2D class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c93a7",
   "metadata": {},
   "source": [
    "## 10.4 2D Neural Network Architecture\n",
    "\n",
    "The PINN for 2D diffusion has:\n",
    "- **Input:** 3 coordinates $(x, y, t)$\n",
    "- **Output:** 1 value $u(x, y, t)$\n",
    "- **Architecture:** Similar to 1D but may need more capacity for 2D spatial patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6988e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2D PINN Architecture\n",
    "# ============================================================================\n",
    "\n",
    "#If your pinn_model function is well defined \n",
    "#you should be able to create a pinn2D just changing the input shape\n",
    "inp_shape2D=(3,)\n",
    "out_shape=1\n",
    "hlayers=[20,50,500,50,20]\n",
    "pinn2d=...\n",
    "\n",
    "    \n",
    "\n",
    "print(\"‚úì Pinn2D class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874dbf7",
   "metadata": {},
   "source": [
    "## 10.5 Loss Functions for 2D Inverse Problem\n",
    "\n",
    "For the 2D Fricke gel inverse problem, we need:\n",
    "\n",
    "1. **PDE Loss** - Enforce 2D diffusion equation:\n",
    "   $$\\mathcal{L}_{PDE} = \\left| \\frac{\\partial u}{\\partial t} - D \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right) \\right|^2$$\n",
    "\n",
    "2. **Boundary Loss** - Enforce Neumann (no-flux) boundary conditions:\n",
    "   $$\\mathcal{L}_{BC} = \\left| \\frac{\\partial u}{\\partial n} \\right|^2 \\text{ at boundaries}$$\n",
    "\n",
    "3. **Data Loss** - Match measurement at $t = T_{measure}$:\n",
    "   $$\\mathcal{L}_{data} = \\left| u(x,y,T_{measure}) - u_{measured}(x,y) \\right|^2$$\n",
    "\n",
    "Note: We replace the initial condition loss with data loss (inverse problem strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Loss Functions for 2D Inverse Problem\n",
    "# ============================================================================\n",
    "\n",
    "def loss_pde_2d(params, pinn, points, D):\n",
    "    \"\"\"\n",
    "    PDE residual loss for 2D diffusion equation.\n",
    "    PDE: du/dt = D * (d¬≤u/dx¬≤ + d¬≤u/dy¬≤)\n",
    "    \"\"\"\n",
    "    # Define function for automatic differentiation\n",
    "    def u_fn(x, y, t):\n",
    "        point = jnp.array([[x, y, t]])\n",
    "        return pinn.neural_net(params, point)[0, 0]\n",
    "    \n",
    "    # Compute derivatives using JAX\n",
    "    def compute_residual(point):\n",
    "        x, y, t = point[0], point[1], point[2]\n",
    "        \n",
    "        # First derivatives\n",
    "        du_dt = grad(lambda t_: u_fn(x, y, t_))(t)\n",
    "        \n",
    "        # Second derivatives in space\n",
    "        du_dxx = ...\n",
    "        du_dyy = ...\n",
    "        \n",
    "        # PDE residual\n",
    "        residual = du_dt - D * (du_dxx + du_dyy)\n",
    "        return residual ** 2\n",
    "    \n",
    "    # Vectorize over all points\n",
    "    residuals = vmap(compute_residual)(points)\n",
    "\n",
    "\n",
    "    # TODO: Implement your pde loss function\n",
    "    # Hint: Calculate the second derivatives with respect to x and y\n",
    "    # Hint: Calculate ‚àÇu/‚àÇt - D * ‚àá¬≤u and return the mean sqaured value\n",
    "\n",
    "\n",
    "def loss_bc_neumann_2d(params, pinn, points):\n",
    "    \"\"\"\n",
    "    Neumann boundary condition loss: du/dn = 0 at all boundaries.\n",
    "    We need to compute normal derivatives at each boundary.\n",
    "    \"\"\"\n",
    "    def u_fn(x, y, t):\n",
    "        point = jnp.array([[x, y, t]])\n",
    "        return pinn.neural_net(params, point)[0, 0]\n",
    "    \n",
    "    def compute_normal_derivative(point):\n",
    "        x, y, t = point[0], point[1], point[2]\n",
    "        \n",
    "        # Determine which boundary this point is on\n",
    "        # Left (x‚âà0): normal derivative is -du/dx\n",
    "        # Right (x‚âàLx): normal derivative is du/dx\n",
    "        # Bottom (y‚âà0): normal derivative is -du/dy\n",
    "        # Top (y‚âàLy): normal derivative is du/dy\n",
    "        \n",
    "\n",
    "        \n",
    "        # For simplicity, penalize both derivatives at boundaries\n",
    "        # This enforces zero normal derivative regardless of which edge\n",
    "        du_dx = ...\n",
    "        du_dy = ...\n",
    "        return du_dx ** 2 + du_dy ** 2\n",
    "    \n",
    "    # Vectorize over all boundary points\n",
    "    bc_residuals = vmap(compute_normal_derivative)(points)\n",
    "    return jnp.mean(bc_residuals)\n",
    "\n",
    "# ============================================================================\n",
    "# Create an interpolating function for 2D Inverse Problem (already DONE!)\n",
    "# ============================================================================\n",
    "def create_measurement_interpolator(x_grid, y_grid, u_measured):\n",
    "    \"\"\"\n",
    "    Create an interpolation function for 2D measured data.\n",
    "    Uses scipy's RegularGridInterpolator converted to JAX-compatible format.\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import RegularGridInterpolator\n",
    "    \n",
    "    # Create scipy interpolator\n",
    "    interp = RegularGridInterpolator(\n",
    "        (x_grid, y_grid), \n",
    "        u_measured.T,  # Transpose because of array indexing convention\n",
    "        method='cubic',\n",
    "        bounds_error=False,\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    \n",
    "    def interpolate_jax(x, y):\n",
    "        \"\"\"JAX-compatible interpolation function.\"\"\"\n",
    "        # Convert to numpy for scipy, then back to jax\n",
    "        points = np.column_stack((np.array(x), np.array(y)))\n",
    "        values = interp(points)\n",
    "        return jnp.array(values)\n",
    "    \n",
    "    return interpolate_jax\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Total Loss Functions for 2D Inverse Problem (already done!)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def loss_data_2d(params, pinn, points, u_interp_fn):\n",
    "    \"\"\"\n",
    "    Data loss: match measurements at t = t_measurement.\n",
    "    \"\"\"\n",
    "    # Network predictions\n",
    "    predictions = pinn.neural_net(params, points).flatten()\n",
    "    \n",
    "    # Interpolated measurements at the same spatial locations\n",
    "    x_coords = points[:, 0]\n",
    "    y_coords = points[:, 1]\n",
    "    u_measured_values = u_interp_fn(x_coords, y_coords)\n",
    "    \n",
    "    # Mean squared error\n",
    "    return jnp.mean((predictions - u_measured_values) ** 2)\n",
    "\n",
    "\n",
    "def total_loss_2d_inverse(params, pinn, points_interior, points_boundary, \n",
    "                          points_data, D, u_interp_fn, \n",
    "                          w_pde=1.0, w_bc=1.0, w_data=10.0):\n",
    "    \"\"\"\n",
    "    Total loss for 2D inverse problem.\n",
    "    Includes weighting for different loss components.\n",
    "    \"\"\"\n",
    "    l_pde = w_pde * loss_pde_2d(params, pinn, points_interior, D)\n",
    "    l_bc = w_bc * loss_bc_neumann_2d(params, pinn, points_boundary)\n",
    "    l_data = w_data * loss_data_2d(params, pinn, points_data, u_interp_fn)\n",
    "    \n",
    "    total = l_pde + l_bc + l_data\n",
    "    return total, (l_pde, l_bc, l_data)\n",
    "\n",
    "print(\"‚úì Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec291f",
   "metadata": {},
   "source": [
    "## 10.6 Training the 2D Inverse PINN\n",
    "\n",
    "Now we set up the data interpolator and train the PINN to solve the inverse problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195cfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10.6 Prepare data and interpolator\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING DATA INTERPOLATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create coordinate grids matching the FTCS data\n",
    "x_ftcs = np.linspace(0, Lx, final_2d.shape[1])\n",
    "y_ftcs = np.linspace(0, Ly, final_2d.shape[0])\n",
    "\n",
    "print(f\"FTCS data shape: {final_2d.shape}\")\n",
    "print(f\"x grid: {len(x_ftcs)} points from 0 to {Lx}\")\n",
    "print(f\"y grid: {len(y_ftcs)} points from 0 to {Ly}\")\n",
    "\n",
    "# Create interpolator for measured data\n",
    "u_interp_2d = create_measurement_interpolator(x_ftcs, y_ftcs, final_2d)\n",
    "\n",
    "# Test interpolation\n",
    "test_x = jnp.array([0.5, 0.3, 0.7])\n",
    "test_y = jnp.array([0.5, 0.4, 0.6])\n",
    "test_vals = u_interp_2d(test_x, test_y)\n",
    "print(f\"\\nInterpolation test at (0.5, 0.5): {test_vals[0]:.6f}\")\n",
    "\n",
    "print(\"‚úì Data interpolator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Initialize 2D PINN and Geometry\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZING 2D PINN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize geometry for training (from t=0 to t_measurement)\n",
    "geo_2d = Geometry2D(Lx=Lx, Ly=Ly, Nx=50, Ny=50, \n",
    "                    t_start=0.0, t_end=t_measure_2d, Nt=25)\n",
    "\n",
    "# Initialize PINN with larger architecture for 2D\n",
    "layer_sizes_2d = [3, 64, 64, 64, 1]  # 3 inputs (x,y,t), 1 output (u)\n",
    "key_2d = random.PRNGKey(42)\n",
    "pinn_2d = Pinn2D(key_2d, layer_sizes_2d)\n",
    "\n",
    "print(f\"Network architecture: {layer_sizes_2d}\")\n",
    "print(f\"Total parameters: {sum(p['W'].size + p['b'].size for p in pinn_2d.params)}\")\n",
    "print(f\"Time domain: [0, {t_measure_2d}]\")\n",
    "print(\"‚úì PINN initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training Loop for 2D Inverse Problem (JIT-optimized)\n",
    "# ============================================================================\n",
    "\n",
    "import optax\n",
    "\n",
    "def train_pinn_2d_inverse(pinn, geo, D, u_interp_fn, \n",
    "                          n_epochs=5000,\n",
    "                          N_interior=2000, \n",
    "                          N_boundary=400, \n",
    "                          N_data=500,\n",
    "                          learning_rate=1e-3,\n",
    "                          w_pde=1.0, w_bc=1.0, w_data=10.0):\n",
    "    \"\"\"\n",
    "    Train 2D PINN for inverse problem with JIT-compiled training step.\n",
    "    \"\"\"\n",
    "    # TODO: Implement your 2d inverse training loop\n",
    "    # Hint: it exactly the same loop, except for the fucntion you use for the loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f095184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "loss_history_2d, loss_components_2d = train_pinn_2d_inverse(\n",
    "    pinn_2d, geo_2d, D_2d, u_interp_2d,\n",
    "    n_epochs=5000,\n",
    "    N_interior=2000,\n",
    "    N_boundary=400, \n",
    "    N_data=500,\n",
    "    learning_rate=1e-3,\n",
    "    w_pde=1.0,\n",
    "    w_bc=1.0, \n",
    "    w_data=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6045b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize Training History\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(loss_history_2d, linewidth=1.5)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Total Loss')\n",
    "axes[0].set_title('2D Inverse Problem: Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss components\n",
    "axes[1].plot(loss_components_2d['pde'], label='L_PDE', linewidth=1.5, alpha=0.8)\n",
    "axes[1].plot(loss_components_2d['bc'], label='L_BC', linewidth=1.5, alpha=0.8)\n",
    "axes[1].plot(loss_components_2d['data'], label='L_data', linewidth=1.5, alpha=0.8)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss Components')\n",
    "axes[1].set_title('Individual Loss Terms')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c010046",
   "metadata": {},
   "source": [
    "## 10.7 Reconstruction Results\n",
    "\n",
    "Now we evaluate the PINN's ability to reconstruct the initial dose distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f751eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Reconstruct Initial Condition\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECONSTRUCTING INITIAL CONDITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create evaluation grid at t=0\n",
    "Nx_eval = 100\n",
    "Ny_eval = 100\n",
    "x_eval = jnp.linspace(0, Lx, Nx_eval)\n",
    "y_eval = jnp.linspace(0, Ly, Ny_eval)\n",
    "X_eval, Y_eval = jnp.meshgrid(x_eval, y_eval)\n",
    "\n",
    "# Flatten for network evaluation\n",
    "x_flat = X_eval.flatten()\n",
    "y_flat = Y_eval.flatten()\n",
    "t_flat = jnp.zeros_like(x_flat)  # t = 0 for initial condition\n",
    "points_t0 = jnp.column_stack((x_flat, y_flat, t_flat))\n",
    "\n",
    "# Predict initial condition\n",
    "print(\"Evaluating PINN at t=0...\")\n",
    "u_reconstructed = pinn_2d(points_t0).reshape(Nx_eval, Ny_eval)\n",
    "\n",
    "# Also evaluate at measurement time for comparison\n",
    "t_meas_flat = jnp.full_like(x_flat, t_measure_2d)\n",
    "points_tmeas = jnp.column_stack((x_flat, y_flat, t_meas_flat))\n",
    "u_at_measurement = pinn_2d(points_tmeas).reshape(Nx_eval, Ny_eval)\n",
    "\n",
    "print(\"‚úì Reconstruction complete\")\n",
    "\n",
    "# Compute errors\n",
    "# Interpolate true initial condition to evaluation grid\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "ini_interp = RectBivariateSpline(\n",
    "    np.linspace(0, Ly, ini_2d.shape[0]),\n",
    "    np.linspace(0, Lx, ini_2d.shape[1]),\n",
    "    ini_2d\n",
    ")\n",
    "u_true_initial_eval = jnp.array(ini_interp(y_eval, x_eval))\n",
    "\n",
    "# Compute metrics\n",
    "mae_recon = jnp.mean(jnp.abs(u_reconstructed - u_true_initial_eval))\n",
    "mse_recon = jnp.mean((u_reconstructed - u_true_initial_eval) ** 2)\n",
    "max_error_recon = jnp.max(jnp.abs(u_reconstructed - u_true_initial_eval))\n",
    "\n",
    "print(f\"\\nReconstruction Metrics:\")\n",
    "print(f\"  Mean Absolute Error: {mae_recon:.6f}\")\n",
    "print(f\"  Mean Squared Error:  {mse_recon:.6f}\")\n",
    "print(f\"  Maximum Error:       {max_error_recon:.6f}\")\n",
    "print(f\"  Relative MAE:        {mae_recon / jnp.mean(u_true_initial_eval):.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32703b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Comprehensive Visualization\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Row 1: Initial condition (true, reconstructed, error)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im1 = ax1.imshow(u_true_initial_eval, cmap='hot', origin='lower', \n",
    "                 extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('True Initial Distribution\\n(t=0, target)', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "im2 = ax2.imshow(u_reconstructed, cmap='hot', origin='lower', \n",
    "                 extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('PINN Reconstruction\\n(t=0, predicted)', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "error_initial = np.abs(u_reconstructed - u_true_initial_eval)\n",
    "im3 = ax3.imshow(error_initial, cmap='viridis', origin='lower', extent=[0, Lx, 0, Ly])\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title(f'Reconstruction Error\\nMAE = {mae_recon:.4f}', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "# Row 2: Measurement time comparison\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "im4 = ax4.imshow(final_2d, cmap='hot', origin='lower', \n",
    "                 extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('y')\n",
    "ax4.set_title(f'True Measurement\\n(t={t_measure_2d}, FTCS)', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "im5 = ax5.imshow(u_at_measurement, cmap='hot', origin='lower', \n",
    "                 extent=[0, Lx, 0, Ly], vmin=0, vmax=1)\n",
    "ax5.set_xlabel('x')\n",
    "ax5.set_ylabel('y')\n",
    "ax5.set_title(f'PINN at Measurement Time\\n(t={t_measure_2d}, fitted)', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "# Interpolate final_2d to evaluation grid for comparison\n",
    "final_interp = RectBivariateSpline(\n",
    "    np.linspace(0, Ly, final_2d.shape[0]),\n",
    "    np.linspace(0, Lx, final_2d.shape[1]),\n",
    "    final_2d\n",
    ")\n",
    "final_2d_eval = jnp.array(final_interp(y_eval, x_eval))\n",
    "error_measurement = np.abs(u_at_measurement - final_2d_eval)\n",
    "im6 = ax6.imshow(error_measurement, cmap='viridis', origin='lower', extent=[0, Lx, 0, Ly])\n",
    "ax6.set_xlabel('x')\n",
    "ax6.set_ylabel('y')\n",
    "mae_meas = jnp.mean(error_measurement)\n",
    "ax6.set_title(f'Measurement Fit Error\\nMAE = {mae_meas:.4f}', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
    "\n",
    "# Row 3: Cross-sections and diffusion visualization\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "mid_y_idx = Ny_eval // 2\n",
    "ax7.plot(x_eval, u_true_initial_eval[mid_y_idx, :], 'k-', \n",
    "         label='True Initial', linewidth=2.5, alpha=0.8)\n",
    "ax7.plot(x_eval, u_reconstructed[mid_y_idx, :], 'r--', \n",
    "         label='PINN Reconstruction', linewidth=2)\n",
    "ax7.plot(x_eval, final_2d_eval[mid_y_idx, :], 'b:', \n",
    "         label='Measurement', linewidth=2, alpha=0.7)\n",
    "ax7.set_xlabel('x')\n",
    "ax7.set_ylabel('u')\n",
    "ax7.set_title(f'Cross-section at y={y_eval[mid_y_idx]:.2f}', fontsize=11, fontweight='bold')\n",
    "ax7.legend(fontsize=9)\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "mid_x_idx = Nx_eval // 2\n",
    "ax8.plot(y_eval, u_true_initial_eval[:, mid_x_idx], 'k-', \n",
    "         label='True Initial', linewidth=2.5, alpha=0.8)\n",
    "ax8.plot(y_eval, u_reconstructed[:, mid_x_idx], 'r--', \n",
    "         label='PINN Reconstruction', linewidth=2)\n",
    "ax8.plot(y_eval, final_2d_eval[:, mid_x_idx], 'b:', \n",
    "         label='Measurement', linewidth=2, alpha=0.7)\n",
    "ax8.set_xlabel('y')\n",
    "ax8.set_ylabel('u')\n",
    "ax8.set_title(f'Cross-section at x={x_eval[mid_x_idx]:.2f}', fontsize=11, fontweight='bold')\n",
    "ax8.legend(fontsize=9)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Error histogram\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.hist(error_initial.flatten(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax9.axvline(mae_recon, color='red', linestyle='--', linewidth=2, label=f'MAE = {mae_recon:.4f}')\n",
    "ax9.set_xlabel('Absolute Error')\n",
    "ax9.set_ylabel('Frequency')\n",
    "ax9.set_title('Reconstruction Error Distribution', fontsize=11, fontweight='bold')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('2D Fricke Gel Inverse Problem: Complete Results', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721237ab",
   "metadata": {},
   "source": [
    "## 10.8 Key Observations and Extensions\n",
    "\n",
    "### üéØ What We Accomplished\n",
    "\n",
    "1. **Solved a realistic 2D inverse problem** - Reconstructed initial dose distribution from diffused measurements\n",
    "2. **Used existing simulation tools** - Leveraged FTCS module for ground truth data generation\n",
    "3. **Implemented JAX-based PINN** - Custom architecture consistent with the notebook's approach\n",
    "4. **Enforced physical constraints** - PDE and Neumann boundary conditions built into the loss\n",
    "\n",
    "### üî¨ Physical Insights\n",
    "\n",
    "- **Diffusion blurs information**: The longer the time between irradiation and measurement, the harder reconstruction becomes\n",
    "- **Inverse problems are ill-posed**: Small changes in measurements can lead to large changes in reconstructed initial conditions\n",
    "- **Boundary conditions matter**: Neumann (no-flux) BCs are physically appropriate for sealed gel containers\n",
    "\n",
    "### üöÄ Suggested Extensions\n",
    "\n",
    "1. **Time sensitivity analysis**: \n",
    "   - Try different `t_measure_2d` values (0.1, 0.3, 0.5, 1.0)\n",
    "   - Plot reconstruction error vs. measurement time\n",
    "   - Find the \"practical limit\" for reconstruction\n",
    "\n",
    "2. **Noise robustness**:\n",
    "   ```python\n",
    "   # Add noise to measurements\n",
    "   noise_level = 0.05\n",
    "   final_2d_noisy = final_2d + noise_level * np.random.randn(*final_2d.shape)\n",
    "   ```\n",
    "   - How does measurement noise affect reconstruction quality?\n",
    "   - Can regularization improve robustness?\n",
    "\n",
    "3. **Unknown diffusion coefficient**:\n",
    "   - Make `D` a learnable parameter\n",
    "   - Simultaneously learn both initial condition and `D`\n",
    "   - This is a \"double inverse\" problem!\n",
    "\n",
    "4. **Sparse measurements**:\n",
    "   - Instead of full 2D measurement, use only scattered point measurements\n",
    "   - More realistic for actual experimental setups\n",
    "   - Tests PINN's interpolation capabilities\n",
    "\n",
    "5. **Different initial geometries**:\n",
    "   - Try circle, multiple spots, or arbitrary shapes\n",
    "   - Test on clinically relevant dose distributions\n",
    "   - Vary smoothness and complexity\n",
    "\n",
    "6. **Comparison with other methods**:\n",
    "   - Backward FTCS (unstable but instructive)\n",
    "   - Tikhonov regularization\n",
    "   - Traditional deconvolution techniques\n",
    "\n",
    "### üí° Tips for Better Results\n",
    "\n",
    "- **Loss weighting**: Experiment with `w_pde`, `w_bc`, `w_data` ratios\n",
    "- **Network capacity**: Increase layers/neurons for complex distributions\n",
    "- **Training duration**: 2D problems may need 10,000+ epochs\n",
    "- **Learning rate schedule**: Consider decay strategies\n",
    "- **Multiple initializations**: Run with different random seeds and ensemble results\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusion\n",
    "\n",
    "This advanced exercise demonstrates PINNs' power for solving **realistic inverse problems** in medical physics. By combining:\n",
    "- Physical laws (diffusion equation)\n",
    "- Boundary constraints (Neumann conditions)  \n",
    "- Sparse/indirect measurements (diffused dose)\n",
    "\n",
    "We can recover information that would be impossible with data-driven methods alone. This approach is directly applicable to:\n",
    "- **Radiation therapy dosimetry** (Fricke gels)\n",
    "- **Drug delivery** (diffusion in tissues)\n",
    "- **Environmental monitoring** (pollutant dispersion)\n",
    "- **Materials science** (thermal/mass transport)\n",
    "\n",
    "The key insight: **Physics-informed learning bridges the gap between incomplete data and underlying physical reality.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
