{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogx1S-gZ-CFB"
   },
   "source": [
    "# Exercise: 1D Burgers Equation\n",
    "\n",
    "The Cauchy Problem to be solved is the Burgers Equation \n",
    "\\begin{align}\n",
    "&\\partial_t u (x,t) + u \\partial_x u (x,t) - \\nu \\partial_x^2 u (x,t) = 0 \\\\\n",
    "&u(t=0, x) = - \\sin \\left( \\pi \\, x \\right) \\\\\n",
    "&u\\left(t, x=\\pm 1\\right) =  0\n",
    "\\end{align}\n",
    "with $(t,x) \\in [0,1] \\times [-1, +1]$, and where we set\n",
    "$$\n",
    "\\nu = \\frac{0.01}{\\pi}\n",
    "$$\n",
    "\n",
    "\n",
    "------\n",
    "Extra: reviews on PINN\n",
    "\n",
    "[1] https://www.nature.com/articles/s42254-021-00314-5\n",
    "\n",
    "[2] https://arxiv.org/pdf/2202.06416.pdf\n",
    "\n",
    "[3] https://www.mdpi.com/2504-2289/6/4/140\n",
    "\n",
    "[4] https://medium.com/@vignesh.g1609/pinn-physics-informed-neural-networks-5f5f05bf7231\n",
    "\n",
    "[5] https://ocw.mit.edu/courses/18-152-introduction-to-partial-differential-equations-fall-2011/29c6f7ee914a1d804899781f9f604f49_MIT18_152F11_lec_24.pdf , https://ocw.mit.edu/courses/18-152-introduction-to-partial-differential-equations-fall-2011/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d3poK6i-OUh"
   },
   "outputs": [],
   "source": [
    "# Find GPU in this environment\n",
    "import os, subprocess, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''.join(re.findall(\"UUID: (MIG-[^)]+)\\)\", str(subprocess.check_output([\"nvidia-smi\", \"-L\"]), 'ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXzkAalV-Orb",
    "outputId": "d09afa2d-10b5-4a92-8664-9570ba7cdf1f"
   },
   "outputs": [],
   "source": [
    "from typing import Type, Union\n",
    "import gc\n",
    "import tqdm\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "try:\n",
    "    import tqdm\n",
    "except:\n",
    "    %pip install tqdm\n",
    "    import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AARgEft4EmDr"
   },
   "source": [
    "## 1. Define a DNN as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k66Jnd8rCzIS"
   },
   "outputs": [],
   "source": [
    "class PINN_DNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Deep Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # \n",
    "    ):\n",
    "        super(PINN_DNN, self).__init__()\n",
    "        #---\n",
    "\n",
    "    def forward(self, x):\n",
    "        return # ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG54qbWxFLmM"
   },
   "source": [
    "## 2. Define the PDE class with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE0is6wAFK1e"
   },
   "outputs": [],
   "source": [
    "class Burgers_1D_PDE:\n",
    "    \"\"\"\n",
    "        Class computing the 1D Burgers PDE equation:\n",
    "            u_t + u u_x - ν u_xx = 0\n",
    "        Notice that we can conveniently rewrite it as\n",
    "            u_t + (1/2)*(u**2)_x- ν u_xx = 0\n",
    "\n",
    "        --------------------------\n",
    "        Args:\n",
    "            nu    (float): Float parameter describing the diffusion term\n",
    "\n",
    "        Methods:\n",
    "            compute_heat      (coords, pred_funcs): returns the computed Heat eq.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nu: float = float(0.01/np.pi),     # PDE coefficient\n",
    "    ):\n",
    "        self.nu = nu\n",
    "\n",
    "    def compute_burgers(self, coords: torch.Tensor, pred_func: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute 1D Heat equation\n",
    "            u_t + (u**2)_x/2 - ν u_xx\n",
    "        It should equate to zero.\n",
    "\n",
    "        Args:\n",
    "            coords    (torch.Tensor): (t,x) coords.\n",
    "            pred_func (torch.Tensor): (u) evaluated at (t,x)\n",
    "        \"\"\"\n",
    "        # ----\n",
    "\n",
    "    def get_derivative(self, y, x, n: int = 1):\n",
    "        \"\"\"\n",
    "        General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(y.device), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "        return self.get_derivative(dy_dx, x, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccFeNNEXITSY"
   },
   "source": [
    "### 2.0 test the PDE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66EteRsZISR3",
    "outputId": "dab435ef-d0b1-4fa3-8f8a-8a59b570f017"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test the Burgers_1D_PDE class\n",
    "\"\"\"\n",
    "nu_param = float(0.01/np.pi)\n",
    "\n",
    "# .....\n",
    "\n",
    "print(f\"Pred: {heat_pred.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9Ou3qo9GeL2"
   },
   "source": [
    "## 3. Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYXJKfDR-B4L"
   },
   "outputs": [],
   "source": [
    "class Burgers_1D_BC:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_function = nn.MSELoss()\n",
    "    ):\n",
    "        # Cost function\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def boundary_cond(self, coords: torch.Tensor, pred_func: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Class for computing BC\n",
    "            u(t, x=±1) = 0\n",
    "\n",
    "        Args:\n",
    "             coords    (torch.Tensor) : The coords at the spatial boundary\n",
    "             pred_func (torch.Tensor) : The predicted function at those points\n",
    "        Returns\n",
    "             bc_loss (torch.tensor)\n",
    "        \"\"\"\n",
    "        #....\n",
    "\n",
    "    def initial_cond(self, coords: torch.Tensor, pred_func: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Class for computing IC\n",
    "            u(x, t=0) = - sin(π x)\n",
    "\n",
    "        Args:\n",
    "             coords    (torch.Tensor) : The coords at the Initial Time\n",
    "             pred_func (torch.Tensor) : The predicted function at those points\n",
    "        Returns\n",
    "             ic_loss (torch.tensor)\n",
    "        \"\"\"\n",
    "        #...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvblWSMx4j-M"
   },
   "source": [
    "### 3.0 Test Boundary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "0LCyrnuw4mIN",
    "outputId": "c680344d-15e6-49d6-fa86-489e0a69e5cb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test the BC_PDE class\n",
    "\"\"\"\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(ic_coords, requires_grad=True)\n",
    "\n",
    "test_pde.get_derivative(- torch.sin(np.pi * X), X, 1)[:,1] + np.pi* torch.cos(np.pi * X[:,1])\n",
    "\n",
    "test_pde.get_derivative( \n",
    "    test_bc.initial_cond(\n",
    "        X, - torch.sin(np.pi * X[:,1]).unsqueeze(-1)\n",
    "    ), X\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WejxF69sPCuU"
   },
   "source": [
    "## 4. Assemble PINN model\n",
    "\n",
    "--------------\n",
    "Refs:\n",
    "\n",
    "[1] https://towardsdatascience.com/improving-pinns-through-adaptive-loss-balancing-55662759e701 , https://github.com/rbischof/relative_balancing , https://arxiv.org/abs/2110.09813\n",
    "\n",
    "[2] https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/theory/advanced_schemes.html#softadapt , original paper: https://arxiv.org/pdf/1912.12355.pdf\n",
    "\n",
    "[3] https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/theory/advanced_schemes.html#learning-rate-annealing . original paper https://arxiv.org/pdf/2001.04536.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HG5ujO4APcWo"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_line_to_file(LOG_FILE: str, log_line: str):\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(log_line)\n",
    "\n",
    "def store_hyp_dict(json_file: str, hyperparam_kwargs: dict, _indent: int = 4):\n",
    "    with open(json_file, 'w') as fp:\n",
    "        json.dump(hyperparam_kwargs, fp, indent=_indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inJ9QvwmPduX"
   },
   "outputs": [],
   "source": [
    "# Checkpoints (to save model parameters during training)\n",
    "# this is implemented by writing a python class that uses the torch.save method\n",
    "class SaveBestModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'best_model',\n",
    "        best_valid_loss=float('inf')\n",
    "    ): #object initialized with best_loss = +infinite\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss,\n",
    "        epoch, model, optimizer, criterion\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch}\\n\")\n",
    "            # method to save a model (the state_dict: a python dictionary object that\n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, f'{self.model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcc_9T7DPCDp"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Full PINN Class for the FORWARD problem.\n",
    "\n",
    "    It incorporates:\n",
    "        1. The PINN DNN (in self.DNN)\n",
    "        2. The PDE      (in self._PDE)\n",
    "        3. The BC       (in self._BC)\n",
    "\n",
    "    It exposes the train_model() method to solve the PINN forward problem, by Performing a training with ADAM optimiser\n",
    "\n",
    "    The train_model() method is thus:\n",
    "        1. ADAM loop\n",
    "            1.1. call training_step() for adam\n",
    "            1.2. Store best model\n",
    "            1.3. Perform step for learning rate stepper\n",
    "            1.4. Logs\n",
    "            1.5. Check if patience reached;\n",
    "\n",
    "    Args:\n",
    "        For Args see init method.\n",
    "\n",
    "    Methods:\n",
    "        init_model      ()  : init the model.\n",
    "        load_best_model ()  : reload the best model.\n",
    "        forward         (x) : DNN Forward passing\n",
    "        pde_loss        (x, pred_funcs) : Compute PDE loss using self.cost_function\n",
    "        ic_loss         (x, pred_funcs) : Compute IC  loss using BC class\n",
    "        bc_loss         (x, pred_funcs) : Compute BC  loss using BC class\n",
    "        soft_adapt      (losses, previous_losses, eps = 1e-8) : perform SoftAdapt algorithm.\n",
    "        generate_coords ()  : generate the coords\n",
    "        closure         ()  : method performing the backprop. Fundamental for L-BFGS part, it is also used for ADAM.\n",
    "        training_step   (epoch, use_adam)   : Single training step.\n",
    "        train_model     ()  : Main method. Trains the model.\n",
    "        store_training_df   ()  : utils method to store the training history as csv\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_rec: bool = True, # parameter to flag False if PINN solve PDE without reconstruction info\n",
    "        # DNN\n",
    "        n_inputs : int = 2,      # number of inputs, e.g. x,y,z,t,....\n",
    "        n_outputs: int = 1,      # number of outputs, e.g. u,v,P, T, ....\n",
    "        hidden_layers: list = [4, 8, 16, 8], # number of hidden layers\n",
    "        dropout: float = 0.2,  # Dropout\n",
    "        activation_func = nn.Tanh(),\n",
    "        learning_rate: float = 0.001,\n",
    "        # True func\n",
    "        exact_solution_func = lambda x: x,\n",
    "        # Geometry\n",
    "        time_interval : list = [0.0,  +1.0] ,\n",
    "        space_interval: list = [-1.0, +1.0] ,\n",
    "        # PDE\n",
    "        diffusion_coefficient: float = float(0.01/np.pi),     # PDE coefficient\n",
    "        # Training\n",
    "        patience_training: int = 200,\n",
    "        epochs: int = 1000,\n",
    "        N_batches: int = 32,\n",
    "        patience_lr: int = 50,\n",
    "        # Soft adapt\n",
    "        use_softadapt : bool = False,\n",
    "        softadapt_starting_epoch : int = 5,\n",
    "        # Loss weights\n",
    "        weight_rec: float = 1.0,\n",
    "        weight_pde: float = 1.0,\n",
    "        weight_bc : float = 1.0,\n",
    "        weight_ic : float = 1.0,\n",
    "        # dataloaders\n",
    "        fun_batch_size: int = 4096,\n",
    "        pde_batch_size: int = 4096,\n",
    "        bc_batch_size: int = 1024,\n",
    "        ic_batch_size: int = 1024,\n",
    "        # device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # Model name\n",
    "        BASE_PATH_TO_STORE: str = '.',\n",
    "        model_name: str = 'pinne_heat',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Init method.\n",
    "\n",
    "        Args:\n",
    "            use_rec     (bool, optional)    : Boolean to pick if rec_loss should be used. Defaults to True.\n",
    "            n_inputs    (int, optional)     : Number of DNN inputs. Defaults to 2.\n",
    "            n_outputs   (int, optional)     : Number of DNN outputs. Defaults to 1.\n",
    "            hidden_layers (list, optional)  : Listof hidden layers dims. Defaults to [4, 8, 16, 8].\n",
    "            learning_rate (float, optional) : Adam Learning rate. Defaults to 0.001.\n",
    "            exact_solution_func     (function, optional): Method to generate the exact solution. Defaults to exact_solution_func.\n",
    "            diffusion_coefficient   (float, optional)   : Heat diffusion coefficient. Defaults to 0.01/π.\n",
    "            patience_training       (int, optional)     : Patience in ADAM training. Defaults to 200.\n",
    "            epochs                  (int, optional)     : Total Number of ADAM epochs. Defaults to 1000.\n",
    "            N_batches               (int, optional)     : Number of ADAM Batches. Defaults to 32.\n",
    "            patience_lr             (int, optional)     : Patience for LR stepper. Defaults to 50.\n",
    "            use_softadapt            (bool, optional) : Boolean to decide whether to use SoftAdapt algorithm. Defaults to True,\n",
    "            softadapt_starting_epoch (int, optional)  : SoftAdapt starting epoch. Defaults to 5.\n",
    "            weight_rec  (float, optional)   : Reconstruction Loss weight. Defaults to 1.0.\n",
    "            weight_pde  (float, optional)   : PDE Loss weight. Defaults to 1.0.\n",
    "            weight_bc   (float, optional)   : BC Loss weight. Defaults to 1.0.\n",
    "            weight_ic   (float, optional)   : IC Loss weight. Defaults to 1.0.\n",
    "            fun_batch_size  (int, optional) : Batch Size for computing rec loss. Defaults to 4096.\n",
    "            pde_batch_size  (int, optional) : Batch Size for computing pde loss. Defaults to 4096.\n",
    "            bc_batch_size   (int, optional) : Batch Size for computing bc  loss. Defaults to 1024.\n",
    "            device      (_type_, optional)  : Device. Defaults to torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\").\n",
    "            BASE_PATH_TO_STORE  (str, optional) : Path to store training and model data. Defaults to '.'.\n",
    "            model_name          (str, optional) : Model name. Defaults to 'pinne_heat'.\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "        self.use_rec = use_rec\n",
    "        # geometry\n",
    "        self._t_min , self._t_max = time_interval\n",
    "        self._x_min , self._x_max = space_interval\n",
    "        # ==== DNN PART ==================\n",
    "        self.n_inputs      = n_inputs\n",
    "        self.n_outputs     = n_outputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_layers      = len(hidden_layers)\n",
    "        self.dropout_prob  = dropout\n",
    "        self.activation_func = activation_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        # ==== SoftAdapt PART ==================\n",
    "        self.use_softadapt = use_softadapt\n",
    "        self.softadapt_starting_epoch = softadapt_starting_epoch if softadapt_starting_epoch >= 2 else 2\n",
    "        # ====  PDE params PART ==================\n",
    "        self.diffusion_coefficient = diffusion_coefficient\n",
    "        self.exact_solution_func   = exact_solution_func\n",
    "\n",
    "        # ==== PINN-DNN PART ==================\n",
    "        # Dataloader\n",
    "        self.fun_batch_size = fun_batch_size\n",
    "        self.pde_batch_size = pde_batch_size\n",
    "        self.ic_batch_size  = ic_batch_size\n",
    "        self.bc_batch_size  = bc_batch_size\n",
    "        self.N_batches   = N_batches\n",
    "        # training vars\n",
    "        self.patience_training = patience_training\n",
    "        self.epochs = epochs\n",
    "        self.patience_lr     = patience_lr\n",
    "        # Loss weights\n",
    "        self.weight_rec = weight_rec\n",
    "        self.weight_pde = weight_pde\n",
    "        self.weight_bc  = weight_bc\n",
    "        self.weight_ic  = weight_ic\n",
    "        # DNN\n",
    "        self.model_kwargs = {\n",
    "           \"n_inputs\"     : self.n_inputs,\n",
    "            \"n_outputs\"   : self.n_outputs,\n",
    "            \"hidden_dims\" : self.hidden_layers,\n",
    "            \"dropout\"     : self.dropout_prob,\n",
    "        }\n",
    "        self.init_model()\n",
    "        # Optimisers\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.DNN.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "        # LR scheduler\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor  = 0.1, # Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "            patience= self.patience_lr, #  Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        )\n",
    "\n",
    "        # cost function\n",
    "        self.cost_function = nn.MSELoss() # Mean squared error\n",
    "\n",
    "        # ==== PDE PART ==================\n",
    "        # PDE+BC\n",
    "        self._PDE = Burgers_1D_PDE(nu=self.diffusion_coefficient)\n",
    "        self._BC  = Burgers_1D_BC(\n",
    "            cost_function = self.cost_function\n",
    "        )\n",
    "\n",
    "        # save best model\n",
    "        self.BASE_PATH_TO_STORE = BASE_PATH_TO_STORE\n",
    "        self.model_name = model_name\n",
    "        self.full_path_to_store = f\"{BASE_PATH_TO_STORE}/{model_name}\"\n",
    "        self.save_best_model = SaveBestModel(model_name=self.full_path_to_store) #initialize checkpoint function\n",
    "\n",
    "        # Storing\n",
    "        # === STORE HYPERPARAMETERS =====\n",
    "        self.hyperparam_kwargs = {\n",
    "            # Model kwargs\n",
    "            **self.model_kwargs,\n",
    "            \"activation_func\" : f\"{self.activation_func}\",\n",
    "            # Dataset info\n",
    "            'train_size': (self.pde_batch_size+self.bc_batch_size)*self.epochs,\n",
    "            # Hyperparameters\n",
    "            'epochs'   : self.epochs,\n",
    "            'patience' : self.patience_training,\n",
    "            'lr_patience': self.patience_lr\n",
    "        }\n",
    "        store_hyp_dict(f'{self.full_path_to_store}.json', self.hyperparam_kwargs)\n",
    "        # logs\n",
    "        self.LOG_FILE = f\"{self.full_path_to_store}.txt\"\n",
    "\n",
    "        # === Training vars =====\n",
    "        self.training_rec_loss = []\n",
    "        self.training_pde_loss = []\n",
    "        self.training_bc_loss  = []\n",
    "        self.training_ic_loss  = []\n",
    "\n",
    "        self.training_loss   = []\n",
    "        self.learning_rates  = []\n",
    "\n",
    "        self.weight_rec_history = []\n",
    "        self.weight_pde_history = []\n",
    "        self.weight_bc_history  = []\n",
    "        self.weight_ic_history  = []\n",
    "\n",
    "        self.Delta_training = 0 # Delta value for L-FBGS traingin\n",
    "\n",
    "        self.current_epoch = 0 # store current epoch to pass to closure for soft-adapt\n",
    "\n",
    "    def init_model(self):\n",
    "        self.DNN = PINN_DNN(\n",
    "            activation_func=self.activation_func,\n",
    "            **self.model_kwargs,\n",
    "        ).to(self.device)\n",
    "\n",
    "    def load_best_model(self):\n",
    "        try:\n",
    "            # load the best model\n",
    "            RELOAD_MODEL_NAME = f\"{self.full_path_to_store}.pth\"\n",
    "            checkpoint = torch.load(RELOAD_MODEL_NAME)\n",
    "            self.init_model()\n",
    "            self.DNN.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded best model: {RELOAD_MODEL_NAME} at epoch: {checkpoint['epoch']}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Impossible to load {RELOAD_MODEL_NAME}\\nError: {e}\\n\")\n",
    "            pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward step via DNN\n",
    "        \"\"\"\n",
    "        pred_funcs = self.DNN(x)\n",
    "        return pred_funcs\n",
    "\n",
    "    def pde_loss(self, x, pred_funcs):\n",
    "        \"\"\"\n",
    "        Method to compute the PDE loss;\n",
    "        the PDE class gives the evaluation of the LHS of the PDE system, i.e.\n",
    "            PDE[u](t,x) = 0\n",
    "        Here we use the cost function to compute the loss.\n",
    "        \"\"\"\n",
    "        burgers_eq = self._PDE.compute_burgers(x, pred_funcs)\n",
    "\n",
    "        # The PDE loss is the mean of the squared (0 - PDE)^2 -- see nVidia Modulus example docs\n",
    "        loss_pde  = self.cost_function(\n",
    "            burgers_eq,\n",
    "            torch.zeros_like(burgers_eq).to(burgers_eq.device)\n",
    "        )\n",
    "\n",
    "        return loss_pde\n",
    "\n",
    "    def bc_loss(self, x, preds_funcs):\n",
    "        \"\"\"\n",
    "        Method to compute the Boundary Condition Loss.\n",
    "        It is already implemented into the BC class, so we simply invoke it.\n",
    "        \"\"\"\n",
    "        bc_loss = self._BC.boundary_cond(x, preds_funcs)\n",
    "        return bc_loss\n",
    "\n",
    "    def ic_loss(self, x, preds_funcs):\n",
    "        \"\"\"\n",
    "        Method to compute the Initial Condition Loss.\n",
    "        It is already implemented into the BC class, so we simply invoke it.\n",
    "        \"\"\"\n",
    "        ic_loss = self._BC.initial_cond(x, preds_funcs)\n",
    "        return ic_loss\n",
    "\n",
    "    def soft_adapt(self, losses: list, previous_losses: list, eps: float = 1e-8) -> list:\n",
    "        \"\"\"\n",
    "        Performs the softadapt computation:\n",
    "\n",
    "        Args:\n",
    "            losses          (list) : list of losses to compute softadapt on\n",
    "            previous_losses (list) : list of losses at previous step\n",
    "            eps (float) : Factor to avoid division by zero. Defaults to 1e-8\n",
    "\n",
    "        Returns\n",
    "            (list) list of W params\n",
    "        \"\"\"\n",
    "        Li = np.array(losses)\n",
    "        Lo = np.array(previous_losses)\n",
    "\n",
    "        _ratio = (Li)/(Lo + eps)\n",
    "        _mu = np.max(_ratio)\n",
    "\n",
    "        _ratio = torch.tensor(_ratio, requires_grad=False)\n",
    "\n",
    "        _w = nn.functional.softmax(_ratio - _mu)\n",
    "\n",
    "        return _w.tolist()\n",
    "\n",
    "    def generate_coords(self):\n",
    "        \"\"\"\n",
    "        Method to generate the coords.\n",
    "        Notice that we have implemented here a RANDOM EXTRACTOR.\n",
    "\n",
    "        Returns\n",
    "            coords    (torch.Tensor) : bulk coordinates\n",
    "            ic_coords (torch.Tensor) : IC coordinates\n",
    "            bc_coords (torch.Tensor) : BC coordinates\n",
    "        \"\"\"\n",
    "        coords = torch.cat(\n",
    "            (\n",
    "                self._t_min + (self._t_max - self._t_min)*torch.rand(self.pde_batch_size).unsqueeze(-1), # t\n",
    "                self._x_min + (self._x_max - self._x_min)*torch.rand(self.pde_batch_size).unsqueeze(-1)  # x\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        coords = Variable(coords.float(), requires_grad=True)\n",
    "        # initial cond\n",
    "        ic_coords = torch.cat(\n",
    "            (\n",
    "                self._t_min * torch.ones(self.ic_batch_size).unsqueeze(-1), # t\n",
    "                self._x_min + (self._x_max - self._x_min)*torch.rand(self.ic_batch_size).unsqueeze(-1)  # x\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        # bc\n",
    "        bc_coords_p = torch.cat(\n",
    "            (\n",
    "                self._t_min + (self._t_max - self._t_min)*torch.rand(self.bc_batch_size//2).unsqueeze(-1), # t\n",
    "                self._x_max*torch.ones(self.bc_batch_size//2).unsqueeze(-1)  # x\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        bc_coords_m = torch.cat(\n",
    "            (\n",
    "                self._t_min + (self._t_max - self._t_min)*torch.rand(self.bc_batch_size//2).unsqueeze(-1), # t\n",
    "                self._x_min*torch.ones(self.bc_batch_size//2).unsqueeze(-1)  # x\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        bc_coords = torch.cat([bc_coords_m, bc_coords_p])\n",
    "\n",
    "        ic_coords = Variable(ic_coords.float(), requires_grad=True)\n",
    "        bc_coords = Variable(bc_coords.float(), requires_grad=True)\n",
    "        # to device\n",
    "        coords     = coords.to(self.device)\n",
    "        ic_coords  = ic_coords.to(self.device)\n",
    "        bc_coords  = bc_coords.to(self.device)\n",
    "\n",
    "        return coords, ic_coords, bc_coords\n",
    "\n",
    "    def closure(self):\n",
    "        \"\"\"\n",
    "        Example of closure func:\n",
    "\n",
    "        if torch.is_grad_enabled():\n",
    "            self.lbfgs_optimizer.zero_grad()\n",
    "        output = self(X_)\n",
    "        loss = self.lossFct(output, y_)\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "        return loss\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        # =================== random extraction =====================\n",
    "        # func + pde\n",
    "        coords, ic_coords, bc_coords = self.generate_coords()\n",
    "        # =================== forward =====================\n",
    "        true_funcs = self.exact_solution_func(coords)# true\n",
    "        pred_funcs = self.forward(coords)            # DNN pred\n",
    "        pred_ic    = self.forward(ic_coords)         # DNN pred - ic\n",
    "        pred_bc    = self.forward(bc_coords)         # DNN pred - bc\n",
    "        # =================== Losses =====================\n",
    "        pde_loss = self.pde_loss(coords, pred_funcs)                                                                        # PDE loss\n",
    "        ic_loss  = self.ic_loss(ic_coords, pred_ic)                                                                         # BC loss\n",
    "        bc_loss  = self.bc_loss(bc_coords, pred_bc)                                                                         # BC loss\n",
    "        rec_loss = self.cost_function(pred_funcs, true_funcs.unsqueeze(-1)) if self.use_rec else torch.zeros_like(pde_loss) # rec loss\n",
    "        # =================== soft-adapt =================\n",
    "        if self.use_softadapt and self.current_epoch >= self.softadapt_starting_epoch:\n",
    "            if self.use_rec:\n",
    "                _losses   =  [rec_loss.item(), pde_loss.item(), ic_loss.item(), bc_loss.item() ]\n",
    "                _p_losses =  [self.training_rec_loss[-1], self.training_pde_loss[-1], self.training_ic_loss[-1], self.training_bc_loss[-1] ]\n",
    "                self.weight_rec, self.weight_pde, self.weight_ic, self.weight_bc = self.soft_adapt(\n",
    "                    losses          = _losses ,\n",
    "                    previous_losses = _p_losses\n",
    "                )\n",
    "            else:\n",
    "                _losses   = [pde_loss.item(), ic_loss.item(), bc_loss.item() ]\n",
    "                _p_losses = [self.training_pde_loss[-1],  self.training_ic_loss[-1], self.training_bc_loss[-1] ]\n",
    "                self.weight_pde, self.weight_ic, self.weight_bc = self.soft_adapt(\n",
    "                    losses          = _losses ,\n",
    "                    previous_losses = _p_losses\n",
    "                )\n",
    "\n",
    "        # full loss\n",
    "        loss = self.weight_pde * pde_loss + self.weight_ic * ic_loss  + self.weight_bc * bc_loss  # <=== Full loss here ====\n",
    "        if self.use_rec:\n",
    "            loss += self.weight_rec * rec_loss\n",
    "        # Normalise loss\n",
    "        loss = loss/( self.weight_rec + self.weight_pde + self.weight_ic + self.weight_bc ) if  self.use_rec else  loss/( self.weight_pde + self.weight_ic + self.weight_bc )\n",
    "\n",
    "        # Append\n",
    "        self.train_loss += loss.item()\n",
    "        self.train_rec_loss += rec_loss.item()\n",
    "        self.train_pde_loss += pde_loss.item()\n",
    "        self.train_ic_loss  += ic_loss.item()\n",
    "        self.train_bc_loss  += bc_loss.item()\n",
    "        #==== backward ======================\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, epoch: int, use_adam: bool = True):\n",
    "        \"\"\"\n",
    "        Single Epoch training step\n",
    "        \"\"\"\n",
    "        # =====================================================\n",
    "        # Training\n",
    "        self.DNN.train()\n",
    "        self.train_loss = 0\n",
    "        self.train_rec_loss = 0\n",
    "        self.train_pde_loss = 0\n",
    "        self.train_ic_loss  = 0\n",
    "        self.train_bc_loss  = 0\n",
    "        # =================== Batches iterations - only ADAM =====================\n",
    "        for _ in tqdm.tqdm(range(self.N_batches)):\n",
    "            # =================== closure ====================\n",
    "            self.optimizer.step(self.closure)\n",
    "        # =================== compute LOSS ====================\n",
    "        # set divisor for batches\n",
    "        _divisor = self.N_batches\n",
    "        # training loss\n",
    "        tr_loss = self.train_loss/_divisor\n",
    "        # others\n",
    "        tr_rec_loss = self.train_rec_loss/_divisor\n",
    "        tr_pde_loss = self.train_pde_loss/_divisor\n",
    "        tr_ic_loss  = self.train_ic_loss /_divisor\n",
    "        tr_bc_loss  = self.train_bc_loss /_divisor\n",
    "        # === Append Losses ========\n",
    "        self.training_loss.append(tr_loss) ## Full trainloss\n",
    "        # others\n",
    "        self.training_rec_loss.append(tr_rec_loss)\n",
    "        self.training_pde_loss.append(tr_pde_loss)\n",
    "        self.training_ic_loss.append(tr_ic_loss)\n",
    "        self.training_bc_loss.append(tr_bc_loss)\n",
    "        # append weights\n",
    "        self.weight_rec_history.append(self.weight_rec)\n",
    "        self.weight_pde_history.append(self.weight_pde)\n",
    "        self.weight_ic_history.append(self.weight_ic)\n",
    "        self.weight_bc_history.append(self.weight_bc)\n",
    "\n",
    "        return tr_loss\n",
    "\n",
    "    def train_model(self):\n",
    "        t0 = time.time()\n",
    "\n",
    "        self.training_rec_loss = []\n",
    "        self.training_pde_loss = []\n",
    "        self.training_ic_loss  = []\n",
    "        self.training_bc_loss  = []\n",
    "\n",
    "        self.training_loss   = []\n",
    "        self.learning_rates  = []\n",
    "        #==== ADAM TRAINING LOOP ================================================================================\n",
    "        for epoch in range(0, self.epochs):\n",
    "            self.current_epoch = epoch\n",
    "            loss = self.training_step(epoch)\n",
    "\n",
    "            #=== GO ON ====\n",
    "            #save best model\n",
    "            self.save_best_model(loss, epoch, self.DNN, self.optimizer, self.cost_function)\n",
    "\n",
    "            # Learning Rate stepper\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.learning_rates.append(current_lr)\n",
    "            # update learning rate schedule\n",
    "            self.lr_scheduler.step(loss) ### NB: ONLY FOR ReduceLROnPlateau\n",
    "\n",
    "            # =================== log ========================\n",
    "            log_line = f'====> Epoch: {epoch}\\tTraining loss: {loss:.6f}\\tlr: {current_lr:.2e}\\tTime: {datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}\\n'\n",
    "            print(log_line)\n",
    "            write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=log_line)\n",
    "\n",
    "            # update number of epochs passed\n",
    "            self.Delta_training += 1\n",
    "            # Check patience\n",
    "            if self.patience_training > 0 and len(self.training_loss) - np.array(self.training_loss).argmin() > self.patience_training:\n",
    "                break_log = f\"\\nPatience treshold = {self.patience_training} reached.\\nExiting at epoch {epoch}.\\n\"\n",
    "                print(break_log)\n",
    "                write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=break_log)\n",
    "                break\n",
    "\n",
    "        # Close up\n",
    "        log_line = f'\\n\\nTotal ADAM training time: {time.time() - t0}\\tEnd time: {datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}\\n\\n'\n",
    "        print(log_line)\n",
    "        write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=log_line)\n",
    "\n",
    "        # store history df\n",
    "        self.store_training_df()\n",
    "\n",
    "    def store_training_df(self):\n",
    "        # store as pandas csv\n",
    "        df_train = pd.DataFrame(\n",
    "            {\n",
    "                \"epochs\"          : [ epoch for epoch in range(len(self.training_loss)) ],\n",
    "                \"training_loss\"   : self.training_loss,\n",
    "                'lr'              : self.learning_rates,\n",
    "                \"rec_train_losses\": self.training_rec_loss,\n",
    "                \"pde_train_losses\": self.training_pde_loss,\n",
    "                \"ic_train_losses\" : self.training_ic_loss,\n",
    "                \"bc_train_losses\" : self.training_bc_loss,\n",
    "                \"weight_rec\"      : self.weight_rec_history,\n",
    "                \"weight_pde\"      : self.weight_pde_history,\n",
    "                \"weight_ic\"       : self.weight_ic_history,\n",
    "                \"weight_bc\"       : self.weight_bc_history,\n",
    "            }\n",
    "        )\n",
    "        df_train.to_csv(f'{self.BASE_PATH_TO_STORE}/{self.model_name}_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAYdhLbUToG4"
   },
   "source": [
    "### 4.0 Try model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G812Og2GTqD5",
    "outputId": "7648338d-d7a7-4c44-efe7-aad91f8e5946"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    %pip install torchsummary\n",
    "    from torchsummary import summary\n",
    "\n",
    "nu_param = float(0.01/np.pi)\n",
    "\n",
    "pinn_model = PINN(\n",
    "    time_interval = [0.0,  +1.0] ,\n",
    "    space_interval= [-1.0, +1.0] ,\n",
    "    diffusion_coefficient =nu_param,\n",
    "    hidden_layers = [\n",
    "        #128, 128, 128\n",
    "        #40,40,40,40,40,40\n",
    "        64, 64, 64, 64, 64\n",
    "    ],\n",
    "    activation_func=nn.GELU(), #nn.Tanh(), \n",
    "    exact_solution_func=lambda x: x ,\n",
    "    learning_rate = 0.001,\n",
    "    BASE_PATH_TO_STORE='./model_data',\n",
    "    model_name = 'pinne_burgers_prova',\n",
    "    use_rec = False,\n",
    "    #use_softadapt = False,\n",
    "    weight_rec = 1.0,\n",
    "    weight_pde = 1.0,\n",
    "    weight_bc  = 1.0,\n",
    "    weight_ic  = 1.0,\n",
    "    #weight_pde = 0.5,\n",
    "    #weight_bc  = 1.5,\n",
    "    #weight_ic  = 4.5,\n",
    "    pde_batch_size = 4096,\n",
    "    bc_batch_size  = 4096,\n",
    "    ic_batch_size  = 4096,\n",
    "    # Training\n",
    "    patience_training = 200,\n",
    "    epochs      = 1000,\n",
    "    N_batches   = 32,\n",
    "    patience_lr = 50,\n",
    "    )\n",
    "\n",
    "print(pinn_model.DNN)\n",
    "\n",
    "print(f\"\\n\\nTorchSummary:\\n\")\n",
    "summary(pinn_model.DNN, input_size=( 2, ), batch_size=4096, device=pinn_model.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jbgvflNThAl"
   },
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIQ2ll1nSyPz",
    "outputId": "44a480e6-38e6-4189-eb02-2011ba8a2ed1"
   },
   "outputs": [],
   "source": [
    "pinn_model.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49UmIJ62uxdB"
   },
   "source": [
    "### 5.1. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKHqz0lTULCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqaveYMcu9I4"
   },
   "source": [
    "## 6. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yay7DcXzug7G"
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "pinn_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Physics-Informed NN",
   "language": "python",
   "name": "pinn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
