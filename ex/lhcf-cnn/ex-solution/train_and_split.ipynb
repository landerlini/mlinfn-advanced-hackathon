{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c270a39-3ce3-4737-9cc7-26397b15498d",
   "metadata": {},
   "source": [
    "# Why Use TFRecord Instead of HDF5 for Neural Network Training?\r\n",
    "\r\n",
    "When preparing a dataset for neural network training, the **TFRecord** format is often preferred over **HDF5 (h5)** for the following reasons:\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. Optimization for TensorFlow\r\n",
    "- **TFRecord** is specifically designed for TensorFlow.\r\n",
    "- It is natively supported by TensorFlow and optimized for input pipelines (`tf.data.Dataset`), enabling more efficient data loading and preprocessing.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. Sequential Read Performance\r\n",
    "- **TFRecord** is a sequential binary format optimized for linear reads from disk.\r\n",
    "- This improves performance when loading large amounts of data during training.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. Data Streaming\r\n",
    "- With **TFRecord**, data can be read and consumed as a continuous stream (**streaming**).\r\n",
    "- This is useful for training on very large datasets that cannot fit entirely in memory.\r\n",
    "- **HDF5**, on the other hand, often requires direct file access and is not optimized for continuous streaming.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 4. Support for Parallelism\r\n",
    "- **TFRecord** integrates with TensorFlow pipelines to support:\r\n",
    "  - **Prefetching**: Loading future batches in parallel with training.\r\n",
    "  - **Shuffling**: Efficiently shuffling data to avoid correlations between consecutive batches.\r\n",
    "  - **Caching**: Storing preprocessed data in memory.\r\n",
    "- **HDF5**, while supporting parallel reads with some libraries, is not as optimized for parallelism with TensorFlow.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "- **TFRecord** is the ideal format for training with TensorFlow due to:\r\n",
    "  - Efficiency\r\n",
    "  - Scalability\r\n",
    "  - Native integration\r\n",
    "cesso casuale ai dati\r\n",
    "  - Compatibilità con altri strumenti\r\n",
    "  - Esplorazione approfondita del dataset\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e5870f-8802-444f-91dd-c582b474c1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 root root 5.6G Nov 14 11:37 /tmp/lhcf-cnn/combined_data.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=/tmp/lhcf-cnn\n",
    "\n",
    "if [ ! -d $DATA_DIR ]; then\n",
    "  mkdir -p $DATA_DIR\n",
    "fi\n",
    "\n",
    "if [ ! -f $DATA_DIR/combined_data.h5 ]; then\n",
    "  wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/lhcf-cnn/combined_data.h5 -O $DATA_DIR/combined_data.h5 &> .log\n",
    "fi\n",
    "\n",
    "ls -lrth $DATA_DIR/combined_data.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58015880-4afc-4c9c-b768-77fd9a307f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:58:20.179497: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-16 15:58:20.179721: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-16 15:58:20.179765: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 15:58:20.189277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb43afd-973d-413f-9a87-b03e43bfeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/tmp/lhcf-cnn\"\n",
    "src_fname = f\"{PATH}/combined_data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44d1f29-e38c-43b3-a49b-7e5e8ca31c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'dE', 'posdE_01xy', 'posdE_23x', 'posdE_23y']\n",
      "\n",
      "ID:\n",
      "Shape: (10000,)\n",
      "Data type: bool\n",
      "\n",
      "dE:\n",
      "Shape: (10000, 16, 1)\n",
      "Data type: float16\n",
      "\n",
      "posdE_01xy:\n",
      "Shape: (10000, 384, 384, 2)\n",
      "Data type: float16\n",
      "\n",
      "posdE_23x:\n",
      "Shape: (10000, 384, 2)\n",
      "Data type: float16\n",
      "\n",
      "posdE_23y:\n",
      "Shape: (10000, 384, 2)\n",
      "Data type: float16\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file\n",
    "with h5py.File(src_fname, 'r') as file:\n",
    "    # List the datasets present in the file\n",
    "    print(list(file.keys()))\n",
    "\n",
    "    # Display the contents of each dataset\n",
    "    for name in file.keys():\n",
    "        data = file[name]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(\"Shape:\", data.shape)\n",
    "        print(\"Data type:\", data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e1ba1-dfac-4077-8fa7-d2b04a67d0bb",
   "metadata": {},
   "source": [
    "### **1. Creating a TFRecord Example**\r\n",
    "\r\n",
    "The `create_tfrecord_example` function generates a TFRecord example from raw data. Each example consists of a series of **features** mapped to the following fields:\r\n",
    "\r\n",
    "- **`posdE_01xy`**, **`posdE_23x`**, **`posdE_23y`**:\r\n",
    "  - Arrays of type float representing the main data.\r\n",
    "  - These arrays are flattened into a list using `.reshape(-1)`.\r\n",
    "\r\n",
    "- **`dE`**:\r\n",
    "  - A one-dimensional float array derived from the original matrix.\r\n",
    "  - The extra dimension is removed using `[:, 0]`.\r\n",
    "\r\n",
    "- **`label`**:\r\n",
    "  - An integer label (of type `int`) representing the class associated with the data.\r\n",
    "\r\n",
    "These data elements are combined into a `tf.train.Example` object, which represents a single serializable example for the TFRecord format.\r\n",
    "eatures(feature=features))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffbbadf-96f2-418f-92e7-0c65426b9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TFRecord example\n",
    "def create_tfrecord_example(posdE_01xy, posdE_23x, posdE_23y, dE, label):\n",
    "    features = {\n",
    "        \"posdE_01xy\": tf.train.Feature(float_list=tf.train.FloatList(value=posdE_01xy.reshape(-1))),\n",
    "        \"posdE_23x\": tf.train.Feature(float_list=tf.train.FloatList(value=posdE_23x.reshape(-1))),\n",
    "        \"posdE_23y\": tf.train.Feature(float_list=tf.train.FloatList(value=posdE_23y.reshape(-1))),\n",
    "        \"dE\": tf.train.Feature(float_list=tf.train.FloatList(value=dE.reshape(-1))),\n",
    "        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ee81e-3464-4cda-a8a3-9f820e8678e5",
   "metadata": {},
   "source": [
    "# Function to Write a Batch of Examples to a TFRecord File\r\n",
    "\r\n",
    "## Introduction\r\n",
    "This function reads a set of data from an HDF5 (`.h5`) file and writes it to a TFRecord file, a format optimized for processing data in TensorFlow.\r\n",
    "\r\n",
    "---\r\n",
    "anzamento\r\n",
    "            progress_queue.put(1)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b13e9b-6673-48a5-a8dd-8dd4b662a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write a batch of examples to a TFRecord file with progress bar updates\n",
    "def write_tfrecord_batch(h5_file_path, indices, tfrecord_file_path, progress_queue):\n",
    "    \"\"\"\n",
    "    Writes a batch of examples from an HDF5 file to a TFRecord file.\n",
    "\n",
    "    Args:\n",
    "        h5_file_path (str): Path to the source HDF5 file.\n",
    "        indices (list): List of indices of the data to write.\n",
    "        tfrecord_file_path (str): Path to the destination TFRecord file.\n",
    "        progress_queue (Queue): Queue to update the progress bar.\n",
    "\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_file_path, \"r\") as f, tf.io.TFRecordWriter(tfrecord_file_path) as writer:\n",
    "        for i in indices:\n",
    "            posdE_01xy = f[\"posdE_01xy\"][i].astype(\"float32\")\n",
    "            posdE_23x = f[\"posdE_23x\"][i].astype(\"float32\")\n",
    "            posdE_23y = f[\"posdE_23y\"][i].astype(\"float32\")\n",
    "            dE = f[\"dE\"][i, :, 0].astype(\"float32\")  # Removes the extra dimension\n",
    "            label = int(f[\"ID\"][i])\n",
    "            \n",
    "            example = create_tfrecord_example(posdE_01xy, posdE_23x, posdE_23y, dE, label)\n",
    "            writer.write(example.SerializeToString())\n",
    "            progress_queue.put(1)  # Updates the progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1e173-5b7f-45bb-899b-4091804f5cbc",
   "metadata": {},
   "source": [
    "# Main Function to Split the Dataset and Write in Parallel with a Progress Bar\r\n",
    "\r\n",
    "## Introduction\r\n",
    "The `split_and_parallel_write` function is designed to:\r\n",
    "1. **Split an HDF5 dataset** into two sets: one for training and one for validation.\r\n",
    "2. Write the split data into TFRecord files in **parallel** using multiple processes.\r\n",
    "3. Update a **progress bar** to monitor the status of the process\r\n",
    "\r\n",
    "---\r\n",
    "zamento\r\n",
    "    progress_process.close()\r\n",
    "    progress_process.join()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3923d9b-b24e-4568-8714-b7b6d0733a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display progress\n",
    "def progress_listener(total_samples, progress_queue):\n",
    "    with tqdm(total=total_samples, desc=\"Converting to TFRecord\") as pbar:\n",
    "        for _ in range(total_samples):\n",
    "            progress_queue.get()\n",
    "            pbar.update(1)\n",
    "\n",
    "# Main function to split the dataset and write in parallel with a progress bar\n",
    "def split_and_parallel_write(h5_file_path, train_tfrecord_path, val_tfrecord_path, train_ratio=0.8, num_processes=4):\n",
    "    \"\"\"\n",
    "    Splits the dataset and writes the data into TFRecord files in parallel.\n",
    "\n",
    "    Args:\n",
    "        h5_file_path (str): Path to the source HDF5 file.\n",
    "        train_tfrecord_path (str): Base path for training TFRecord files.\n",
    "        val_tfrecord_path (str): Base path for validation TFRecord files.\n",
    "        train_ratio (float): Proportion of the data to use for training (default 0.8).\n",
    "        num_processes (int): Number of processes for parallelization (default 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    with h5py.File(h5_file_path, \"r\") as f:\n",
    "        n_samples = f[\"ID\"].shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=1 - train_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "    # Split the indices into batches for parallel processing\n",
    "    train_batches = np.array_split(train_indices, num_processes)\n",
    "    val_batches = np.array_split(val_indices, num_processes)\n",
    "\n",
    "    # Manager for the progress bar using multiprocessing\n",
    "    manager = Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "\n",
    "    # Start the progress bar process\n",
    "    total_samples = len(train_indices) + len(val_indices)\n",
    "    progress_process = Pool(1, progress_listener, (total_samples, progress_queue))\n",
    "\n",
    "    # Parallel writing of TFRecord files\n",
    "    with Pool(num_processes) as pool:\n",
    "        pool.starmap(write_tfrecord_batch, [(h5_file_path, batch, f\"{train_tfrecord_path}_part{i}\", progress_queue) for i, batch in enumerate(train_batches)])\n",
    "        pool.starmap(write_tfrecord_batch, [(h5_file_path, batch, f\"{val_tfrecord_path}_part{i}\", progress_queue) for i, batch in enumerate(val_batches)])\n",
    "\n",
    "    # Close the progress bar process\n",
    "    progress_process.close()\n",
    "    progress_process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d829ef-c83e-4183-b8a0-6afd4e2f3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to TFRecord: 100%|██████████| 10000/10000 [01:47<00:00, 92.62it/s]\n"
     ]
    }
   ],
   "source": [
    "exp_dirname = f\"{PATH}/Train_and_Validation\"\n",
    "\n",
    "if not os.path.exists(exp_dirname):\n",
    "    os.makedirs(exp_dirname)\n",
    "else:\n",
    "    shutil.rmtree(exp_dirname)\n",
    "    os.makedirs(exp_dirname)\n",
    "\n",
    "# Run conversion using multiple processes with progress bar\n",
    "split_and_parallel_write(src_fname, f\"{exp_dirname}/train.tfrecord\", f\"{exp_dirname}/validation.tfrecord\", num_processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8d8a68-6da5-4810-8404-793ba0157e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 16:00:12.658829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3234 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB MIG 1g.5gb, pci bus id: 0000:e1:00.0, compute capability: 8.0\n",
      "Concatenating /tmp/lhcf-cnn/train.tfrecord: 100%|██████████| 8000/8000 [00:14<00:00, 566.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated TFRecord file saved: /tmp/lhcf-cnn/train.tfrecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating /tmp/lhcf-cnn/validation.tfrecord: 100%|██████████| 2000/2000 [00:03<00:00, 517.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated TFRecord file saved: /tmp/lhcf-cnn/validation.tfrecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to concatenate TFRecord files with a progress bar\n",
    "def concatenate_tfrecords(input_files, output_file):\n",
    "    total_records = sum(1 for input_file in input_files for _ in tf.data.TFRecordDataset(input_file))\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        with tqdm(total=total_records, desc=f\"Concatenating {output_file}\") as pbar:\n",
    "            for input_file in input_files:\n",
    "                for record in tf.data.TFRecordDataset(input_file):\n",
    "                    writer.write(record.numpy())\n",
    "                    pbar.update(1)\n",
    "    print(f\"Concatenated TFRecord file saved: {output_file}\")\n",
    "\n",
    "# Get the list of train and validation TFRecord files\n",
    "train_files = sorted([os.path.join(exp_dirname, f) for f in os.listdir(exp_dirname) if f.startswith(\"train.tfrecord_part\")])\n",
    "validation_files = sorted([os.path.join(exp_dirname, f) for f in os.listdir(exp_dirname) if f.startswith(\"validation.tfrecord_part\")])\n",
    "\n",
    "# Concatenate files into a single TFRecord for train and validation with a progress bar\n",
    "concatenate_tfrecords(train_files, f\"{PATH}/train.tfrecord\")\n",
    "concatenate_tfrecords(validation_files, f\"{PATH}/validation.tfrecord\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-k2",
   "language": "python",
   "name": "cnn-k2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
