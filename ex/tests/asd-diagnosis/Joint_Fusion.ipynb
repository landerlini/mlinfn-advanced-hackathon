{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2445673-9e2d-4ca1-9bea-df97bf76e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=/tmp/asd-diagnosis\n",
    "\n",
    "if [ ! -d $DATA_DIR ]; then\n",
    "  mkdir -p $DATA_DIR\n",
    "fi\n",
    "\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_structural_features.csv -O $DATA_DIR/Harmonized_structural_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_functional_features.csv -O $DATA_DIR/Harmonized_functional_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/dict.csv -O $DATA_DIR/dict.csv &> .log\n",
    "\n",
    "ls -lrth $DATA_DIR/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63d352-6713-4c2a-b1ca-5d7cc86efb6f",
   "metadata": {},
   "source": [
    "# Autism Spectrum Disorders (ASD) diagnosis combining structural and functional Magnetic Resonance Imaging and RadiomicsÂ¶\n",
    "\n",
    "For an introduction to Autism Spectrum Disorders (ASD) and to the dataset of this ML example please refer to the [first notebook](sMRI_fMRI_sep.ipynb). In that notebook structural and functional MRI features datasets were used separately. While here we are going to combine those datasets and evaluate if the ML model predictions have improved. \n",
    "\n",
    "## Usefull libraries to import\n",
    "\n",
    "Here, you will have to read the data and pre-process them. First you need to import the necessary libraries and the apply a scaler to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794072c-9064-4023-9140-52f99640a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import warnings\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3f6bf-297b-4cfe-95b9-d8c50ce1eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22096a80-4167-41d2-ad44-7911a7980e89",
   "metadata": {},
   "source": [
    "## Callback\n",
    "\n",
    "A callback is an action passed to the model fit function which is performend while training the neural network. These actions allow you to modify certain parameters when a specific condition is met while training. These actions may be implemented before or after an epoch or batch is processed, when training begins or ends and when a specific condition is met. Callbacks can help with tasks like saving the model, reducing learning rates, stopping training early, or logging performance metrics. Here we implement two actions:\n",
    "\n",
    "* EarlyStopping\n",
    "* ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648fd44-f3a1-4378-a8eb-39832415cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332bff4-711d-4293-8dea-eb0fea2d9809",
   "metadata": {},
   "source": [
    "# Import dataframe and normalize with RobustScaler: DF_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28901f7b-ffd4-4dd7-8896-c04ca98d6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_data = '/tmp/asd-diagnosis/'\n",
    "\n",
    "DF_struct = pd.read_csv(os.path.join(path_to_data,'Harmonized_structural_features.csv'))\n",
    "DF_funct  = pd.read_csv(os.path.join(path_to_data,'Harmonized_functional_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740feb8-8dc6-430d-9a64-a6d0e7a695a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_struct = DF_struct.set_index('FILE_ID')\n",
    "DF_funct = DF_funct.set_index('FILE_ID')\n",
    "DF_funct= DF_funct.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'DX_GROUP'], axis =1)\n",
    "DF_merge = DF_struct.join(DF_funct,how='inner')\n",
    "DF_merge = DF_merge.reset_index()\n",
    "DF_merge.loc[DF_merge.DX_GROUP == -1, 'DX_GROUP'] = 0\n",
    "DF_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60010b0c-4d8e-4ce4-84c0-3b6a390c9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_normalized = DF_merge.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'FILE_ID'], axis =1) #questa riga lasciamola che vanno tolte le feature ridondanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6172e-3fa8-4a45-bb2a-dd9e7da5812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#normalize data\n",
    "RS_instance = RobustScaler()\n",
    "DF_normalized.iloc[:,1:] = RS_instance.fit_transform(DF_normalized.iloc[:,1:].to_numpy())\n",
    "DF_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845905e4-a41e-4b2f-bd51-82c6beb6b456",
   "metadata": {},
   "source": [
    "# ASD joint structural and functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c8269-9235-4865-bcf8-1bce8819c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.models import Model #Sequential, serve ?\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73748f61-62bc-4754-8a90-52d912842a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_model():\n",
    "    \"\"\" This function returns a model ...\n",
    "    \"\"\"\n",
    "    input_data= Input(221,)\n",
    "    x = Dense(32,  'relu', kernel_initializer='normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    return Model(inputs=input_data, outputs=x)\n",
    "\n",
    "\n",
    "def functional_model():\n",
    "    \"\"\" This function returns a model ...\n",
    "    \"\"\"\n",
    "    input_data= Input(5253,)\n",
    "    x = Dense(64, 'relu', kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    return Model(inputs=input_data, outputs=x)\n",
    "\n",
    "\n",
    "def joint_model(model_1, model_2):\n",
    "    \"\"\" This function combines the output of two keras models\n",
    "    model_1\n",
    "    model_2\n",
    "\n",
    "    returns: a new model which combines model_1 and model_2 by adding 3 dense layers\n",
    "    \"\"\"\n",
    "    combined = Concatenate(axis=-1)([model_1.output, model_2.output])\n",
    "    z = Dense(16, 'relu',  kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(combined)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    z = Dense(8, 'relu',  kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "    z = Dense(1, 'sigmoid')(z) #, kernel_regularizer=l1(0.01))(z)\n",
    "    return Model(inputs=[model_1.input, model_2.input], outputs = z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937403b2-e935-4efd-a92d-334e29bf1228",
   "metadata": {},
   "source": [
    "**Descrivere il training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a8e3e-dc7f-49a1-992c-641b03088e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_normalized.head()  #il dataframe deve contenere sia le features strutturali che funzionali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8c029-f4f7-4a3f-8933-3682ddbbe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "AUC = []\n",
    "shap_values_per_cv_s =[]\n",
    "shap_values_per_cv_f =[]\n",
    "var_f = []\n",
    "var_s = []\n",
    "np.random.seed(1) # Reproducibility\n",
    "n_cv = 2 #NUMERO DI KFOLD\n",
    "rs_=13 \n",
    "tprs = []\n",
    "aucs = []\n",
    "interp_fpr = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2199b2-9049-47b7-a6a6-44e2668499fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD #stochastic gradient descent\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# import sklearn \n",
    "from sklearn.model_selection import StratifiedKFold #train_test_split, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import shap # colpa sua \"l'errore\" qui sotto: /envs/ai4ni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cfb1d-0688-4b05-9fa9-e5ca4fb6468c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=rs_) # Set random state\n",
    "\n",
    "for train_index, test_index in cv.split(DF_normalized.iloc[:, 1:], DF_normalized.iloc[:, 0]):\n",
    "    start = time.time()\n",
    "    df_train, df_val = DF_normalized.iloc[train_index, :], DF_normalized.iloc[test_index, :]\n",
    "\n",
    "    X_train_struct, X_test_struct = df_train.iloc[:, 1:222], df_val.iloc[:, 1:222]\n",
    "    X_train_funct, X_test_funct = df_train.iloc[:, 222:], df_val.iloc[:, 222:]\n",
    "    y_train, y_test = df_train.iloc[:, 0], df_val.iloc[:, 0]\n",
    "\n",
    "        #load the model\n",
    "    clear_session()\n",
    "    mod_1 = structural_model\n",
    "    mod_2 = functional_model\n",
    "    model_joint = joint_model(mod_1(), mod_2())\n",
    "\n",
    "        # Compile the model\n",
    "    model_joint.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Fit data to model\n",
    "    history = model_joint.fit((X_train_struct, X_train_funct), y_train,\n",
    "                          batch_size=64,\n",
    "                          epochs=EPOCH,\n",
    "                          verbose=0,\n",
    "                          validation_data = ((X_test_struct, X_test_funct), y_test),\n",
    "                          callbacks=[reduce_on_plateau]\n",
    "                          )\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(1, len(acc)+1)\n",
    "    \n",
    "    #Train and validation accuracy\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training ')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation ')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Accuracy')\n",
    "    #Train and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training ')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation ')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(' Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare for SHAP\n",
    "    X_train_struct_SHAP = X_train_struct.to_numpy()\n",
    "    X_test_struct_SHAP = X_test_struct.to_numpy()\n",
    "    X_train_funct_SHAP = X_train_funct.to_numpy()\n",
    "    X_test_funct_SHAP = X_test_funct.to_numpy()\n",
    "\n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.GradientExplainer(model_joint, [X_train_struct_SHAP, X_train_funct_SHAP])\n",
    "    shap_values = explainer.shap_values([X_test_struct_SHAP, X_test_funct_SHAP])\n",
    "    # nsamples maggario permettono un calcolo piÃ¹ accurato - rappresenta il numero di perturbazioni 200 def nsamples = 200\n",
    "    #print(shap_values)\n",
    "    #  SHAP information per fold\n",
    "    shap_values_per_cv_s.append(shap_values[0]) #  221 features, appendo un array di dimensione 138/139,  221, 1\n",
    "    shap_values_per_cv_f.append(shap_values[1]) # 5253 features, appendo un array di dimensione 138/139, 5253, 1\n",
    "\n",
    "    print(\"len(shap_values[0]) \",len(shap_values[0]))\n",
    "    print(\"shap_values[0].shape \",shap_values[0].shape)\n",
    "    print(\"len(shap_values[1]) \",len(shap_values[1]))\n",
    "    print(\"shap_values[1].shape \",shap_values[1].shape)\n",
    "\n",
    "    #print(X_test_struct.shape)\n",
    "        ###########################################################\n",
    "    _, val_acc = model_joint.evaluate((X_test_struct, X_test_funct), y_test, verbose=0)\n",
    "    acc.append(val_acc)\n",
    "\n",
    "        #Compute Receiver operating characteristic (ROC)\n",
    "    i=0\n",
    "    preds = model_joint.predict((X_test_struct, X_test_funct), verbose=1)\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    interp_tpr = np.interp(interp_fpr, fpr, tpr)\n",
    "    tprs.append(interp_tpr)\n",
    "    AUC.append(roc_auc)\n",
    "    i += 1\n",
    "    print('---------------------AUC------------------', roc_auc)\n",
    "    end = time.time()\n",
    "    print('----------------------T-------------------', end - start)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a880c14-639e-4bda-8c5f-ddd796d60743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "      label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(interp_fpr, mean_tpr)\n",
    "std_auc = np.std(AUC)\n",
    "plt.plot(interp_fpr, mean_tpr, color='b',\n",
    "        label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})',\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(interp_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Joint Fusion model',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16c351-b7c8-4008-b64a-392cc6f550ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print(len(AUC))\n",
    "print(f'AUC:{np.mean(AUC)} (+- {np.std(AUC)})')\n",
    "print(f'accuracy: {np.mean(acc)} (+- {np.std(acc)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bae4c-1a37-48d6-9894-396372b86aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish lists to keep average Shap values\n",
    "average_shap_values_s = []\n",
    "average_shap_values_f = []\n",
    "\n",
    "for i in range(0, len(AUC)):\n",
    "    df_per_obs = shap_values_per_cv_s[i].copy()\n",
    "    df_per_obs = np.absolute(df_per_obs)\n",
    "    average_shap_values_s.append(df_per_obs.mean(axis=0))\n",
    "\n",
    "    df_per_f = shap_values_per_cv_f[i].copy()\n",
    "    df_per_f = np.absolute(df_per_f)\n",
    "    average_shap_values_f.append(df_per_f.mean(axis=0))\n",
    "\n",
    "fold_s = np.transpose(np.array(average_shap_values_s)[...,0])\n",
    "fold_f = np.transpose(np.array(average_shap_values_f)[...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878cca-77ee-4027-bc43-a72023081c3c",
   "metadata": {},
   "source": [
    "# SHAP values Joint model: NORMALIZATION SHAP VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770088e1-d24b-425d-a6fa-dac767ae79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_s = pd.DataFrame.from_dict(fold_s)\n",
    "fold_f = pd.DataFrame.from_dict(fold_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cca31-3b63-4a6b-b21e-06d382b62efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_s = fold_s.iloc[:,1:]\n",
    "fold_f = fold_f.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4493d-e29f-4431-8152-cba82c2a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 221/(2*(221 + 5253))\n",
    "f = 5253/(2*(221 + 5253))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664963f-d6a5-44ec-9401-c36fed3cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMNALIZZAZIONE STRUTTURALE\n",
    "\n",
    "fold_s_n = (fold_s/fold_s.sum(axis=0))*s*100\n",
    "plot = fold_s_n.mean(axis=1).values\n",
    "strutt = pd.DataFrame(plot, index = DF_normalized.iloc[:, 1:222].columns )\n",
    "strutt['std']=fold_s_n.std(axis=1).values\n",
    "strutt = strutt.sort_values(by=0, ascending=False)\n",
    "strutt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84bf97-482a-457c-920d-ad0fe126409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMNALIZZAZIONE FUNZIONALE\n",
    "\n",
    "norm = fold_f.sum(axis=0)\n",
    "fold_f_n = (fold_f/fold_f.sum(axis=0))*f*100\n",
    "plot_f = fold_f_n.mean(axis=1).values\n",
    "func = pd.DataFrame(plot_f, index = DF_normalized.iloc[:, 222:].columns )\n",
    "func['std']=fold_f_n.std(axis=1).values\n",
    "func = func.sort_values(by=0, ascending=False)\n",
    "'''func = func.iloc[:35, :]\n",
    "func.to_excel(path+'risultati_funzionali.xlsx')'''\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6eb20-73c2-4519-a3a2-ad22df9a33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non capisco perche' si vada a leggere un xlsx\n",
    "\n",
    "#func =pd.read_excel(path+'risultati_funzionali.xlsx')\n",
    "#func['name'] = func['r1'].astype(str) + ' - ' + func['r2'].astype(str)\n",
    "\n",
    "#del func['r1']\n",
    "#del func['r2']\n",
    "#del func['Unnamed: 0']\n",
    "#func = func.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8532d9-16d7-4cd4-bba1-7fcfaf11237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_m_s =  pd.concat([strutt,func])\n",
    "all_m_s = all_m_s.sort_values(by=0, ascending=False)\n",
    "all_m_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630eea8-397e-442d-8e0e-390a053b2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "th99 = all_m_s.iloc[:, 0].quantile(0.995)\n",
    "th99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212994f-d836-4c74-8569-f1edb3b8b9f2",
   "metadata": {},
   "source": [
    "## 99-95 esimo percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44cc1c-e9c2-428b-b3ad-9c5af7a80d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_m_s[all_m_s[0] >= th99][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5217bcc-82b6-4ea9-8bc8-e128c4b707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = all_m_s[all_m_s[0] >= th99][0].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP value|)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e5b24-b062-4dee-b880-0482a7e25f3b",
   "metadata": {},
   "source": [
    "## Cohen d coeff\n",
    "\n",
    "The Cohen's *d* coefficient is a statistical measure used to quantify the *effect size* between two groups, indicating the standardized difference between their means. It is commonly used in psychology, social sciences, and other fields to assess the difference between two sample groups.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The formula for Cohenâs *d* is:\n",
    "$$\n",
    "d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s}\n",
    "$$\n",
    "where: $\\bar{X}_1$ and $\\bar{X}_2$ are the means of the two groups. $s$ is the pooled standard deviation of the two groups, calculated as:\n",
    "$$\n",
    "s = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n",
    "$$\n",
    "where: $n_1$ and $n_2$ are the sample sizes of the two groups. $s_1$ and $s_2$ are the standard deviations of the two groups.\n",
    "\n",
    "### Interpretation\n",
    "Cohen's *d* provides a way to interpret the magnitude of the difference, regardless of the scale of the data, making it easier to compare across studies. Common interpretations are:\n",
    "- **0.2** - Small effect size\n",
    "- **0.5** - Medium effect size\n",
    "- **0.8** or higher - Large effect size\n",
    "\n",
    "These are general guidelines, and the interpretation can vary by field. Cohen's *d* is particularly helpful because it puts the difference in a standardized context, allowing researchers to understand the size of an effect without being influenced by sample size alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b93664-c70f-402f-bfb0-da55a7c17740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohen_d(g1, g2, f):\n",
    "    \"\"\"Function to compute the Cohen's d Coefficient.\n",
    "\n",
    "    g1: infered results by predictor 1\n",
    "    g2: infered results by predictor 2\n",
    "    f:  predicted class ?\n",
    "\n",
    "    it returns the 'd' value of agreement\n",
    "    \"\"\"\n",
    "    n1 = len(g1)                   # number of data in g1\n",
    "    n2 = len(g2)                   # number of data in g2\n",
    "    N = n1 + n2                    # total number of data\n",
    "    Scores1 = g1[f].dropna()       # remove nan results\n",
    "    Scores2 = g2[f].dropna()\n",
    "    var1 = Scores1.var()           # compute the variance over the dataset\n",
    "    var2 = Scores2.var()\n",
    "    mean1 = Scores1.mean()         # compute the mean result\n",
    "    mean2 = Scores2.mean()\n",
    "    sp = (((n1 - 1)*var1 + (n2 - 1)*var2) / (N - 2))**0.5  # A me sembra tanto un confronto tra gaussiane\n",
    "    d = (mean1 - mean2) / sp\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c98ddd-2c58-420b-ac3d-b8a55e9f3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = DF_normalized[DF_normalized.DX_GROUP==0]\n",
    "ASD =  DF_normalized[DF_normalized.DX_GROUP==1]\n",
    "list_f = all_m_s.iloc[:].index.tolist()\n",
    "score_df = []\n",
    "for item in list_f:\n",
    "    score =  Cohen_d(ASD, controls, item)\n",
    "    score_df.append(score)\n",
    "    #print(item, score)\n",
    "all_m_s['cohen']=score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754b9b9-ce6c-4a34-94ff-00f045dd9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_m_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5f0f5-9fea-423f-8b58-009997990514",
   "metadata": {},
   "source": [
    "## Find region's coord in HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4914a9-f908-4eac-a3fd-3afc47e66b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, plotting\n",
    "\n",
    "path_to_data = '/tmp/asd-diagnosis/'\n",
    "dict_f = pd.read_csv(path_to_data + 'dict.csv')\n",
    "df2 = pd.read_csv(path_to_data + 'functional_features.csv')\n",
    "df2 = df2.set_index('F')\n",
    "\n",
    "perc_95 = dict_f['Unnamed: 0'][0:25]\n",
    "perc_95\n",
    "lista_con = df2.loc[perc_95]\n",
    "lista_con\n",
    "all_m_s.iloc[:28, :]\n",
    "'''#features ASD>TD\n",
    "lista_conn = lista_con.drop(731)\n",
    "lista_conn = lista_conn.drop(3254)\n",
    "lista_conn = lista_conn.drop(3295)\n",
    "\n",
    "lista_conn = lista_conn.drop(3004)\n",
    "lista_conn = lista_conn.drop(1737)\n",
    "lista_conn = lista_conn.drop(4545)\n",
    "lista_conn = lista_conn.drop(4935)\n",
    "lista_conn = lista_conn.drop(1455)\n",
    "lista_conn = lista_conn.drop(395)'''\n",
    "\n",
    "\n",
    "#features ASD>TD\n",
    "lista_conn = lista_con.loc[[731, 3254, 3295, 3004, 1737, 4545, 4935, 1455, 395]]\n",
    "lista_conn\n",
    "l1 = lista_conn['r1'].to_list()\n",
    "l2 = lista_conn['r2'].to_list()\n",
    "l = l1 + l2\n",
    "sam_list = list(set(l))\n",
    "sam_list\n",
    "len(sam_list)\n",
    "region_c = []\n",
    "label_c = []\n",
    "#load HO atlas\n",
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')  #sub-maxprob-thr50-2mm - cortl-maxprob-thr25-2mm\n",
    "atlas_file = atlas_ho.maps\n",
    "\n",
    "# Load labels for each atlas region\n",
    "labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)\n",
    "print(len(labels))\n",
    "\n",
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('cortl-maxprob-thr25-2mm')\n",
    "atlas_file = atlas_ho.maps\n",
    "# Load labels for each atlas region\n",
    "labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)\n",
    "print(len(labels))\n",
    "\n",
    "for i, e in enumerate(labels):\n",
    "  for j, n in enumerate(sam_list):\n",
    "    if e == n:\n",
    "      region_c.append(coordinates[i])\n",
    "      label_c.append(e)\n",
    "\n",
    "len(label_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e9109-f368-4f51-b8a4-3cfcf8d91a93",
   "metadata": {},
   "source": [
    "## PLOT connectoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62298c8-e7d9-4800-89e0-f57dd3b49ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "\n",
    "#print(\"lista_conn\")\n",
    "#print(lista_conn)\n",
    "#print(\"\")\n",
    "\n",
    "for i in range(0, len(label_c)):\n",
    "    print(\"i, region_c[i],label_c[i]\")\n",
    "    print(i, region_c[i],label_c[i])\n",
    "\n",
    "index_1 = []\n",
    "for j, n in enumerate(l1):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_1.append(i)\n",
    "\n",
    "index_2=[]\n",
    "for j, n in enumerate(l2):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_2.append(i)\n",
    "\n",
    "print(\"\")\n",
    "print(\"lista_conn['i1']\",lista_conn['r1'])\n",
    "print(\"lista_conn['i2']\",lista_conn['r2'])\n",
    "\n",
    "#lista_conn['r1'] = index_1\n",
    "#lista_conn['r2'] = index_2\n",
    "#lista_conn\n",
    "\n",
    "mat =np.zeros((len(region_c), len(region_c)))\n",
    "\n",
    "for index, (value1, value2) in enumerate(zip(index_1, index_2)):\n",
    "    #print(index, value1 , value2)\n",
    "    mat[value1][value2] = 1\n",
    "mat\n",
    "\n",
    "mat = mat + mat.T\n",
    "coordinates = np.array(region_c)  # 3D coordinates of parcels\n",
    "\n",
    "color_dict = {}\n",
    "cmap = cm.get_cmap('tab20')\n",
    "\n",
    "for rsn, c in zip(label_c, cmap.colors):\n",
    "    color_dict[rsn] = tuple(c)\n",
    "\n",
    "\n",
    "\n",
    "node_color = []\n",
    "for nw in label_c:\n",
    "    node_color.append(color_dict[nw])\n",
    "\n",
    "coords = coordinates\n",
    "\n",
    "N = len(coords)\n",
    "\n",
    "plotting.plot_connectome(mat,\n",
    "                         coords,\n",
    "                         title='ASD>TD',\n",
    "                         node_color=node_color,\n",
    "                          display_mode=\"lyrz\",\n",
    "                         edge_kwargs = {\"linewidth\":1.7, \"color\": 'red'})\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = []\n",
    "for k,v in color_dict.items():\n",
    "    legend_elements.append(Line2D([0], [0], marker='o', color=v, label=k,\n",
    "                          markerfacecolor=v, markersize=5))\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.legend(handles=legend_elements, loc='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4ni",
   "language": "python",
   "name": "ai4ni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
