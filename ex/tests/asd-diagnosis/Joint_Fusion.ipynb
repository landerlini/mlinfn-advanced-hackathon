{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2445673-9e2d-4ca1-9bea-df97bf76e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=/tmp/asd-diagnosis\n",
    "\n",
    "if [ ! -d $DATA_DIR ]; then\n",
    "  mkdir -p $DATA_DIR\n",
    "fi\n",
    "\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_structural_features.csv -O $DATA_DIR/Harmonized_structural_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_functional_features.csv -O $DATA_DIR/Harmonized_functional_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/dict.csv -O $DATA_DIR/dict.csv &> .log\n",
    "\n",
    "ls -lrth $DATA_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794072c-9064-4023-9140-52f99640a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import warnings\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05706335-7d5b-4012-8154-f07c9f9eb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CV = 2\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63d352-6713-4c2a-b1ca-5d7cc86efb6f",
   "metadata": {},
   "source": [
    "# Autism Spectrum Disorders (ASD) diagnosis combining structural and functional Magnetic Resonance Imaging and RadiomicsÂ¶\n",
    "\n",
    "For an introduction to Autism Spectrum Disorders (ASD) and to the dataset of this ML example please refer to the [first notebook](sMRI_fMRI_sep.ipynb). In that notebook structural and functional MRI features datasets were used separately. While here we are going to combine those datasets and evaluate if the ML model predictions have improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332bff4-711d-4293-8dea-eb0fea2d9809",
   "metadata": {},
   "source": [
    "# Import dataframe and normalize with RobustScaler: DF_normalized\n",
    "Pre-processing is the same as the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28901f7b-ffd4-4dd7-8896-c04ca98d6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_data = '/tmp/asd-diagnosis/'\n",
    "\n",
    "DF_struct = pd.read_csv(os.path.join(path_to_data,'Harmonized_structural_features.csv'))\n",
    "DF_funct  = pd.read_csv(os.path.join(path_to_data,'Harmonized_functional_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740feb8-8dc6-430d-9a64-a6d0e7a695a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_struct = DF_struct.set_index('FILE_ID')\n",
    "DF_funct = DF_funct.set_index('FILE_ID')\n",
    "DF_funct= DF_funct.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'DX_GROUP'], axis =1)\n",
    "DF_merge = DF_struct.join(DF_funct,how='inner')\n",
    "DF_merge = DF_merge.reset_index()\n",
    "DF_merge.loc[DF_merge.DX_GROUP == -1, 'DX_GROUP'] = 0\n",
    "DF_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60010b0c-4d8e-4ce4-84c0-3b6a390c9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_normalized = DF_merge.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'FILE_ID'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6172e-3fa8-4a45-bb2a-dd9e7da5812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "RS_instance = RobustScaler()\n",
    "DF_normalized.iloc[:,1:] = RS_instance.fit_transform(DF_normalized.iloc[:,1:].to_numpy())\n",
    "DF_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d11d4-5039-4a48-a0e0-9e0a63a092c3",
   "metadata": {},
   "source": [
    "# ASD/TD classification using both structural and functional features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22096a80-4167-41d2-ad44-7911a7980e89",
   "metadata": {},
   "source": [
    "First, you have to define the model architecture. To implement a joint fusion approach you have to use as input both structural and functional features to 2 different neural networks.\n",
    "\n",
    "### HINT: you can copy the networks of the other notebook\n",
    "\n",
    "Then you have to find a way to merge these two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c8269-9235-4865-bcf8-1bce8819c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73748f61-62bc-4754-8a90-52d912842a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_model():\n",
    "    \"\"\" This function returns a model ...\n",
    "    \"\"\"\n",
    "    input_data= Input(221,)\n",
    "    x = Dense(32,  'relu', kernel_initializer='normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    return Model(inputs=input_data, outputs=x)\n",
    "\n",
    "\n",
    "def functional_model():\n",
    "    \"\"\" This function returns a model ...\n",
    "    \"\"\"\n",
    "    input_data= Input(5253,)\n",
    "    x = Dense(64, 'relu', kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    return Model(inputs=input_data, outputs=x)\n",
    "\n",
    "\n",
    "def joint_model(model_1, model_2):\n",
    "    \"\"\" This function combines the output of two keras models\n",
    "    model_1\n",
    "    model_2\n",
    "\n",
    "    returns: a new model which combines model_1 and model_2 by adding 3 dense layers\n",
    "    \"\"\"\n",
    "    combined = Concatenate(axis=-1)([model_1.output, model_2.output])\n",
    "    z = Dense(16, 'relu',  kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(combined)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    z = Dense(8, 'relu',  kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "    z = Dense(1, 'sigmoid')(z) #, kernel_regularizer=l1(0.01))(z)\n",
    "    return Model(inputs=[model_1.input, model_2.input], outputs = z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc01322-5fbf-483e-acea-22f5d47c3293",
   "metadata": {},
   "source": [
    "## Callback\n",
    "\n",
    "A callback is an action passed to the model fit function which is performend while training the neural network. These actions allow you to modify certain parameters when a specific condition is met while training. These actions may be implemented before or after an epoch or batch is processed, when training begins or ends and when a specific condition is met. Callbacks can help with tasks like saving the model, reducing learning rates, stopping training early, or logging performance metrics. Here we implement two actions:\n",
    "\n",
    "* EarlyStopping\n",
    "* ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648fd44-f3a1-4378-a8eb-39832415cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937403b2-e935-4efd-a92d-334e29bf1228",
   "metadata": {},
   "source": [
    "# Cross-validation setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8c029-f4f7-4a3f-8933-3682ddbbe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc = []\n",
    "AUC = []\n",
    "shap_values_per_cv_s =[]\n",
    "shap_values_per_cv_f =[]\n",
    "var_f = []\n",
    "var_s = []\n",
    "np.random.seed(1) # Reproducibility\n",
    "n_cv = N_CV\n",
    "rs_=13 \n",
    "tprs = []\n",
    "aucs = []\n",
    "interp_fpr = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2199b2-9049-47b7-a6a6-44e2668499fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD #stochastic gradient descent\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# import sklearn \n",
    "from sklearn.model_selection import StratifiedKFold #train_test_split, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cfb1d-0688-4b05-9fa9-e5ca4fb6468c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=rs_) # Set random state\n",
    "\n",
    "for train_index, test_index in cv.split(DF_normalized.iloc[:, 1:], DF_normalized.iloc[:, 0]):\n",
    "    start = time.time()\n",
    "    df_train, df_test = DF_normalized.iloc[train_index, :], DF_normalized.iloc[test_index, :]\n",
    "\n",
    "    X_train_struct, X_test_struct = df_train.iloc[:, 1:222], df_test.iloc[:, 1:222]\n",
    "    X_train_funct, X_test_funct = df_train.iloc[:, 222:], df_test.iloc[:, 222:]\n",
    "    y_train, y_test = df_train.iloc[:, 0], df_test.iloc[:, 0]\n",
    "\n",
    "        #load the model\n",
    "    clear_session()\n",
    "    mod_1 = structural_model\n",
    "    mod_2 = functional_model\n",
    "    model_joint = joint_model(mod_1(), mod_2())\n",
    "\n",
    "        # Compile the model\n",
    "    model_joint.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Fit data to model\n",
    "    history = model_joint.fit((X_train_struct, X_train_funct), y_train,\n",
    "                          batch_size=64,\n",
    "                          epochs=EPOCHS,\n",
    "                          verbose=0,\n",
    "                          validation_data = ((X_test_struct, X_test_funct), y_test),\n",
    "                          callbacks=[reduce_on_plateau]\n",
    "                          )\n",
    "    \n",
    "    #Train and validation accuracy\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training ')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation ')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Accuracy')\n",
    "    #Train and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training ')\n",
    "    plt.plot(history.history['val_loss'], label='Validation ')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(' Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare for SHAP\n",
    "    X_train_struct_SHAP = X_train_struct.to_numpy()\n",
    "    X_test_struct_SHAP = X_test_struct.to_numpy()\n",
    "    X_train_funct_SHAP = X_train_funct.to_numpy()\n",
    "    X_test_funct_SHAP = X_test_funct.to_numpy()\n",
    "\n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.GradientExplainer(model_joint, [X_train_struct_SHAP, X_train_funct_SHAP])\n",
    "    shap_values = explainer.shap_values([X_test_struct_SHAP, X_test_funct_SHAP])\n",
    "    \n",
    "    #  SHAP information per fold\n",
    "    shap_values_per_cv_s.append(shap_values[0]) #  221 features, append an array with dims 138/139,  221, 1\n",
    "    shap_values_per_cv_f.append(shap_values[1]) # 5253 features, append an array with dims 138/139, 5253, 1\n",
    "\n",
    "    print(\"len(shap_values[0]) \",len(shap_values[0]))\n",
    "    print(\"shap_values[0].shape \",shap_values[0].shape)\n",
    "    print(\"len(shap_values[1]) \",len(shap_values[1]))\n",
    "    print(\"shap_values[1].shape \",shap_values[1].shape)\n",
    "\n",
    "    #print(X_test_struct.shape)\n",
    "        ###########################################################\n",
    "    _, val_acc = model_joint.evaluate((X_test_struct, X_test_funct), y_test, verbose=0)\n",
    "    acc.append(val_acc)\n",
    "\n",
    "        #Compute Receiver operating characteristic (ROC)\n",
    "    i=0\n",
    "    preds = model_joint.predict((X_test_struct, X_test_funct), verbose=1)\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    interp_tpr = np.interp(interp_fpr, fpr, tpr)\n",
    "    tprs.append(interp_tpr)\n",
    "    AUC.append(roc_auc)\n",
    "    i += 1\n",
    "    print('---------------------AUC------------------', roc_auc)\n",
    "    end = time.time()\n",
    "    print('----------------------T-------------------', end - start)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a880c14-639e-4bda-8c5f-ddd796d60743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "      label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(interp_fpr, mean_tpr)\n",
    "std_auc = np.std(AUC)\n",
    "plt.plot(interp_fpr, mean_tpr, color='b',\n",
    "        label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})',\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(interp_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Joint Fusion model',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16c351-b7c8-4008-b64a-392cc6f550ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print(len(AUC))\n",
    "print(f'AUC:{np.mean(AUC):.4f} (+- {np.std(AUC):.4f})')\n",
    "print(f'accuracy: {np.mean(acc):.4f} (+- {np.std(acc):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bae4c-1a37-48d6-9894-396372b86aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish lists to keep average Shap values\n",
    "average_shap_values_s = []\n",
    "average_shap_values_f = []\n",
    "\n",
    "for i in range(0, len(AUC)):\n",
    "    df_per_obs = shap_values_per_cv_s[i].copy()\n",
    "    df_per_obs = np.absolute(df_per_obs)\n",
    "    average_shap_values_s.append(df_per_obs.mean(axis=0))\n",
    "\n",
    "    df_per_f = shap_values_per_cv_f[i].copy()\n",
    "    df_per_f = np.absolute(df_per_f)\n",
    "    average_shap_values_f.append(df_per_f.mean(axis=0))\n",
    "\n",
    "fold_s = np.transpose(np.array(average_shap_values_s)[...,0])\n",
    "fold_f = np.transpose(np.array(average_shap_values_f)[...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878cca-77ee-4027-bc43-a72023081c3c",
   "metadata": {},
   "source": [
    "# SHAP values Joint model: NORMALIZATION SHAP VALUES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3a531-e6ab-4ede-948a-58ad58e2068f",
   "metadata": {},
   "source": [
    "Here, we normalize SHAP values separatly for structural and functional model. The SHAP values for each cross-validation fold are processed as follows:\n",
    "\n",
    "1) They are transformed into a DataFrame\n",
    "2) Then the SHAP values are weighted and expressed in percentage, taking into account the fact that the number of functional features is higher with respect do the structural one. Without this weight the SHAP values of structural features would be very much higher with respect to the functional one.\n",
    "3) Finally, the SHAP values are concatenated and sorted according to their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770088e1-d24b-425d-a6fa-dac767ae79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_s = pd.DataFrame.from_dict(fold_s)\n",
    "fold_f = pd.DataFrame.from_dict(fold_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4493d-e29f-4431-8152-cba82c2a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the weighting factor\n",
    "s = 221/(2*(221 + 5253))\n",
    "f = 5253/(2*(221 + 5253))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664963f-d6a5-44ec-9401-c36fed3cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_s_n = (fold_s/fold_s.sum(axis=0))*s*100\n",
    "plot = fold_s_n.mean(axis=1).values\n",
    "struct_SHAP = pd.DataFrame({\"SHAP_values\": plot}, index = DF_normalized.iloc[:, 1:222].columns )\n",
    "struct_SHAP['std']=fold_s_n.std(axis=1).values\n",
    "struct_SHAP = struct_SHAP.sort_values(by=\"SHAP_values\", ascending=False)\n",
    "struct_SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84bf97-482a-457c-920d-ad0fe126409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = fold_f.sum(axis=0)\n",
    "fold_f_n = (fold_f/fold_f.sum(axis=0))*f*100\n",
    "plot_f = fold_f_n.mean(axis=1).values\n",
    "funct_SHAP = pd.DataFrame({\"SHAP_values\" : plot_f}, index = DF_normalized.iloc[:, 222:].columns )\n",
    "funct_SHAP['std']=fold_f_n.std(axis=1).values\n",
    "funct_SHAP = funct_SHAP.sort_values(by=\"SHAP_values\", ascending=False)\n",
    "funct_SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6eb20-73c2-4519-a3a2-ad22df9a33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_m_s =  pd.concat([struct_SHAP,funct_SHAP])\n",
    "all_for_SHAP = all_m_s.sort_values(by=\"SHAP_values\", ascending=False)\n",
    "all_for_SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d207c2-857d-488b-a5a6-4721b6a5b98d",
   "metadata": {},
   "source": [
    "As a check, the sum of all the SHAP values must be equal to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8532d9-16d7-4cd4-bba1-7fcfaf11237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_for_SHAP.sum(axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e03e7-4fdb-495e-a6e0-8bf7957d6f13",
   "metadata": {},
   "source": [
    "At this point, we have a dataframe that contains all the features from the most important to the less important. Now we select the scores above the 99th percentile of importance features selected by SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212994f-d836-4c74-8569-f1edb3b8b9f2",
   "metadata": {},
   "source": [
    "## Selection of 99th percentile of features importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2493d-9465-41d6-9ec3-a873fa15fded",
   "metadata": {},
   "source": [
    "Since we cannot plot the feature importance for all the features, we select only the ones with an importance over the 99% of the importance values in the dataset. We will define two variables: ine with all the sorted feature importance and one with only the functional features. This will help in the visualization of the functional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44cc1c-e9c2-428b-b3ad-9c5af7a80d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "th99 = all_for_SHAP.iloc[:, 0].quantile(0.99)\n",
    "th99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374025d-f710-4014-b4e3-7646d2eccc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SHAP_99 = all_for_SHAP[all_for_SHAP[\"SHAP_values\"] >= th99][\"SHAP_values\"]\n",
    "funct_SHAP_99 = funct_SHAP[funct_SHAP[\"SHAP_values\"] >= th99][\"SHAP_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e430b73-0e8c-44c9-b073-a2ba459ed954",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_funct_features_SHAP = funct_SHAP_99.index.astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5217bcc-82b6-4ea9-8bc8-e128c4b707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = all_for_SHAP[all_for_SHAP[\"SHAP_values\"] >= th99][\"SHAP_values\"].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP value|)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e5b24-b062-4dee-b880-0482a7e25f3b",
   "metadata": {},
   "source": [
    "## Cohen d coeff\n",
    "\n",
    "The Cohen's *d* coefficient is a statistical measure used to quantify the *effect size* between two groups, indicating the standardized difference between their means. It is commonly used in psychology, social sciences, and other fields to assess the difference between two sample groups.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The formula for Cohenâs *d* is:\n",
    "$$\n",
    "d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s}\n",
    "$$\n",
    "where: $\\bar{X}_1$ and $\\bar{X}_2$ are the means of the two groups. $s$ is the pooled standard deviation of the two groups, calculated as:\n",
    "$$\n",
    "s = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n",
    "$$\n",
    "where: $n_1$ and $n_2$ are the sample sizes of the two groups. $s_1$ and $s_2$ are the standard deviations of the two groups.\n",
    "\n",
    "### Interpretation\n",
    "Cohen's *d* provides a way to interpret the magnitude of the difference, regardless of the scale of the data, making it easier to compare across studies. Common interpretations are:\n",
    "- **0.2** - Small effect size\n",
    "- **0.5** - Medium effect size\n",
    "- **0.8** or higher - Large effect size\n",
    "\n",
    "These are general guidelines, and the interpretation can vary by field. Cohen's *d* is particularly helpful because it puts the difference in a standardized context, allowing researchers to understand the size of an effect without being influenced by sample size alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b93664-c70f-402f-bfb0-da55a7c17740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohen_d(g1, g2, f):\n",
    "    \"\"\"Function to compute the Cohen's d Coefficient.\n",
    "\n",
    "    g1: infered results by predictor 1\n",
    "    g2: infered results by predictor 2\n",
    "    f:  predicted class ?\n",
    "\n",
    "    it returns the 'd' value of agreement\n",
    "    \"\"\"\n",
    "    n1 = len(g1)                   # number of data in g1\n",
    "    n2 = len(g2)                   # number of data in g2\n",
    "    N = n1 + n2                    # total number of data\n",
    "    Scores1 = g1[f].dropna()       # remove nan results\n",
    "    Scores2 = g2[f].dropna()\n",
    "    var1 = Scores1.var()           # compute the variance over the dataset\n",
    "    var2 = Scores2.var()\n",
    "    mean1 = Scores1.mean()         # compute the mean result\n",
    "    mean2 = Scores2.mean()\n",
    "    sp = (((n1 - 1)*var1 + (n2 - 1)*var2) / (N - 2))**0.5\n",
    "    d = (mean1 - mean2) / sp\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c98ddd-2c58-420b-ac3d-b8a55e9f3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = DF_normalized[DF_normalized.DX_GROUP==0]\n",
    "ASD =  DF_normalized[DF_normalized.DX_GROUP==1]\n",
    "list_f = all_m_s.iloc[:].index.tolist()\n",
    "score_df = []\n",
    "for item in list_f:\n",
    "    score =  Cohen_d(ASD, controls, item)\n",
    "    score_df.append(score)\n",
    "    #print(item, score)\n",
    "all_m_s['cohen']=score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754b9b9-ce6c-4a34-94ff-00f045dd9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_all = all_m_s.drop(['SHAP_values','std'], axis = 'columns')\n",
    "cohen_funct = cohen_all[221:]\n",
    "cohen_sorted = cohen_funct.sort_values(by=\"cohen\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b539739-64fc-473b-86cb-275e23e3c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_funct_features_COHEN_ASD = cohen_sorted[:20].index.astype('int').tolist()\n",
    "important_funct_features_COHEN_TD = cohen_sorted[5233:].index.astype('int').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5f0f5-9fea-423f-8b58-009997990514",
   "metadata": {},
   "source": [
    "## Find region's coord in HO\n",
    "\n",
    "Since functional features are not easy to be associated with the real meaning, i. e. it is not easy to understand the parts of the brain that are used to compute that feature, here we go back from the functional features to some images that helps in understanding their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4914a9-f908-4eac-a3fd-3afc47e66b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, plotting\n",
    "\n",
    "functional_names = pd.read_csv(path_to_data + 'functional_features.csv')\n",
    "functional_names = functional_names.set_index('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d558b-1f6d-44cb-acd1-82be7abd8e06",
   "metadata": {},
   "source": [
    "Here, we reduce the nyumber of features to match the available color maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb869a2-7f26-4897-8395-3f0f7afed415",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_features = important_funct_features_SHAP[:13]\n",
    "perc_95 = functional_names.loc[reduction_features]\n",
    "connection_list = functional_names.loc[reduction_features]\n",
    "l1 = connection_list['r1'].to_list()\n",
    "l2 = connection_list['r2'].to_list()\n",
    "l = l1 + l2\n",
    "sam_list = list(set(l)) # the redundant feature names are deleted\n",
    "region_c = []\n",
    "label_c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d7102-d8fd-46b2-a390-b054c74ed3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load HO atlas\n",
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')  #sub-maxprob-thr50-2mm - cortl-maxprob-thr25-2mm\n",
    "atlas_file = atlas_ho.maps\n",
    "labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fef1f3-79ac-44fb-8a3c-5021edac003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for each atlas region\n",
    "\n",
    "print(len(labels))\n",
    "\n",
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('cortl-maxprob-thr25-2mm')\n",
    "atlas_file = atlas_ho.maps\n",
    "# Load labels for each atlas region\n",
    "atlas_labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)\n",
    "print(len(labels))\n",
    "\n",
    "for i, e in enumerate(atlas_labels):\n",
    "  for j, n in enumerate(sam_list):\n",
    "    if e == n:\n",
    "      region_c.append(coordinates[i])\n",
    "      label_c.append(e)\n",
    "\n",
    "len(label_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e9109-f368-4f51-b8a4-3cfcf8d91a93",
   "metadata": {},
   "source": [
    "## PLOT connectoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62298c8-e7d9-4800-89e0-f57dd3b49ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "for i in range(0, len(label_c)):\n",
    "    print(\"i, region_c[i],label_c[i]\")\n",
    "    print(i, region_c[i],label_c[i])\n",
    "\n",
    "index_1 = []\n",
    "for j, n in enumerate(l1):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_1.append(i)\n",
    "\n",
    "index_2=[]\n",
    "for j, n in enumerate(l2):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc691275-2a79-4c8c-947e-9cb79bb1b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat =np.zeros((len(region_c), len(region_c)))\n",
    "\n",
    "for index, (value1, value2) in enumerate(zip(index_1, index_2)):\n",
    "    #print(index, value1 , value2)\n",
    "    mat[value1][value2] = 1\n",
    "mat\n",
    "\n",
    "mat = mat + mat.T\n",
    "coordinates = np.array(region_c)  # 3D coordinates of parcels\n",
    "\n",
    "color_dict = {}\n",
    "cmap = cm.get_cmap('tab20')\n",
    "\n",
    "for rsn, c in zip(label_c, cmap.colors):\n",
    "    color_dict[rsn] = tuple(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd4e9a-56f6-4d91-a977-4f03f9e9c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color = []\n",
    "for nw in label_c:\n",
    "    node_color.append(color_dict[nw])\n",
    "\n",
    "coords = coordinates\n",
    "N = len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa5bbb-020d-450d-aed8-cdbf94b01921",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_connectome(mat,\n",
    "                         coords,\n",
    "                         title='Most important features according to SHAP',\n",
    "                         node_color=node_color,\n",
    "                          display_mode=\"lyrz\",\n",
    "                         edge_kwargs = {\"linewidth\":1.7, \"color\": 'red'})\n",
    "legend_elements = []\n",
    "for k,v in color_dict.items():\n",
    "    legend_elements.append(Line2D([0], [0], marker='o', color=v, label=k,\n",
    "                          markerfacecolor=v, markersize=5))\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.legend(handles=legend_elements, loc='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cb337-855b-4b67-999e-72870441c9ec",
   "metadata": {},
   "source": [
    "## Connettoma plot according to Cohen Values\n",
    "\n",
    "### ASD > TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c698b-56c1-4bf4-ba39-3a4f802a44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_features = important_funct_features_COHEN_ASD\n",
    "perc_95 = functional_names.loc[reduction_features]\n",
    "connection_list = functional_names.loc[reduction_features]\n",
    "l1 = connection_list['r1'].to_list()\n",
    "l2 = connection_list['r2'].to_list()\n",
    "l = l1 + l2\n",
    "sam_list = list(set(l)) # the redundant feature names are deleted\n",
    "region_c = []\n",
    "label_c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6a2c3-fca4-47ee-acbc-b36a846ceff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('cortl-maxprob-thr25-2mm')\n",
    "atlas_file = atlas_ho.maps\n",
    "# Load labels for each atlas region\n",
    "atlas_labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)\n",
    "print(len(labels))\n",
    "\n",
    "for i, e in enumerate(atlas_labels):\n",
    "  for j, n in enumerate(sam_list):\n",
    "    if e == n:\n",
    "      region_c.append(coordinates[i])\n",
    "      label_c.append(e)\n",
    "\n",
    "len(label_c)\n",
    "for i in range(0, len(label_c)):\n",
    "    print(\"i, region_c[i],label_c[i]\")\n",
    "    print(i, region_c[i],label_c[i])\n",
    "\n",
    "index_1 = []\n",
    "for j, n in enumerate(l1):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_1.append(i)\n",
    "\n",
    "index_2=[]\n",
    "for j, n in enumerate(l2):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_2.append(i)\n",
    "mat =np.zeros((len(region_c), len(region_c)))\n",
    "\n",
    "for index, (value1, value2) in enumerate(zip(index_1, index_2)):\n",
    "    #print(index, value1 , value2)\n",
    "    mat[value1][value2] = 1\n",
    "mat\n",
    "\n",
    "mat = mat + mat.T\n",
    "coordinates = np.array(region_c)  # 3D coordinates of parcels\n",
    "\n",
    "color_dict = {}\n",
    "cmap = cm.get_cmap('tab20')\n",
    "\n",
    "for rsn, c in zip(label_c, cmap.colors):\n",
    "    color_dict[rsn] = tuple(c)\n",
    "node_color = []\n",
    "for nw in label_c:\n",
    "    node_color.append(color_dict[nw])\n",
    "\n",
    "coords = coordinates\n",
    "N = len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1033c-0dd0-4e02-84db-c38e503f2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_connectome(mat,\n",
    "                         coords,\n",
    "                         title='ASD>TD',\n",
    "                         node_color=node_color,\n",
    "                          display_mode=\"lyrz\",\n",
    "                         edge_kwargs = {\"linewidth\":1.7, \"color\": 'red'})\n",
    "legend_elements = []\n",
    "for k,v in color_dict.items():\n",
    "    legend_elements.append(Line2D([0], [0], marker='o', color=v, label=k,\n",
    "                          markerfacecolor=v, markersize=5))\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.legend(handles=legend_elements, loc='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de39b28-0318-4e5b-b3c1-28ca55aa8d58",
   "metadata": {},
   "source": [
    "### TD>ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de3b4e-8724-4792-bb31-599920d56087",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_features = important_funct_features_COHEN_TD[:16]\n",
    "perc_95 = functional_names.loc[reduction_features]\n",
    "connection_list = functional_names.loc[reduction_features]\n",
    "l1 = connection_list['r1'].to_list()\n",
    "l2 = connection_list['r2'].to_list()\n",
    "l = l1 + l2\n",
    "sam_list = list(set(l)) # the redundant feature names are deleted\n",
    "region_c = []\n",
    "label_c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdd7a5-c5f6-4549-a67b-fc7c12f2639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_ho = datasets.fetch_atlas_harvard_oxford('cortl-maxprob-thr25-2mm')\n",
    "atlas_file = atlas_ho.maps\n",
    "# Load labels for each atlas region\n",
    "atlas_labels = atlas_ho.labels[1:]\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_file)\n",
    "\n",
    "for i, e in enumerate(atlas_labels):\n",
    "  for j, n in enumerate(sam_list):\n",
    "    if e == n:\n",
    "      region_c.append(coordinates[i])\n",
    "      label_c.append(e)\n",
    "\n",
    "len(label_c)\n",
    "for i in range(0, len(label_c)):\n",
    "    print(\"i, region_c[i],label_c[i]\")\n",
    "    print(i, region_c[i],label_c[i])\n",
    "\n",
    "index_1 = []\n",
    "for j, n in enumerate(l1):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_1.append(i)\n",
    "\n",
    "index_2=[]\n",
    "for j, n in enumerate(l2):\n",
    "    for i, e in enumerate(label_c):\n",
    "        if n == e:\n",
    "            index_2.append(i)\n",
    "mat =np.zeros((len(region_c), len(region_c)))\n",
    "\n",
    "for index, (value1, value2) in enumerate(zip(index_1, index_2)):\n",
    "    #print(index, value1 , value2)\n",
    "    mat[value1][value2] = 1\n",
    "mat\n",
    "\n",
    "mat = mat + mat.T\n",
    "coordinates = np.array(region_c)  # 3D coordinates of parcels\n",
    "\n",
    "color_dict = {}\n",
    "cmap = cm.get_cmap('tab20')\n",
    "\n",
    "for rsn, c in zip(label_c, cmap.colors):\n",
    "    color_dict[rsn] = tuple(c)\n",
    "node_color = []\n",
    "for nw in label_c:\n",
    "    node_color.append(color_dict[nw])\n",
    "\n",
    "coords = coordinates\n",
    "N = len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb184f65-276b-4329-ac3e-f9feb5c66083",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_connectome(mat,\n",
    "                         coords,\n",
    "                         title='TD>ASD',\n",
    "                         node_color=node_color,\n",
    "                          display_mode=\"lyrz\",\n",
    "                         edge_kwargs = {\"linewidth\":1.7, \"color\": 'blue'})\n",
    "legend_elements = []\n",
    "for k,v in color_dict.items():\n",
    "    legend_elements.append(Line2D([0], [0], marker='o', color=v, label=k,\n",
    "                          markerfacecolor=v, markersize=5))\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.legend(handles=legend_elements, loc='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4ni",
   "language": "python",
   "name": "ai4ni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
