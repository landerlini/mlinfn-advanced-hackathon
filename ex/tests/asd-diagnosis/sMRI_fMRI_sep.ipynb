{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9a6ea-67fa-4463-879a-105d9c3531f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=/tmp/asd-diagnosis\n",
    "\n",
    "if [ ! -d $DATA_DIR ]; then\n",
    "  mkdir -p $DATA_DIR\n",
    "fi\n",
    "\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/functional_features.csv -O $DATA_DIR/functional_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_structural_features.csv -O $DATA_DIR/Harmonized_structural_features.csv &> .log\n",
    "wget https://minio.131.154.99.37.myip.cloud.infn.it/hackathon-data/asd-diagnosis/Harmonized_functional_features.csv -O $DATA_DIR/Harmonized_functional_features.csv &> .log\n",
    "\n",
    "ls -lrth $DATA_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c8fc9-0179-4394-86cf-53778ee0cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import warnings\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce9629-8e19-470f-8dfc-bea755f2ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CV = 2\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec68d3-8ad2-4485-b348-e026e9ee7012",
   "metadata": {},
   "source": [
    "# Autism Spectrum Disorders (ASD) diagnosis using structural and functional Magnetic Resonance Imaging and Radiomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978eb93-32bf-4a3b-bc6a-af1b87a0fa3c",
   "metadata": {},
   "source": [
    "Autism spectrum disorders (**ASD**) are a heterogeneous group of neurodevelopmental disorders characterized by persistent deficits in reciprocal social interaction, communication, and the presence of restricted, repetitive behaviors and interests, which can include sensory processing difficulties.\n",
    "Despite ASD being currently diagnosed through a multidisciplinary and comprehensive direct evaluation of the individual with suspected ASD, associated with gold-standard behavioral observation and interview performed by clinicians expert in neurodevelopmental disorders, neuroimaging is playing a key role in identifying the neural correlates of this condition. In particular, machine learning (ML) and deep learning (DL) techniques are gaining considerable importance in supporting the diagnosis of ASD on the basis of magnetic resonance imaging (MRI) though these modalities do not have yet a clinical application. Early identification of the ASD can really help patients to manage the possible issues they may encounter given their condition.\n",
    "\n",
    "\n",
    "## Dataset and features extraction\n",
    "Since we want you to learn and understand deep learning based models, the data we give to you are directly harmonized features. However, it is interesting to show you at least an idea on how we extracted them. The datasets we are going to use are the Autism Brain Imaging Data Exchange datasets [(ABIDE 1 and ABIDE 2)](http://fcon_1000.projects.nitrc.org/indi/abide/). Data you are analyzing are the T1-weighted structural MRI (sMRI) and resting-state functional MRI (rs-fMRI or fMRI) data of the ABIDE I and ABIDE II. These two datasets contain more than one thousand cases as well as controls. However, since there is an unbalancement as regard some patients characteristics we operated a patient selection as follows:\n",
    "\n",
    "1. Since 97% of the subjects were under the age of 40 years, we limited to subjects aged 5 to 40 years only;\n",
    "2. We restricted our analysis to male subjects, due to both the limited representation of female subjects in the ABIDE collection (less than 20% of subjects, spread over different sites and a wide age-range), and the sex differences in functional brain connectivity;\n",
    "3. We excluded subjects with missing multimodal MRI data after using the preprocessing pipeline.\n",
    "\n",
    "Once the patients have been selected, the feature extraction pipelines have been applied separately for sMRI and fMRI. This is necessary since the different nature of this kind of data.\n",
    "\n",
    "### Structural MRI features extraction\n",
    "The pipeline for the features extraction on structural MRI are the following:\n",
    "\n",
    "1.  The sMRI scans have been processed with Freesurfer version 6.0 with the [recon-all](https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all) pipeline. This procedure includes cortical surface modelling, spherical coordinate transformation, non-linear curvature registration, automated volumetric segmentation and cortical reconstruction.\n",
    "2.  The following brain features have been selected: the global measures and the subcortical features available in the file aseg.stats and the cortical features available in the bilateral files aparc.stats.\n",
    "\n",
    "In this way, a total number of 221 brain morphometric features have been obtained. These brain descriptive characteristics can be grouped into:\n",
    "\n",
    "- **9 global quantities**: left (L) and right (R) mean thickness, L and R cortex volumes, L and R cerebral white matter\n",
    "volume, cerebrospinal fluid volume, total gray volumes and the volume of segmented brain without ventricles;\n",
    "- **26 volumes of sub-cortical** structures and corpus callosum;\n",
    "- **186 measures**, including the volume, the mean and standard deviation of the thickness of 62 structures (31 per\n",
    "hemisphere) from the **Desikan–Killiany–Tourville Atlas**: 14 in the temporal lobe, 20 in the frontal lobe, 10\n",
    "in the parietal lobe, 8 in the occipital lobe and 10 in the cingulate cortex.\n",
    "\n",
    "### Functional MRI features extraction\n",
    "\n",
    "The rs-fMRI scans selected from ABIDE I and ABIDE II cohorts have been processed as follows:\n",
    "\n",
    "1. The Configurable Pipeline for the Analysis of Connectomes (CPAC), that includes motion correction, slice timing correction, band-pass filtering, spatial smoothing and registration has been applied;\n",
    "2. The Harvard-Oxford (HO) atlas was used to extract time series from brain regions, obtaining 103 timeseries for each subject.\n",
    "3. The Pearson correlation was calculated between the timeseries of pairs of regions to derive a functional connectivity (FC) matrix.\n",
    "4. The correlation values were normalized according to Fisher transformation in order to make them approximately normally-distributed.\n",
    "\n",
    "This way, we obtain 5253 connectivity features for each subject. In the following, you can have an idea of what functional MRI features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ced1cc-c519-43a6-9f4e-5d3de2dd9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_data = '/tmp/asd-diagnosis/'\n",
    "\n",
    "functional_names = pd.read_csv(path_to_data + 'functional_features.csv')\n",
    "functional_names = functional_names.set_index('F')\n",
    "\n",
    "functional_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215423c4-699e-4ba8-8296-b2a64f685536",
   "metadata": {},
   "source": [
    "## Feature Harmonization\n",
    "Due to the multisite nature of the dataset, we separately harmonized the Freesurfer structural features and the\n",
    "functional connectivity measures using the publicly available Python package NeuroHarmonize, which is the state-of-\n",
    "the-art tool for multi-site neuroimaging analysis developed by Pomponio et al. \n",
    "We estimated the NeuroHarmonize model parameters on the entire cohort of **control** subjects, by specifying the age as a covariate, whose effect is to be preserved during the harmonization process. Finally, we applied the estimated model on the entire sample of subjects with ASD and TD controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b2960-e054-4f76-ba95-6222b9596ef3",
   "metadata": {},
   "source": [
    "# Exercise: first part\n",
    "\n",
    "Here, you will have to read the data and pre-process them. First you need to import the necessary libraries and the apply a scaler to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2dfd66-0e96-40ad-ac8c-0c9e2c146d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9fb85-2fa9-403f-8c9c-26a4dd8c557a",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "As discussed this morning, the available data has already been pre-processed using Neuroharmonize package. This was needed to delete the site bias.\n",
    "\n",
    "MRI images are prone to acquisition site bias. So anytime you want to perform an analysis on MRI and you have the acquisition site information, the harmonization is mandatory.\n",
    "\n",
    "Usually harmonizing data leads to a performance decrease. Even if this could seem a nasty problem, it is more important to perform unbiased analysis than having a high but fake accuracy :)\n",
    "\n",
    "We decided to avoid the harmonization part because it may take long time and we prefer to focus on models. \n",
    "Even if a first pre-processing has been already done, there are still some missing procedures you have to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856d3e5-f821-4553-b552-91ce97f09eaa",
   "metadata": {},
   "source": [
    "1) Load data from both structural and functional MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0710b-abdb-40b6-b5cb-f0f010ec36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '/tmp/asd-diagnosis/'\n",
    "\n",
    "DF_struct = pd.read_csv(path_to_data + 'Harmonized_structural_features.csv')\n",
    "DF_funct = pd.read_csv(path_to_data + 'Harmonized_functional_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6bf90-dc83-46d6-846c-af3d36dde5c7",
   "metadata": {},
   "source": [
    "> - Set the DataFrame index using 'FILE_ID'\n",
    "> - Drop the variable that are redundant\n",
    "> - Merge the two DataFrame\n",
    "\n",
    "DX_GROUP is the target variable, i.e. Autism Spectrum Disorder or Typical Development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973d7dc-cdcc-49ab-8b29-4761fe7b8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_struct = DF_struct.set_index('FILE_ID')\n",
    "DF_funct = DF_funct.set_index('FILE_ID')\n",
    "DF_funct= DF_funct.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'DX_GROUP'], axis =1)\n",
    "DF_merge = DF_struct.join(DF_funct,how='inner')\n",
    "DF_merge = DF_merge.reset_index()\n",
    "DF_merge.loc[DF_merge.DX_GROUP == -1, 'DX_GROUP'] = 0\n",
    "DF_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe02ef-91f8-4333-81ca-504e57338e72",
   "metadata": {},
   "source": [
    "> Before going through the exercise, let's plot the histogram of the patients versus the acquisition site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3fa97-b32f-44e7-a28a-fae33f8a4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = DF_merge.groupby(['SITE', 'DX_GROUP'])['SITE'].count()\n",
    "unstacked = stack.unstack('DX_GROUP')\n",
    "ax = unstacked.plot(kind='bar', stacked=False, fontsize = 25, figsize=(20,5))\n",
    "ax.legend([\"TD\", \"ASD\"], fontsize=25);\n",
    "plt.ylabel('Number of subjects', fontsize = 25)\n",
    "plt.xlabel('Sites', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6452bc-a962-4cf2-84c4-959093e07350",
   "metadata": {},
   "source": [
    "> - Normalize the data using [RobustScaler](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.html) of scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ad09e-4f04-4977-a108-40fc4bf8a8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF_normalized = DF_merge.drop(['SITE', 'Database_Abide', 'AGE_AT_SCAN', 'FILE_ID'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59408511-2f3c-45fb-86ff-fba21ff72388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "RS_instance = RobustScaler()\n",
    "DF_normalized.iloc[:,1:] = RS_instance.fit_transform(DF_normalized.iloc[:,1:].to_numpy())\n",
    "DF_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6a2d1-0123-4384-a344-6cd080e02977",
   "metadata": {},
   "source": [
    "# Classification ASD/TD using only structural features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb77a29-04a8-44f4-99b2-fb6c6bed1682",
   "metadata": {},
   "source": [
    "- Now you have to develop the model using the structural features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14030c56-1445-42b1-9aa1-492f507d3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import SGD #stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df86ea-316b-442f-95dd-24ce532ba5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_model():\n",
    "  input_data= Input(221,)\n",
    "  x = Dense(32,  'relu', kernel_initializer='normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x) #glorot_normal\n",
    "  x = BatchNormalization()(x)\n",
    "  #x = Dropout(0.1)(x)\n",
    "  x = Dense(1, 'sigmoid', kernel_regularizer=l1(0.01))(x)\n",
    "  struct_model = tf.keras.Model(inputs=input_data, outputs=x)\n",
    "  return struct_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083334cf-2198-4f36-a776-1efb6f3c9645",
   "metadata": {},
   "source": [
    "### Callback\n",
    "\n",
    "A callback is an action passed to the model fit function which is performend while training the neural network. These actions allow you to modify certain parameters when a specific condition is met while training. These actions may be implemented before or after an epoch or batch is processed, when training begins or ends and when a specific condition is met. Callbacks can help with tasks like saving the model, reducing learning rates, stopping training early, or logging performance metrics. Here we implement two actions:\n",
    "\n",
    "* EarlyStopping\n",
    "* ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc613c-29a1-4284-947a-a0e7030a234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041c95d-5195-4c9e-b9fe-5cb5792e2578",
   "metadata": {},
   "source": [
    "## Cross-validation setting\n",
    "Usually, when we train a deep learning algorithm, we divide the data set into three sets:\n",
    "1. Training set;\n",
    "2. Validation set;\n",
    "3. Test set.\n",
    "\n",
    "The algorithm is trained on the training set and evaluated on the validation set during the training phase. The test set is used once the algorithm performs well on the validation set.\n",
    "\n",
    "In this case, since the number of patient is low with respect to the number of features and the computing burden allows it, we will train the algorithm in a cross validation fashion.\n",
    "\n",
    "The entire data set will be divided into 10 folds and 1 is used as validation and the remaining will be used as training set. This way, we can consider the algorithm performance more stable. In fact, the main drawback of using train, validation and test is the possibility that the samples in the test set do not reflect the entire variability of the data. Using cross-validation, we are cyclically validate the algorithm on the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15a1ab-a870-4641-aab5-37a0d5a48973",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv = N_CV\n",
    "AUC = []\n",
    "acc = []\n",
    "shap_values_per_cv = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "np.random.seed(1) # Reproducibility\n",
    "rs_ = 13 # Reproducibility, random state for division in folds\n",
    "\n",
    "interp_fpr = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4152a-4fc7-4db3-8d08-c5cfb4f42155",
   "metadata": {},
   "source": [
    "Now, first we used the StratifiedKFold of scikit learn to divide and manage our sub sets of data.\n",
    "Then, we train 10 models using the cross validation. \n",
    "So we use a for cycle to slide along the 10 subset and we will use one of them as test/validation and the remaining data as training set.\n",
    "\n",
    "For each traning we will also register the shap values that will help to explain the behaviour of our algorithm. Once the values are collected for each fold, we will average them per features, obtaining the most important average features that contribute to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784dd96d-312f-4a8c-89ac-6168b0e4ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=rs_) \n",
    "\n",
    "for train_index, test_index in cv.split(DF_normalized.iloc[:, 1:], DF_normalized.iloc[:, 0]):\n",
    "    clear_session() # to ensure that no weights of other folds remain in memory\n",
    "\n",
    "    df_train, df_test = DF_normalized.iloc[train_index, :222], DF_normalized.iloc[test_index, :222]\n",
    "        \n",
    "    X_train, X_test = df_train.iloc[:, 1:], df_test.iloc[:, 1:]\n",
    "    y_train, y_test = df_train.iloc[:, 0], df_test.iloc[:, 0]\n",
    "\n",
    "    #load the model\n",
    "    model_struct = structural_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model_struct.compile(optimizer=SGD(learning_rate = 0.001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model_struct.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data = (X_test, y_test),\n",
    "          verbose=0,\n",
    "          callbacks=[reduce_on_plateau, early_stop]) \n",
    "\n",
    "        ###########################################################\n",
    "    \n",
    "    #Train and validation accuracy\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training ')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation ')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Accuracy')\n",
    "    #Train and validation loss\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training ')\n",
    "    plt.plot(history.history['val_loss'], label='Validation ')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(' Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "    # prepare for SHAP\n",
    "    \n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "\n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.GradientExplainer(model_struct, [X_train]) # the explainer is initialized on training set\n",
    "    shap_values = explainer.shap_values(X_test) # the values are computed on the validation set\n",
    "\n",
    "    #  SHAP information per fold per sample\n",
    "    \n",
    "    shap_values_per_cv.append(shap_values[:138])\n",
    "    \n",
    "        ###########################################################\n",
    "    _, val_acc = model_struct.evaluate(df_test.iloc[:, 1:], df_test.iloc[:, 0], verbose=0)\n",
    "    acc.append(val_acc)\n",
    "\n",
    "        #Compute Receiver operating characteristic (ROC)\n",
    "    i=0\n",
    "    preds = model_struct.predict(df_test.iloc[:, 1:], verbose=1)\n",
    "    fpr, tpr, _ = roc_curve(df_test.iloc[:, 0], preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    interp_tpr = np.interp(interp_fpr, fpr, tpr)\n",
    "    tprs.append(interp_tpr)\n",
    "    AUC.append(roc_auc)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e531c-044b-4dce-a9ba-4bc8f229d9db",
   "metadata": {},
   "source": [
    "# ROC curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e59d06-e30a-444e-bb9b-9b883b478394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "      label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(interp_fpr, mean_tpr)\n",
    "std_auc = np.std(AUC)\n",
    "plt.plot(interp_fpr, mean_tpr, color='b',\n",
    "        label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})',\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(interp_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Structural model',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474dbc93-cbeb-43bc-889f-e291460a67db",
   "metadata": {},
   "source": [
    "## Results in terms of accuracy and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82637a38-35cf-4478-9eef-9fb7df0e4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'AUC: {np.mean(AUC):.4f} (+- {np.std(AUC):.4f})')\n",
    "print(f'accuracy: {np.mean(acc):.4f} (+- {np.std(acc):.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5034ab0-df66-4041-98d7-2f05bc4eefd9",
   "metadata": {},
   "source": [
    "# Explain the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556ffa8-cdd3-4f2a-a256-21bebec86c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_shap_values_s = []\n",
    "\n",
    "for i in range(0, len(AUC)):\n",
    "    df_per_obs = shap_values_per_cv[i].copy()\n",
    "    df_per_obs = np.absolute(df_per_obs)\n",
    "    average_shap_values_s.append(df_per_obs.mean(axis=0))\n",
    "\n",
    "fold_s = np.array(average_shap_values_s)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793f0d0-26ee-44f8-8881-fcefe0c73515",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8ddd5-ea01-449a-975e-e5b3e80737dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_s = np.mean(fold_s, axis = 0)\n",
    "std_s = np.std(fold_s, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34223982-3867-41ad-b389-a6c7c775bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_struct = pd.DataFrame({\"SHAP_values\": mean_s}, index = df_train.iloc[:, 1:].columns)\n",
    "shap_struct['std']=std_s\n",
    "shap_struct = shap_struct.sort_values(by='SHAP_values', ascending=False)\n",
    "shap_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d168839-f98f-41d9-821e-1f5720b56899",
   "metadata": {},
   "source": [
    "## Visualize SHAP values and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3686737-de53-49f2-9eb6-08b97e626b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = shap_struct.iloc[:20,0].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP value| (average of 10-fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a34ef-4c97-45a5-bf85-17f5ea8f7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = shap_struct.iloc[:20].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean and std(|SHAP value| (average of 10-fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e378907-f68e-4c60-95ed-ed48f6c621eb",
   "metadata": {},
   "source": [
    "## Cohen d coeff\n",
    "\n",
    "The Cohen's *d* coefficient is a statistical measure used to quantify the *effect size* between two groups, indicating the standardized difference between their means. It is commonly used in psychology, social sciences, and other fields to assess the difference between two sample groups.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The formula for Cohen’s *d* is:\n",
    "$$\n",
    "d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s}\n",
    "$$\n",
    "where: $\\bar{X}_1$ and $\\bar{X}_2$ are the means of the two groups. $s$ is the pooled standard deviation of the two groups, calculated as:\n",
    "$$\n",
    "s = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n",
    "$$\n",
    "where: $n_1$ and $n_2$ are the sample sizes of the two groups. $s_1$ and $s_2$ are the standard deviations of the two groups.\n",
    "\n",
    "### Interpretation\n",
    "Cohen's *d* provides a way to interpret the magnitude of the difference, regardless of the scale of the data, making it easier to compare across studies. Common interpretations are:\n",
    "- **0.2** - Small effect size\n",
    "- **0.5** - Medium effect size\n",
    "- **0.8** or higher - Large effect size\n",
    "\n",
    "These are general guidelines, and the interpretation can vary by field. Cohen's *d* is particularly helpful because it puts the difference in a standardized context, allowing researchers to understand the size of an effect without being influenced by sample size alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abd469-84e6-4800-b497-15b6af7ab518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohen(g1, g2, f):\n",
    "    n1 = len(g1)\n",
    "    n2 = len(g2)\n",
    "    N = n1 + n2\n",
    "    Scores1 = g1[f].dropna()\n",
    "    Scores2 = g2[f].dropna()\n",
    "    var1 = Scores1.var()\n",
    "    var2 = Scores2.var()\n",
    "    mean1 = Scores1.mean()\n",
    "    mean2 = Scores2.mean()\n",
    "    sp = (((n1 - 1)*var1 + (n2 - 1)*var2) / (N - 2))**0.5\n",
    "    d = (mean1 - mean2) / sp\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f82e3f-f60c-46b3-90d7-13d64baa107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = df_train[df_train.DX_GROUP==0]\n",
    "ASD =  df_train[df_train.DX_GROUP==1]\n",
    "list_f = shap_struct.index.tolist()\n",
    "cohen_val = {'Feature Name': [], 'Cohen Value' : []}\n",
    "for item in list_f:\n",
    "    score =  Cohen(ASD, controls, item)\n",
    "    cohen_val['Feature Name'].append(item)\n",
    "    cohen_val['Cohen Value'].append(score)\n",
    "\n",
    "cohen_struct = pd.DataFrame(cohen_val)\n",
    "cohen_struct.set_index('Feature Name')\n",
    "cohen_struct = cohen_struct.sort_values(by='Cohen Value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef271b-ab26-416d-952c-2ac242ec10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218651d8-6e9b-49c9-b02d-6049286b9450",
   "metadata": {},
   "source": [
    "# Classification ASD/TD using only functional features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab966d8-864d-4c9d-89f6-d51149610d2a",
   "metadata": {},
   "source": [
    "We repeat the same procedure we did for the structural feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea758205-e9b6-4dd5-b184-6fad16f853a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_model():\n",
    "  input_data= Input(5253,)\n",
    "  x = Dense(64, 'relu', kernel_initializer= 'normal', kernel_regularizer=l1(0.01))(input_data)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = Dense(32, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Dense(16, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Dense(8, 'relu',kernel_initializer='normal', kernel_regularizer=l1(0.01))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  x = Dense(1, 'sigmoid', kernel_regularizer=l1(0.01))(x)\n",
    "  func_model = tf.keras.Model(inputs=input_data, outputs=x)\n",
    "  return func_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699403a0-d3ba-450f-b8bd-4f4afb93a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_normalized.iloc[:,0] # Grabs all rows and first 2 columns\n",
    "DF_normalized.iloc[:,222:] # Grabs all rows and last 2 columns\n",
    "\n",
    "DF_normalized= pd.concat([DF_normalized.iloc[:,0],DF_normalized.iloc[:,222:]],axis=1) # Puts them together row wise\n",
    "DF_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f493c2d-431c-4c63-a854-23e749a6a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv = N_CV\n",
    "AUC = []\n",
    "acc = []\n",
    "shap_values_per_cv = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "np.random.seed(1) # Reproducibility\n",
    "rs_ = 13 # Reproducibility, random state for division in folds\n",
    "\n",
    "interp_fpr = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd705c3-6f7a-496e-99da-e00ee60cc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=rs_) \n",
    "\n",
    "for train_index, test_index in cv.split(DF_normalized.iloc[:, 1:], DF_normalized.iloc[:, 0]):\n",
    "    clear_session() # to ensure that no weights of other folds remain in memory\n",
    "\n",
    "    df_train, df_test = DF_normalized.iloc[train_index,:], DF_normalized.iloc[test_index,:]\n",
    "        \n",
    "    X_train, X_test = df_train.iloc[:, 1:], df_test.iloc[:, 1:]\n",
    "    y_train, y_test = df_train.iloc[:, 0], df_test.iloc[:, 0]\n",
    "\n",
    "    #load the model\n",
    "    model_funct = functional_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model_funct.compile(optimizer=SGD(learning_rate = 0.001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model_funct.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data = (X_test, y_test),\n",
    "          verbose=0,\n",
    "          callbacks=[reduce_on_plateau, early_stop])\n",
    "\n",
    "        ###########################################################\n",
    "    \n",
    "    #Train and validation accuracy\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training ')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation ')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Accuracy')\n",
    "    #Train and validation loss\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training ')\n",
    "    plt.plot(history.history['val_loss'], label='Validation ')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(' Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    ###########################################################\n",
    "    # prepare for SHAP\n",
    "    \n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "\n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.GradientExplainer(model_funct, [X_train]) # the explainer is initialized on training set\n",
    "    shap_values = explainer.shap_values(X_test) # the values are computed on the validation set\n",
    "\n",
    "    #  SHAP information per fold per sample\n",
    "    \n",
    "    shap_values_per_cv.append(shap_values[:138])\n",
    "    \n",
    "        ###########################################################\n",
    "    _, val_acc = model_funct.evaluate(df_test.iloc[:, 1:], df_test.iloc[:, 0], verbose=0)\n",
    "    acc.append(val_acc)\n",
    "\n",
    "        #Compute Receiver operating characteristic (ROC)\n",
    "    i=0\n",
    "    preds = model_funct.predict(df_test.iloc[:, 1:], verbose=1)\n",
    "    fpr, tpr, _ = roc_curve(df_test.iloc[:, 0], preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    interp_tpr = np.interp(interp_fpr, fpr, tpr)\n",
    "    tprs.append(interp_tpr)\n",
    "    AUC.append(roc_auc)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b64882-1ba1-403a-900f-b34b2c6de3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "      label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(interp_fpr, mean_tpr)\n",
    "std_auc = np.std(AUC)\n",
    "plt.plot(interp_fpr, mean_tpr, color='b',\n",
    "        label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})',\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(interp_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Functional model',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64881cba-876a-41dd-9d03-f50bf1b2eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(AUC))\n",
    "print(f'AUC: {np.mean(AUC):.4f} (+- {np.std(AUC):.4f})')\n",
    "print(f'accuracy: {np.mean(acc):.4f} (+- {np.std(acc):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eda474-e260-4e32-8a8f-407e1e3025bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_shap_values_f = []\n",
    "\n",
    "for i in range(0, len(AUC)):\n",
    "    df_per_obs = shap_values_per_cv[i].copy()\n",
    "    df_per_obs = np.absolute(df_per_obs)\n",
    "    average_shap_values_f.append(df_per_obs.mean(axis=0))\n",
    "\n",
    "fold_f = np.array(average_shap_values_f)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac52c4-bb21-4f0d-bec9-81714f122dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184413e6-ff9e-437b-9bb5-54dc2058f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f = np.mean(fold_f, axis = 0)\n",
    "std_f = np.std(fold_f, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a011bd-3e09-4fdf-9d42-b711b2cd8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_funct = pd.DataFrame({\"SHAP_values\": mean_f}, index = df_train.iloc[:, 1:].columns)\n",
    "shap_funct['std']=std_f\n",
    "shap_funct = shap_funct.sort_values(by='SHAP_values', ascending=False)\n",
    "shap_funct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751b588-97ed-4cb1-824e-2a69637c1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = shap_funct.iloc[:20,0].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP value| (average of 10 fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00954f21-8ca0-4464-b8a0-8e8880511aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.figure(figsize=(40, 50))\n",
    "ax2 = shap_funct.iloc[:20].plot(kind=\"barh\", figsize=(10,10))\n",
    "ax2.invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP value| (average of 10 fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1d8c1-b7a7-4bcf-a7c3-720d68da730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = int(shap_funct.index[0])\n",
    "feat_name = functional_names.loc[feature_index]\n",
    "feat_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4ni",
   "language": "python",
   "name": "ai4ni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
