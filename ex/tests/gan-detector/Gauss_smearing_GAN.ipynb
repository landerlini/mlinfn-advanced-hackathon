{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3f1d5-a9bd-4187-bf3f-fb2cdb93c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pyy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3ad77-b953-46b8-8f2c-5483eef1a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/tmp/gan-detector\"\n",
    "\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f0081-a362-4816-a48f-10866339df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Versione di Python:\", sys.version)\n",
    "print(\"Versione di TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8982d5-f912-4acf-a447-e204913205ab",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca6cad-bcc1-4547-9a3f-84ba1551dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_mom(P):\n",
    "    # calcualtes the absolute value of the momentum\n",
    "    \n",
    "    return np.sqrt(P[:,1]*P[:,1] + P[:,2]*P[:,2] + P[:,3]*P[:,3])\n",
    "\n",
    "\n",
    "def calculate_errors_and_pulls(real_data, generated_data, bins=50):\n",
    "    # quantifies the error between real and generated data\n",
    "    \n",
    "    hist_real, edges_real = np.histogram(real_data, bins=bins)\n",
    "    hist_generated, edges_generated = np.histogram(generated_data, bins=bins)\n",
    "    bin_centers = 0.5 * (edges_real[1:] + edges_real[:-1])\n",
    "\n",
    "    err_real = np.sqrt(hist_real)\n",
    "    err_generated = np.sqrt(hist_generated)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    pulls = np.where((hist_real != 0) | (hist_generated != 0),\n",
    "                     (hist_real - hist_generated) / np.sqrt(hist_real + hist_generated + epsilon),\n",
    "                             0.0)\n",
    "\n",
    "    return bin_centers, hist_real, hist_generated, err_real, err_generated, pulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b6b1a-1e47-4539-bec9-e7a88de63143",
   "metadata": {},
   "source": [
    "### Training Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53589d0-84cd-4206-89ce-ec3edbd74b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_mom = (0.1, 0.05, 0.07, -0.5)\n",
    "\n",
    "identical_events = np.tile(four_mom, (100000, 1))\n",
    "\n",
    "std_dev_smearing = np.array([0.1, 0.6, 0.4, 0.7])\n",
    "\n",
    "# Generate smeared events by adding Gaussian noise to each event\n",
    "smeared_events = identical_events + np.random.normal(loc=0, scale=std_dev_smearing, size=(100000, 4))\n",
    "\n",
    "\n",
    "P_PS = tri_mom(identical_events)\n",
    "P_smeared = tri_mom(smeared_events)\n",
    "\n",
    "print(P_PS.shape, P_smeared.shape)\n",
    "\n",
    "V_data_tensor = tf.convert_to_tensor(identical_events, dtype=tf.float32)\n",
    "smeared_data_tensor = tf.convert_to_tensor(smeared_events, dtype=tf.float32)\n",
    "\n",
    "print(V_data_tensor.shape, smeared_data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef0ddf-2d52-41ef-b93e-c40d85723a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_file(f'{PATH}/V_data', tf.io.serialize_tensor(V_data_tensor))\n",
    "tf.io.write_file(f'{PATH}/smeared_data', tf.io.serialize_tensor(smeared_data_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cab973-e164-4ecf-85b4-73781126468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrth /tmp/gan-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ffbf2-2399-4ac2-b7df-193c30ec1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1D histograms for each component of the 4-momentum vector (identical_events)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# Plot a histogram for one component, say 'p0' (since all are identical)\n",
    "ax.hist(P_PS, bins=100, alpha=0.7, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title(\"P distribution of vertex level events\")\n",
    "ax.set_xlabel(\"P[GeV]\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "##############################\n",
    "\n",
    "# Create 1D histograms for each component of the 4-momentum vector (identical_events)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# Plot a histogram for one component, say 'p0' (since all are identical)\n",
    "ax.hist(P_smeared, bins=100, alpha=0.7, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title(\"P distribution of smeared level events\")\n",
    "ax.set_xlabel(\"P[GeV]\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb4d21-2305-4dff-b705-6b38b1e7b196",
   "metadata": {},
   "source": [
    "### Training dataset loading + some checks and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99638bee-fd0c-47fd-be9f-d2bc2d5ee774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "V_data = tf.io.parse_tensor(tf.io.read_file(f'{PATH}/V_data'), out_type=tf.float32)\n",
    "smeared_data = tf.io.parse_tensor(tf.io.read_file(f'{PATH}/smeared_data'), out_type=tf.float32)\n",
    "\n",
    "# Check the structure of the loaded data:\n",
    "print(V_data.shape, smeared_data.shape)\n",
    "\n",
    "V_data_np = V_data.numpy()\n",
    "smeared_data_np = smeared_data.numpy()\n",
    "\n",
    "V_data_mean = tf.reduce_mean(V_data, axis=0)\n",
    "V_data_std = tf.math.reduce_std(V_data, axis=0)\n",
    "\n",
    "smeared_data_mean = tf.reduce_mean(smeared_data, axis=0)\n",
    "smeared_data_std = tf.math.reduce_std(smeared_data, axis=0)\n",
    "\n",
    "print(\"V_data mean:\", V_data_mean.numpy())\n",
    "print(\"V_data std dev:\", V_data_std.numpy())\n",
    "\n",
    "print(\"Smeared_data mean:\", smeared_data_mean.numpy())\n",
    "print(\"Smeared_data std dev:\", smeared_data_std.numpy())\n",
    "\n",
    "######## check for nan values \n",
    "nan_mask_V_data = tf.math.is_nan(V_data)\n",
    "nan_per_column_V_data = tf.reduce_sum(tf.cast(nan_mask_V_data, tf.int32), axis=0)\n",
    "\n",
    "nan_mask_smeared_data = tf.math.is_nan(smeared_data)\n",
    "nan_per_column_smeared_data = tf.reduce_sum(tf.cast(nan_mask_smeared_data, tf.int32), axis=0)\n",
    "\n",
    "print(\"Numero di NaN in V_data per colonna:\", nan_per_column_V_data.numpy())\n",
    "print(\"Numero di NaN in smeared_data per colonna:\", nan_per_column_smeared_data.numpy())\n",
    "\n",
    "###################################\n",
    "\n",
    "plt.hist(smeared_data_np[:5000,0], bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(smeared_data_np[:5000,1], bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(smeared_data_np[:5000,2], bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(smeared_data_np[:5000,3], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709b548-bef9-4646-b403-d23672c062ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_events=smeared_data\n",
    "\n",
    "print(training_events.shape)\n",
    "print(training_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36876c-01fb-4296-b463-4d95414534c1",
   "metadata": {},
   "source": [
    "### GAN model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408b200-c672-4893-95b9-bbe21e426b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "t0 = time.time()\n",
    "class LSGAN():\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 100\n",
    "        optimizer = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "        \n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        generator_noise = Input(shape=(self.latent_dim,))\n",
    "        Det_events = self.generator([generator_noise])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated detector_events as input and determines validity\n",
    "        valid = self.discriminator(Det_events)\n",
    "       \n",
    "        # The combined model  (stacked generator and discriminator)--->Trains generator to fool discriminator\n",
    "        self.combined = Model([generator_noise], valid )\n",
    "        \n",
    "        # (!!!) Optimize w.r.t. MSE loss instead of crossentropy\n",
    "        # MMD_loss = self.MMD_loss()    \n",
    "        self.combined.compile(loss='mse', optimizer=optimizer)    \n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        x= Dense(64)(noise)\n",
    "        x=LeakyReLU(alpha=0.2)(x)\n",
    "        #x=BatchNormalization(momentum=0.8)(x)\n",
    "        \n",
    "        x= Dense(128)(x)\n",
    "        x=LeakyReLU(alpha=0.2)(x)\n",
    "        #x=BatchNormalization(momentum=0.8)(x)\n",
    "        \n",
    "        x= Dense(256)(x)\n",
    "        x=LeakyReLU(alpha=0.2)(x)\n",
    "        #x=BatchNormalization(momentum=0.8)(x)\n",
    "        \n",
    "        output = Dense(4)(x)\n",
    "        # outputmerge = concatenate([v, output ])\n",
    "        generator = Model(inputs=noise, outputs=output, name=\"generator\")\n",
    "        generator.summary()\n",
    "        return(generator)\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        D = Input(shape=(training_events.shape[1],))\n",
    "        x = Dense(256)(D)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        output = Dense(1)(x)\n",
    "        discriminator = Model(inputs=D, outputs=output, name=\"discriminator\")\n",
    "        discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50): \n",
    "        # training loop for the full model\n",
    "        \n",
    "        X_train = training_events\n",
    "        \n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        dloss=[]\n",
    "        gloss=[]\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            #Select a random batch of real detector level events\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = tf.gather(X_train, idx)\n",
    "\n",
    "            #Generates a batch of \"fake\" detector level events \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_detect_evts = self.generator.predict([noise], verbose = 0)\n",
    "\n",
    "            # ---------------------\n",
    "            # Train the Discriminator\n",
    "            # ---------------------\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_detect_evts, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train the Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise], valid)\n",
    "\n",
    "            dloss=np.append(dloss,d_loss[0])\n",
    "            gloss=np.append(gloss,g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch, dloss, gloss)\n",
    "                \n",
    "                self.generator.save_weights('./Gauss_smearing/G_%d.h5'%(epoch+EP))\n",
    "                self.discriminator.save_weights('./Gauss_smearing/D_%d.h5'%(epoch+EP))\n",
    "                print('saved ...')\n",
    "\n",
    "    def sample_images(self, epoch, dloss, gloss):\n",
    "\n",
    "        dirname = f\"./Gauss_smearing/{epoch+EP}/P\"\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        \n",
    "        # plots the training varibales and compares them to the generated variables during the training\n",
    "        \n",
    "        t = time.time()\n",
    "        time_elapsed = t - t0\n",
    "        print(\"time elapsed: %.2f\"%(time_elapsed))\n",
    "\n",
    "        # Define SAMPLE_SIZE and noise\n",
    "        SAMPLE_SIZE = 20000\n",
    "        noise = np.random.normal(0, 1, (SAMPLE_SIZE, 100))\n",
    "\n",
    "        # Generate samples using the generator\n",
    "        results = self.generator.predict([noise], batch_size = 128)\n",
    "        \n",
    "        # Extract the 4-momenta components from generated samples\n",
    "        generated_E = results[:, 0]\n",
    "        generated_Px = results[:, 1]\n",
    "        generated_Py = results[:, 2]\n",
    "        generated_Pz = results[:, 3]\n",
    "        \n",
    "        G_P = tri_mom(results)\n",
    "\n",
    "        # Sample real data for comparison\n",
    "        idx = np.random.randint(0, training_events.shape[0], SAMPLE_SIZE)\n",
    "        idx_tensor = tf.constant(idx, dtype=tf.int32)\n",
    "        real_samples = tf.gather(training_events, idx_tensor)\n",
    "        real_E = real_samples[:, 0]\n",
    "        real_Px = real_samples[:, 1]\n",
    "        real_Py = real_samples[:, 2]\n",
    "        real_Pz = real_samples[:, 3]\n",
    "\n",
    "        R_P = tri_mom(real_samples)\n",
    "\n",
    "        print(results.shape, real_samples.shape)\n",
    "\n",
    "        bin_centers_E, hist_real_E, hist_generated_E, err_real_E, err_generated_E, pulls_E = calculate_errors_and_pulls(real_E, generated_E)\n",
    "        bin_centers_Px, hist_real_Px, hist_generated_Px, err_real_Px, err_generated_Px, pulls_Px = calculate_errors_and_pulls(real_Px, generated_Px)\n",
    "        bin_centers_Py, hist_real_Py, hist_generated_Py, err_real_Py, err_generated_Py, pulls_Py = calculate_errors_and_pulls(real_Py, generated_Py)\n",
    "        bin_centers_Pz, hist_real_Pz, hist_generated_Pz, err_real_Pz, err_generated_Pz, pulls_Pz = calculate_errors_and_pulls(real_Pz, generated_Pz)\n",
    "\n",
    "                        # Plot histograms for P\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(R_P, bins=50, alpha=0.5, label='Real P')\n",
    "        plt.hist(G_P, bins=50, alpha=0.5, label='Generated P')\n",
    "        plt.title('P Distribution')\n",
    "        plt.legend()\n",
    "        plt.savefig(\"./Gauss_smearing/%d/P\"%(epoch+EP))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        ## histograms for training variables\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        # Plot histogram for E\n",
    "        axs[0, 0].hist(real_E, bins=50, alpha=0.5, label='Real E')\n",
    "        axs[0, 0].hist(generated_E, bins=50, alpha=0.5, label='Generated E')\n",
    "        axs[0, 0].set_title('E Distribution')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        # Plot histogram for Px\n",
    "        axs[0, 1].hist(real_Px, bins=50, alpha=0.5, label='Real Px')\n",
    "        axs[0, 1].hist(generated_Px, bins=50, alpha=0.5, label='Generated Px')\n",
    "        axs[0, 1].set_title('Px Distribution')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        # Plot histogram for Py\n",
    "        axs[1, 0].hist(real_Py, bins=50, alpha=0.5, label='Real Py')\n",
    "        axs[1, 0].hist(generated_Py, bins=50, alpha=0.5, label='Generated Py')\n",
    "        axs[1, 0].set_title('Py Distribution')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        # Plot histogram for Pz\n",
    "        axs[1, 1].hist(real_Pz, bins=50, alpha=0.5, label='Real Pz')\n",
    "        axs[1, 1].hist(generated_Pz, bins=50, alpha=0.5, label='Generated Pz')\n",
    "        axs[1, 1].set_title('Pz Distribution')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        # Adjust layout for spacing and display the plots\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./Gauss_smearing/%d/Training_variables\"%(epoch+EP))\n",
    "        plt.show()        \n",
    "        \n",
    "        nrows,ncols=1,1\n",
    "        fig = pyy.figure(figsize=(ncols*7,nrows*5))\n",
    "\n",
    "        ax=pyy.subplot(nrows,ncols,1)\n",
    "        ax.plot(range(1,len(gloss)+1),gloss,label=r'$\\rm generator$')\n",
    "        ax.plot(range(1,len(dloss)+1),dloss,label=r'$\\rm discriminator$')\n",
    "        ax.semilogy()\n",
    "        ax.semilogx()\n",
    "        ax.legend(fontsize=20)\n",
    "        ax.set_ylabel(r'$\\rm Loss$',size=20)\n",
    "        ax.set_xlabel(r'$\\rm epochs$',size=20)\n",
    "        ax.tick_params(axis='both', which='both', labelsize=15,direction='in')\n",
    "        plt.savefig(\"./Gauss_smearing/loss_function_%d\"%(epoch+EP))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493aa46c-446d-4ed9-ba6d-10e1d3f6da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = LSGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2be9e5-55fd-49a9-80e9-e0a0b6d58376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "EP = 0\n",
    "\n",
    "#gan.generator.load_weights('/home/private/Gauss_smearing/G_%d.h5'%(EP))\n",
    "#gan.discriminator.load_weights('/home/private/Gauss_smearing/D_%d.h5'%(EP))\n",
    "\n",
    "gan.train(epochs=EPOCH, batch_size=10000, sample_interval=EPOCH - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b8c45-a8a3-4867-a58c-4ddca68137e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EP = 4\n",
    "\n",
    "gan.generator.load_weights('./Gauss_smearing/G_%d.h5'%(EP))\n",
    "gan.discriminator.load_weights('./Gauss_smearing/D_%d.h5'%(EP))\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "noise = np.random.normal(0, 1, (SAMPLE_SIZE, 100))\n",
    "\n",
    "results = gan.generator.predict([noise], batch_size = 10000) ###BATCHSIZE = 128\n",
    "\n",
    "\n",
    "        #Extract the different components from generated samples\n",
    "generated_E = results[:, 0]\n",
    "generated_Px = results[:, 1]\n",
    "generated_Py = results[:, 2]\n",
    "generated_Pz = results[:, 3]\n",
    "\n",
    "G_P = tri_mom(results)\n",
    "\n",
    "        # Sample real data for comparison\n",
    "idx = np.random.randint(0, training_events.shape[0], SAMPLE_SIZE)\n",
    "idx_tensor = tf.constant(idx, dtype=tf.int32)\n",
    "real_samples = tf.gather(training_events, idx_tensor)\n",
    "\n",
    "real_E = real_samples[:, 0]\n",
    "real_Px = real_samples[:, 1]\n",
    "real_Py = real_samples[:, 2]\n",
    "real_Pz = real_samples[:, 3]\n",
    "\n",
    "R_P = tri_mom(real_samples)\n",
    "\n",
    "print(results.shape, real_samples.shape)\n",
    "\n",
    "\n",
    "bin_centers_E, hist_real_E, hist_generated_E, err_real_E, err_generated_E, pulls_E = calculate_errors_and_pulls(real_E, generated_E)\n",
    "bin_centers_Px, hist_real_Px, hist_generated_Px, err_real_Px, err_generated_Px, pulls_Px = calculate_errors_and_pulls(real_Px, generated_Px)\n",
    "bin_centers_Py, hist_real_Py, hist_generated_Py, err_real_Py, err_generated_Py, pulls_Py = calculate_errors_and_pulls(real_Py, generated_Py)\n",
    "bin_centers_Pz, hist_real_Pz, hist_generated_Pz, err_real_Pz, err_generated_Pz, pulls_Pz = calculate_errors_and_pulls(real_Pz, generated_Pz)\n",
    "\n",
    "\n",
    "        ############ Plot errors for E\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5), gridspec_kw={'height_ratios': [5, 2]})\n",
    "\n",
    "# Top plot for Px distribution with errors\n",
    "axs[0].errorbar(bin_centers_Px, hist_real_Px, yerr=err_real_Px, fmt='o', label='Real Px', alpha=0.5)\n",
    "axs[0].errorbar(bin_centers_Px, hist_generated_Px, yerr=err_generated_Px, fmt='o', label='Generated Px', alpha=0.5)\n",
    "axs[0].set_title('Px Distribution with Errors')\n",
    "axs[0].legend()\n",
    "\n",
    "        # Bottom plot for Px pulls\n",
    "axs[1].scatter(bin_centers_Px, pulls_Px, label='Px Pulls')\n",
    "axs[1].axhline(1, ls=':', color='k')\n",
    "axs[1].axhline(-1, ls=':', color='k')\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].set_title('Px Pulls')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        ########### Plot errors for Px\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5), gridspec_kw={'height_ratios': [5, 2]})\n",
    "\n",
    "        # Top plot for Px\n",
    "axs[0].errorbar(bin_centers_Px, hist_real_Px, yerr=err_real_Px, fmt='o', label='Real Px', alpha=0.5)\n",
    "axs[0].errorbar(bin_centers_Px, hist_generated_Px, yerr=err_generated_Px, fmt='o', label='Generated Px', alpha=0.5)\n",
    "axs[0].set_title('Px Distribution with Errors')\n",
    "axs[0].legend()\n",
    "\n",
    "        # Plot pulls for Px\n",
    "axs[1].scatter(bin_centers_Px, pulls_Px, label='Px Pulls')\n",
    "axs[1].axhline(1, ls=':', color='k')\n",
    "axs[1].axhline(-1, ls=':', color='k')\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].set_title('Px Pulls')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        ########### Plot errors for Py\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5), gridspec_kw={'height_ratios': [5, 2]})\n",
    "\n",
    "        # Top plot for Py\n",
    "axs[0].errorbar(bin_centers_Py, hist_real_Py, yerr=err_real_Py, fmt='o', label='Real Py', alpha=0.5)\n",
    "axs[0].errorbar(bin_centers_Py, hist_generated_Py, yerr=err_generated_Py, fmt='o', label='Generated Py', alpha=0.5)\n",
    "axs[0].set_title('Py Distribution with Errors')\n",
    "axs[0].legend()\n",
    "\n",
    "        # Plot pulls for Py\n",
    "axs[1].scatter(bin_centers_Py, pulls_Py, label='Py Pulls')\n",
    "axs[1].axhline(1, ls=':', color='k')\n",
    "axs[1].axhline(-1, ls=':', color='k')\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].set_title('Py Pulls')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        ########### Plot errors for Pz\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5), gridspec_kw={'height_ratios': [5, 2]})\n",
    "\n",
    "        # Top plot for Pz\n",
    "axs[0].errorbar(bin_centers_Pz, hist_real_Pz, yerr=err_real_Pz, fmt='o', label='Real Pz', alpha=0.5)\n",
    "axs[0].errorbar(bin_centers_Pz, hist_generated_Pz, yerr=err_generated_Pz, fmt='o', label='Generated Pz', alpha=0.5)\n",
    "axs[0].set_title('Pz Distribution with Errors')\n",
    "axs[0].legend()\n",
    "\n",
    "        # Plot pulls for Pz\n",
    "axs[1].scatter(bin_centers_Pz, pulls_Pz, label='Pz Pulls')\n",
    "axs[1].axhline(1, ls=':', color='k')\n",
    "axs[1].axhline(-1, ls=':', color='k')\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].set_title('Pz Pulls')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfc9bf-173e-44f1-b858-01fabb9df8cc",
   "metadata": {},
   "source": [
    "### Proposed exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73820ac2-8b93-436f-a2f3-30e7cac330f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ~ 6min ogni 500 epoche. Dopo le 1500 epoche, il training smette di migliorare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acf172-a74b-465b-a991-71ec389abdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Change the dimensionality of the architecture:\n",
    "        # - Make the models less complex (to understand where is the lower limit in which the model\n",
    "        #    loses expressivity\n",
    "\n",
    "        # - Make the model more complex (deeper and wider) to see if there how much we improve the results \n",
    "        #    compared to a simpler one\n",
    "\n",
    "# --> Understand the dinamic training of the model: \n",
    "        # - Make the model unbalanced (Discriminator >> Generator) or viceversa and look at the loss function\n",
    "        #    during the training to see what happens to the convergence of the model\n",
    "\n",
    "        # - Play with the batch size of the training to see what is the minimum number of events that gives a full\n",
    "        #    generalisable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d182002-de7d-41d2-ab54-9a38976e034d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-k2",
   "language": "python",
   "name": "gan-k2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
